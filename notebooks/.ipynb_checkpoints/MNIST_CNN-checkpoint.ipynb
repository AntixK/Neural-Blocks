{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/robot/miniconda3/envs/main/lib/python36.zip', '/home/robot/miniconda3/envs/main/lib/python3.6', '/home/robot/miniconda3/envs/main/lib/python3.6/lib-dynload', '', '/home/robot/miniconda3/envs/main/lib/python3.6/site-packages', '/home/robot/miniconda3/envs/main/lib/python3.6/site-packages/IPython/extensions', '/home/robot/.ipython', '/home/robot/Anand/']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm_notebook\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns             \n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "PATH = '/home/robot/Anand/'\n",
    "sys.path.append(PATH)\n",
    "print(sys.path)\n",
    "\n",
    "from NeuralBlocks.models import LeNet\n",
    "from NeuralBlocks.trainers import SupervisedTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2456)\n",
    "# cudnn.deterministic = True\n",
    "cudnn.benchmark = True #False\n",
    "np.random.seed(2456)\n",
    "\n",
    "NUM_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "CHECKPOINT_INTERVAL = 100\n",
    "LRS = [0.0001, 0.001, 0.01]\n",
    "NORMS =[None,'BN', 'SN', 'WN','MWN', 'MSN', 'MSNTReLU', 'MWNTReLU']\n",
    "DATA_PATH = PATH+\"NeuralBlocks/data_utils/datasets/MNIST/\"\n",
    "SAVE_PATH = PATH+\"NeuralBlocks/experiments/MNIST/\"\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [#transforms.RandomAffine(degrees=7, translate=(0.1, 0.1), scale=(0.95, 1.05)), \n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.19036,), (0.34743,)),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.19036,), (0.34743,)),\n",
    "    ])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net =  LeNet(1, num_class=10, norm='BN')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "trainer = SupervisedTrainer(net, data_bunch=[trainloader, testloader], optimizer=optimizer,\n",
    "                           loss_function=criterion, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>95.62</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.52</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.607</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>99.117</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>99.352</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.468</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.57</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.662</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.87</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.77</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-265c681b0384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Anand/NeuralBlocks/trainers/supervised_trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_epochs, model_save_path)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Display Time taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mwall_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wall Time: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwall_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# Save model on keyboard interrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'time'"
     ]
    }
   ],
   "source": [
    "trainer.run(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.log_handle.get_epoch_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.log.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc = 0 \n",
    "# for lr in tqdm_notebook(LRS):\n",
    "#     for norm in tqdm_notebook(NORMS):\n",
    "#         net =  LeNet(1, num_class=10, norm=norm).cuda()\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# #         train_loss_log =[]\n",
    "# #         train_acc_log = []\n",
    "# #         test_loss_log =[]\n",
    "# #         test_acc_log =[]\n",
    "\n",
    "# #         for epoch in tqdm_notebook(range(NUM_EPOCH)):\n",
    "# #             train(epoch)\n",
    "# #             test(epoch)\n",
    "\n",
    "# #         np.save(SAVE_PATH+\"LeNet_Train_loss_{}_{}.npy\".format(norm,lr), train_loss_log)  \n",
    "#         np.save(SAVE_PATH+\"LeNet_Test_loss_{}_{}.npy\".format(norm,lr), test_loss_log)    \n",
    "#         np.save(SAVE_PATH+\"LeNet_Train_Acc_{}_{}.npy\".format(norm,lr), train_acc_log)    \n",
    "#         np.save(SAVE_PATH+\"LeNet_Test_Acc_{}_{}.npy\".format(norm,lr), test_acc_log)   \n",
    "#         del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 0\n",
    "# plt.figure(figsize=(18, 15))\n",
    "# for i, lr in enumerate(tqdm_notebook(LRS)):    \n",
    "#     for j, norm in enumerate(tqdm_notebook([None,'BN', 'SN', 'MSN'])):\n",
    "#         print(k)\n",
    "#         train_loss_log = np.load(SAVE_PATH+\"LeNet_Train_loss_{}_{}.npy\".format(norm,lr) )  \n",
    "#         test_loss_log = np.load(SAVE_PATH+\"LeNet_Test_loss_{}_{}.npy\".format(norm,lr))    \n",
    "#         train_acc_log = np.load(SAVE_PATH+\"LeNet_Train_Acc_{}_{}.npy\".format(norm,lr))    \n",
    "#         test_acc_log= np.load(SAVE_PATH+\"LeNet_Test_Acc_{}_{}.npy\".format(norm,lr))   \n",
    "        \n",
    "#         ax = plt.subplot(3,4,k+1)\n",
    "#         plt.plot(train_loss_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Train Loss', fontsize=18)     \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "#         ax = plt.subplot(3,4,k+2)\n",
    "#         plt.plot(test_loss_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Test Loss', fontsize=18) \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "#         ax = plt.subplot(3,4,k+3)\n",
    "#         plt.plot(train_acc_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Train Accuracy', fontsize=18) \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "                \n",
    "#         ax = plt.subplot(3,4,k+4)\n",
    "#         plt.plot(test_acc_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Test Accuracy', fontsize=18)        \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "#     k+= 4\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(SAVE_PATH+'Act_Norm_Results.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 0\n",
    "# plt.figure(figsize=(18, 15))\n",
    "# for i, lr in enumerate(tqdm_notebook(LRS)):    \n",
    "#     for j, norm in enumerate(tqdm_notebook(['WN', 'SN', 'MSN'])):\n",
    "#         print(k)\n",
    "#         train_loss_log = np.load(SAVE_PATH+\"LeNet_Train_loss_{}_{}.npy\".format(norm,lr) )  \n",
    "#         test_loss_log = np.load(SAVE_PATH+\"LeNet_Test_loss_{}_{}.npy\".format(norm,lr))    \n",
    "#         train_acc_log = np.load(SAVE_PATH+\"LeNet_Train_Acc_{}_{}.npy\".format(norm,lr))    \n",
    "#         test_acc_log= np.load(SAVE_PATH+\"LeNet_Test_Acc_{}_{}.npy\".format(norm,lr))   \n",
    "        \n",
    "#         ax = plt.subplot(3,4,k+1)\n",
    "#         plt.plot(train_loss_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Train Loss', fontsize=18)     \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "#         ax = plt.subplot(3,4,k+2)\n",
    "#         plt.plot(test_loss_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Test Loss', fontsize=18) \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "#         ax = plt.subplot(3,4,k+3)\n",
    "#         plt.plot(train_acc_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Train Accuracy', fontsize=18) \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "                \n",
    "#         ax = plt.subplot(3,4,k+4)\n",
    "#         plt.plot(test_acc_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Test Accuracy', fontsize=18)        \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "#     k+= 4\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(SAVE_PATH+'Weight_reparam_Results.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 0\n",
    "# plt.figure(figsize=(18, 15))\n",
    "# for i, lr in enumerate(tqdm_notebook(LRS)):    \n",
    "#     for j, norm in enumerate(tqdm_notebook(['MSNTReLU', 'WNTReLU'])):\n",
    "#         print(k)\n",
    "#         train_loss_log = np.load(SAVE_PATH+\"LeNet_Train_loss_{}_{}.npy\".format(norm,lr) )  \n",
    "#         test_loss_log = np.load(SAVE_PATH+\"LeNet_Test_loss_{}_{}.npy\".format(norm,lr))    \n",
    "#         train_acc_log = np.load(SAVE_PATH+\"LeNet_Train_Acc_{}_{}.npy\".format(norm,lr))    \n",
    "#         test_acc_log= np.load(SAVE_PATH+\"LeNet_Test_Acc_{}_{}.npy\".format(norm,lr))   \n",
    "        \n",
    "#         ax = plt.subplot(3,4,k+1)\n",
    "#         plt.plot(train_loss_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Train Loss', fontsize=18)     \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "#         ax = plt.subplot(3,4,k+2)\n",
    "#         plt.plot(test_loss_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Test Loss', fontsize=18) \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "#         ax = plt.subplot(3,4,k+3)\n",
    "#         plt.plot(train_acc_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Train Accuracy', fontsize=18) \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "                \n",
    "#         ax = plt.subplot(3,4,k+4)\n",
    "#         plt.plot(test_acc_log, lw=2.5, label=str(norm))\n",
    "#         plt.xlabel('Iterations', fontsize=18)\n",
    "#         plt.ylabel('Test Accuracy', fontsize=18)        \n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.grid(True)\n",
    "#         plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "#     k+= 4\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(SAVE_PATH+'Weight_reparam_act_Results.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = ['Epoch',  'Train_Loss', 'Test_Loss',\n",
    "                     'Train_Accuracy', 'Test_Accuracy']\n",
    "logger = pd.DataFrame(\n",
    "    columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {5:[1,1,1,1,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.DataFrame.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
