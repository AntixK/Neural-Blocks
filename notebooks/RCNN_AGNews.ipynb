{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns        \n",
    "from torch.nn import functional as F\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "PATH ='/home/antixk/Anand/' #'/home/robot/Anand/'\n",
    "sys.path.append(PATH)\n",
    "\n",
    "# Reference - https://github.com/AnubhavGupta3377/Text-Classification-Models-Pytorch/blob/master/Model_RCNN/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = PATH + \"NeuralBlocks/data_utils/datasets/AGnews/\"\n",
    "train_file = DATA_PATH + 'ag_news.train'\n",
    "test_file = DATA_PATH + 'ag_news.test'\n",
    "\n",
    "Word2Vec_file = DATA_PATH + 'glove.6B.300d.txt'\n",
    "\n",
    "class Config(object):\n",
    "    embed_size = 300\n",
    "    hidden_layers = 1\n",
    "    hidden_size = 64\n",
    "    output_size = 4\n",
    "    max_epochs = 15\n",
    "    hidden_size_linear = 64\n",
    "    lr = 0.5\n",
    "    batch_size = 128\n",
    "    seq_len = None # Sequence length for RNN\n",
    "    max_sen_len = None\n",
    "    dropout_keep = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_iterator = None\n",
    "        self.test_iterator = None\n",
    "        self.val_iterator = None\n",
    "        self.vocab = []\n",
    "        self.word_embeddings = {}\n",
    "    \n",
    "    def parse_label(self, label):\n",
    "        '''\n",
    "        Get the actual labels from label string\n",
    "        Input:\n",
    "            label (string) : labels of the form '__label__2'\n",
    "        Returns:\n",
    "            label (int) : integer value corresponding to label string\n",
    "        '''\n",
    "        return int(label.strip()[-1])\n",
    "\n",
    "    def get_pandas_df(self, filename):\n",
    "        '''\n",
    "        Load the data into Pandas.DataFrame object\n",
    "        This will be used to convert data to torchtext object\n",
    "        '''\n",
    "        with open(filename, 'r') as datafile:     \n",
    "            data = [line.strip().split(',', maxsplit=1) for line in datafile]\n",
    "            data_text = list(map(lambda x: x[1], data))\n",
    "            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n",
    "\n",
    "        full_df = pd.DataFrame({\"text\":data_text, \"label\":data_label})\n",
    "        return full_df\n",
    "    \n",
    "    def load_data(self, w2v_file, train_file, test_file, val_file=None):\n",
    "        '''\n",
    "        Loads the data from files\n",
    "        Sets up iterators for training, validation and test data\n",
    "        Also create vocabulary and word embeddings based on the data\n",
    "        \n",
    "        Inputs:\n",
    "            w2v_file (String): absolute path to file containing word embeddings (GloVe/Word2Vec)\n",
    "            train_file (String): absolute path to training file\n",
    "            test_file (String): absolute path to test file\n",
    "            val_file (String): absolute path to validation file\n",
    "        '''\n",
    "\n",
    "        NLP = spacy.load('en')\n",
    "        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != \" \"]\n",
    "        \n",
    "        # Creating Field for data\n",
    "        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n",
    "        LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "        datafields = [(\"text\",TEXT),(\"label\",LABEL)]\n",
    "        \n",
    "        # Load data from pd.DataFrame into torchtext.data.Dataset\n",
    "        train_df = self.get_pandas_df(train_file)\n",
    "        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n",
    "        train_data = data.Dataset(train_examples, datafields)\n",
    "        \n",
    "        test_df = self.get_pandas_df(test_file)\n",
    "        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n",
    "        test_data = data.Dataset(test_examples, datafields)\n",
    "        \n",
    "        # If validation file exists, load it. Otherwise get validation data from training data\n",
    "        if val_file:\n",
    "            val_df = self.get_pandas_df(val_file)\n",
    "            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n",
    "            val_data = data.Dataset(val_examples, datafields)\n",
    "        else:\n",
    "            train_data, val_data = train_data.split(split_ratio=0.8)\n",
    "        \n",
    "        TEXT.build_vocab(train_data, vectors=Vectors(w2v_file))\n",
    "        self.word_embeddings = TEXT.vocab.vectors\n",
    "        self.vocab = TEXT.vocab\n",
    "        \n",
    "        self.train_iterator = data.BucketIterator(\n",
    "            (train_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=True)\n",
    "        \n",
    "        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (val_data, test_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=False)\n",
    "        \n",
    "        print (\"Loaded {} training examples\".format(len(train_data)))\n",
    "        print (\"Loaded {} test examples\".format(len(test_data)))\n",
    "        print (\"Loaded {} validation examples\".format(len(val_data)))\n",
    "\n",
    "\n",
    "def evaluate_model(model, iterator):\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for idx,batch in enumerate(iterator):\n",
    "        if torch.cuda.is_available():\n",
    "            x = batch.text.cuda()\n",
    "        else:\n",
    "            x = batch.text\n",
    "        y_pred = model(x)\n",
    "        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_y.extend(batch.label.numpy())\n",
    "    score = accuracy_score(all_y, np.array(all_preds).flatten())\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, config, vocab_size, word_embeddings, norm='BN'):\n",
    "        super(RCNN, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n",
    "        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
    "        \n",
    "        # Bi-directional LSTM for RCNN\n",
    "        self.lstm = nn.LSTM(input_size = self.config.embed_size,\n",
    "                            hidden_size = self.config.hidden_size,\n",
    "                            num_layers = self.config.hidden_layers,\n",
    "                            dropout = self.config.dropout_keep,\n",
    "                            bidirectional = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.config.dropout_keep)\n",
    "        \n",
    "        # Linear layer to get \"convolution output\" to be passed to Pooling Layer\n",
    "        self.W = nn.Linear(\n",
    "            self.config.embed_size + 2*self.config.hidden_size,\n",
    "            self.config.hidden_size_linear\n",
    "        )\n",
    "        \n",
    "        # Tanh non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Fully-Connected Layer\n",
    "        self.fc = nn.Linear(\n",
    "            self.config.hidden_size_linear,\n",
    "            self.config.output_size\n",
    "        )\n",
    "        \n",
    "        # Softmax non-linearity\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape = (seq_len, batch_size)\n",
    "        embedded_sent = self.embeddings(x)\n",
    "        # embedded_sent.shape = (seq_len, batch_size, embed_size)\n",
    "\n",
    "        lstm_out, (h_n,c_n) = self.lstm(embedded_sent)\n",
    "        # lstm_out.shape = (seq_len, batch_size, 2 * hidden_size)\n",
    "        \n",
    "        input_features = torch.cat([lstm_out,embedded_sent], 2).permute(1,0,2)\n",
    "        # final_features.shape = (batch_size, seq_len, embed_size + 2*hidden_size)\n",
    "        \n",
    "        linear_output = self.tanh(\n",
    "            self.W(input_features)\n",
    "        )\n",
    "        # linear_output.shape = (batch_size, seq_len, hidden_size_linear)\n",
    "        \n",
    "        linear_output = linear_output.permute(0,2,1) # Reshaping fot max_pool\n",
    "        \n",
    "        max_out_features = F.max_pool1d(linear_output, linear_output.shape[2]).squeeze(2)\n",
    "        # max_out_features.shape = (batch_size, hidden_size_linear)\n",
    "        \n",
    "        max_out_features = self.dropout(max_out_features)\n",
    "        final_out = self.fc(max_out_features)\n",
    "        return self.softmax(final_out)\n",
    "    def add_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def add_loss_op(self, loss_op):\n",
    "        self.loss_op = loss_op\n",
    "    \n",
    "    def reduce_lr(self):\n",
    "        print(\"Reducing LR\")\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g['lr'] = g['lr'] / 2\n",
    "                \n",
    "    def run_epoch(self, train_iterator, val_iterator, epoch):\n",
    "        train_losses = []\n",
    "        val_accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        # Reduce learning rate as number of epochs increase\n",
    "        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n",
    "            self.reduce_lr()\n",
    "            \n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            self.optimizer.zero_grad()\n",
    "            if torch.cuda.is_available():\n",
    "                x = batch.text.cuda()\n",
    "                y = (batch.label - 1).type(torch.cuda.LongTensor)\n",
    "            else:\n",
    "                x = batch.text\n",
    "                y = (batch.label - 1).type(torch.LongTensor)\n",
    "            y_pred = self.__call__(x)\n",
    "            loss = self.loss_op(y_pred, y)\n",
    "            loss.backward()\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            self.optimizer.step()\n",
    "    \n",
    "            if i % 100 == 0:\n",
    "                print(\"Iter: {}\".format(i+1))\n",
    "                avg_train_loss = np.mean(losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n",
    "                losses = []\n",
    "                \n",
    "                # Evalute Accuracy on validation set\n",
    "                val_accuracy = evaluate_model(self, val_iterator)\n",
    "                print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
    "                self.train()\n",
    "                \n",
    "        return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399110/400000 [00:38<00:00, 9444.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 96000 training examples\n",
      "Loaded 7600 test examples\n",
      "Loaded 24000 validation examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 399110/400000 [00:50<00:00, 9444.95it/s]"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "dataset = Dataset(config)\n",
    "dataset.load_data(Word2Vec_file, train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antixk/miniconda3/envs/main/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.24410\n",
      "\tVal Accuracy: 0.2553\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.49350\n",
      "\tVal Accuracy: 0.7905\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.76865\n",
      "\tVal Accuracy: 0.8374\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.81973\n",
      "\tVal Accuracy: 0.8490\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.82936\n",
      "\tVal Accuracy: 0.8578\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.84179\n",
      "\tVal Accuracy: 0.8589\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.84205\n",
      "\tVal Accuracy: 0.8621\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.85221\n",
      "\tVal Accuracy: 0.8660\n",
      "Epoch: 1\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.85179\n",
      "\tVal Accuracy: 0.8650\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.85748\n",
      "\tVal Accuracy: 0.8721\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.86298\n",
      "\tVal Accuracy: 0.8710\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.86680\n",
      "\tVal Accuracy: 0.8755\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.86678\n",
      "\tVal Accuracy: 0.8762\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.86727\n",
      "\tVal Accuracy: 0.8745\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.86962\n",
      "\tVal Accuracy: 0.8794\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.87425\n",
      "\tVal Accuracy: 0.8811\n",
      "Epoch: 2\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.90854\n",
      "\tVal Accuracy: 0.8784\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.87035\n",
      "\tVal Accuracy: 0.8769\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.87625\n",
      "\tVal Accuracy: 0.8654\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.88111\n",
      "\tVal Accuracy: 0.8846\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.87519\n",
      "\tVal Accuracy: 0.8837\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.88148\n",
      "\tVal Accuracy: 0.8836\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.88262\n",
      "\tVal Accuracy: 0.8735\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88605\n",
      "\tVal Accuracy: 0.8872\n",
      "Epoch: 3\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.94349\n",
      "\tVal Accuracy: 0.8934\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.87880\n",
      "\tVal Accuracy: 0.8911\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.88197\n",
      "\tVal Accuracy: 0.8885\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.88578\n",
      "\tVal Accuracy: 0.8826\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.88419\n",
      "\tVal Accuracy: 0.8932\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.88970\n",
      "\tVal Accuracy: 0.8839\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.88331\n",
      "\tVal Accuracy: 0.8919\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88634\n",
      "\tVal Accuracy: 0.8924\n",
      "Epoch: 4\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.85044\n",
      "\tVal Accuracy: 0.8895\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.88808\n",
      "\tVal Accuracy: 0.8907\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.88685\n",
      "\tVal Accuracy: 0.8728\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.89262\n",
      "\tVal Accuracy: 0.8938\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.89302\n",
      "\tVal Accuracy: 0.8928\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.88999\n",
      "\tVal Accuracy: 0.8923\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.88676\n",
      "\tVal Accuracy: 0.8779\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88875\n",
      "\tVal Accuracy: 0.8904\n",
      "Epoch: 5\n",
      "Reducing LR\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.86812\n",
      "\tVal Accuracy: 0.8975\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.89508\n",
      "\tVal Accuracy: 0.8945\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.89534\n",
      "\tVal Accuracy: 0.8978\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.89781\n",
      "\tVal Accuracy: 0.8997\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.89578\n",
      "\tVal Accuracy: 0.8972\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.89575\n",
      "\tVal Accuracy: 0.8956\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.89570\n",
      "\tVal Accuracy: 0.8956\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.89703\n",
      "\tVal Accuracy: 0.8930\n",
      "Epoch: 6\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.87920\n",
      "\tVal Accuracy: 0.8992\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.89916\n",
      "\tVal Accuracy: 0.8905\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.89767\n",
      "\tVal Accuracy: 0.8954\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90210\n",
      "\tVal Accuracy: 0.8965\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.89727\n",
      "\tVal Accuracy: 0.9002\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.89370\n",
      "\tVal Accuracy: 0.9020\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.89929\n",
      "\tVal Accuracy: 0.8984\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.89988\n",
      "\tVal Accuracy: 0.8987\n",
      "Epoch: 7\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.92067\n",
      "\tVal Accuracy: 0.8973\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.89711\n",
      "\tVal Accuracy: 0.9005\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.89714\n",
      "\tVal Accuracy: 0.8991\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90019\n",
      "\tVal Accuracy: 0.9002\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.89784\n",
      "\tVal Accuracy: 0.9040\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90004\n",
      "\tVal Accuracy: 0.8990\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90164\n",
      "\tVal Accuracy: 0.9025\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.90346\n",
      "\tVal Accuracy: 0.9031\n",
      "Epoch: 8\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.88508\n",
      "\tVal Accuracy: 0.9003\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90604\n",
      "\tVal Accuracy: 0.8994\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.89744\n",
      "\tVal Accuracy: 0.9041\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90521\n",
      "\tVal Accuracy: 0.8962\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90235\n",
      "\tVal Accuracy: 0.8838\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90161\n",
      "\tVal Accuracy: 0.9025\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90422\n",
      "\tVal Accuracy: 0.9028\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.89858\n",
      "\tVal Accuracy: 0.9040\n",
      "Epoch: 9\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.89053\n",
      "\tVal Accuracy: 0.9021\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90273\n",
      "\tVal Accuracy: 0.9031\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.90493\n",
      "\tVal Accuracy: 0.9020\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90012\n",
      "\tVal Accuracy: 0.8987\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90627\n",
      "\tVal Accuracy: 0.9035\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.89876\n",
      "\tVal Accuracy: 0.8993\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90259\n",
      "\tVal Accuracy: 0.8983\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.89822\n",
      "\tVal Accuracy: 0.9018\n",
      "Epoch: 10\n",
      "Reducing LR\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.89470\n",
      "\tVal Accuracy: 0.9029\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90604\n",
      "\tVal Accuracy: 0.9042\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.90527\n",
      "\tVal Accuracy: 0.9048\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90727\n",
      "\tVal Accuracy: 0.9060\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90980\n",
      "\tVal Accuracy: 0.9056\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90336\n",
      "\tVal Accuracy: 0.9032\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90488\n",
      "\tVal Accuracy: 0.9061\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.90666\n",
      "\tVal Accuracy: 0.9052\n",
      "Epoch: 11\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.92897\n",
      "\tVal Accuracy: 0.9040\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90572\n",
      "\tVal Accuracy: 0.9042\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.90556\n",
      "\tVal Accuracy: 0.9063\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90577\n",
      "\tVal Accuracy: 0.9077\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90960\n",
      "\tVal Accuracy: 0.9044\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90575\n",
      "\tVal Accuracy: 0.9074\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90277\n",
      "\tVal Accuracy: 0.9015\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.91054\n",
      "\tVal Accuracy: 0.9068\n",
      "Epoch: 12\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.89233\n",
      "\tVal Accuracy: 0.9068\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90826\n",
      "\tVal Accuracy: 0.9070\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.90955\n",
      "\tVal Accuracy: 0.9083\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.91011\n",
      "\tVal Accuracy: 0.9071\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90560\n",
      "\tVal Accuracy: 0.9065\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90799\n",
      "\tVal Accuracy: 0.9069\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.91006\n",
      "\tVal Accuracy: 0.9062\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.90734\n",
      "\tVal Accuracy: 0.9066\n",
      "Epoch: 13\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.89949\n",
      "\tVal Accuracy: 0.9069\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.91316\n",
      "\tVal Accuracy: 0.9072\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.90509\n",
      "\tVal Accuracy: 0.9073\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.91271\n",
      "\tVal Accuracy: 0.9081\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.90764\n",
      "\tVal Accuracy: 0.9079\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90715\n",
      "\tVal Accuracy: 0.9072\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90736\n",
      "\tVal Accuracy: 0.9081\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.90866\n",
      "\tVal Accuracy: 0.9080\n",
      "Epoch: 14\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.89399\n",
      "\tVal Accuracy: 0.9085\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.90983\n",
      "\tVal Accuracy: 0.9082\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.91381\n",
      "\tVal Accuracy: 0.9073\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.90885\n",
      "\tVal Accuracy: 0.9075\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.91189\n",
      "\tVal Accuracy: 0.9072\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.90716\n",
      "\tVal Accuracy: 0.9054\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.90738\n",
      "\tVal Accuracy: 0.9049\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.91126\n",
      "\tVal Accuracy: 0.9070\n",
      "Final Training Accuracy: 0.9091\n",
      "Final Validation Accuracy: 0.9062\n",
      "Final Test Accuracy: 0.9001\n"
     ]
    }
   ],
   "source": [
    "# Create Model with specified optimizer and loss function\n",
    "##############################################################\n",
    "model = RCNN(config, len(dataset.vocab), dataset.word_embeddings)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.train()\n",
    "optimizer = optim.SGD(model.parameters(), lr=config.lr)\n",
    "NLLLoss = nn.NLLLoss()\n",
    "model.add_optimizer(optimizer)\n",
    "model.add_loss_op(NLLLoss)\n",
    "##############################################################\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for i in range(config.max_epochs):\n",
    "    print (\"Epoch: {}\".format(i))\n",
    "    train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n",
    "    train_losses.append(train_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "train_acc = evaluate_model(model, dataset.train_iterator)\n",
    "val_acc = evaluate_model(model, dataset.val_iterator)\n",
    "test_acc = evaluate_model(model, dataset.test_iterator)\n",
    "\n",
    "print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
    "print ('Final Validation Accuracy: {:.4f}'.format(val_acc))\n",
    "print ('Final Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
