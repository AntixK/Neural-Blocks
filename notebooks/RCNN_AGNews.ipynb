{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns             \n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "PATH ='/home/antixk/Anand/' #'/home/robot/Anand/'\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = PATH + \"NeuralBlocks/data_utils/datasets/AGnews/\"\n",
    "train_file + DATA_PATH + 'ag_news.train'\n",
    "test_file + DATA_PATH + 'ag_news.test'\n",
    "\n",
    "Word2Vec_file = DATA_PATH + 'glove.840B.300d.txt'\n",
    "\n",
    "class Config(object):\n",
    "    embed_size = 300\n",
    "    hidden_layers = 1\n",
    "    hidden_size = 64\n",
    "    output_size = 4\n",
    "    max_epochs = 15\n",
    "    hidden_size_linear = 64\n",
    "    lr = 0.5\n",
    "    batch_size = 128\n",
    "    seq_len = None # Sequence length for RNN\n",
    "    dropout_keep = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_iterator = None\n",
    "        self.test_iterator = None\n",
    "        self.val_iterator = None\n",
    "        self.vocab = []\n",
    "        self.word_embeddings = {}\n",
    "    \n",
    "    def parse_label(self, label):\n",
    "        '''\n",
    "        Get the actual labels from label string\n",
    "        Input:\n",
    "            label (string) : labels of the form '__label__2'\n",
    "        Returns:\n",
    "            label (int) : integer value corresponding to label string\n",
    "        '''\n",
    "        return int(label.strip()[-1])\n",
    "\n",
    "    def get_pandas_df(self, filename):\n",
    "        '''\n",
    "        Load the data into Pandas.DataFrame object\n",
    "        This will be used to convert data to torchtext object\n",
    "        '''\n",
    "        with open(filename, 'r') as datafile:     \n",
    "            data = [line.strip().split(',', maxsplit=1) for line in datafile]\n",
    "            data_text = list(map(lambda x: x[1], data))\n",
    "            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n",
    "\n",
    "        full_df = pd.DataFrame({\"text\":data_text, \"label\":data_label})\n",
    "        return full_df\n",
    "    \n",
    "    def load_data(self, w2v_file, train_file, test_file, val_file=None):\n",
    "        '''\n",
    "        Loads the data from files\n",
    "        Sets up iterators for training, validation and test data\n",
    "        Also create vocabulary and word embeddings based on the data\n",
    "        \n",
    "        Inputs:\n",
    "            w2v_file (String): absolute path to file containing word embeddings (GloVe/Word2Vec)\n",
    "            train_file (String): absolute path to training file\n",
    "            test_file (String): absolute path to test file\n",
    "            val_file (String): absolute path to validation file\n",
    "        '''\n",
    "\n",
    "        NLP = spacy.load('en')\n",
    "        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != \" \"]\n",
    "        \n",
    "        # Creating Field for data\n",
    "        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n",
    "        LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "        datafields = [(\"text\",TEXT),(\"label\",LABEL)]\n",
    "        \n",
    "        # Load data from pd.DataFrame into torchtext.data.Dataset\n",
    "        train_df = self.get_pandas_df(train_file)\n",
    "        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n",
    "        train_data = data.Dataset(train_examples, datafields)\n",
    "        \n",
    "        test_df = self.get_pandas_df(test_file)\n",
    "        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n",
    "        test_data = data.Dataset(test_examples, datafields)\n",
    "        \n",
    "        # If validation file exists, load it. Otherwise get validation data from training data\n",
    "        if val_file:\n",
    "            val_df = self.get_pandas_df(val_file)\n",
    "            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n",
    "            val_data = data.Dataset(val_examples, datafields)\n",
    "        else:\n",
    "            train_data, val_data = train_data.split(split_ratio=0.8)\n",
    "        \n",
    "        TEXT.build_vocab(train_data, vectors=Vectors(w2v_file))\n",
    "        self.word_embeddings = TEXT.vocab.vectors\n",
    "        self.vocab = TEXT.vocab\n",
    "        \n",
    "        self.train_iterator = data.BucketIterator(\n",
    "            (train_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=True)\n",
    "        \n",
    "        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (val_data, test_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=False)\n",
    "        \n",
    "        print (\"Loaded {} training examples\".format(len(train_data)))\n",
    "        print (\"Loaded {} test examples\".format(len(test_data)))\n",
    "        print (\"Loaded {} validation examples\".format(len(val_data)))\n",
    "\n",
    "\n",
    "def evaluate_model(model, iterator):\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for idx,batch in enumerate(iterator):\n",
    "        if torch.cuda.is_available():\n",
    "            x = batch.text.cuda()\n",
    "        else:\n",
    "            x = batch.text\n",
    "        y_pred = model(x)\n",
    "        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_y.extend(batch.label.numpy())\n",
    "    score = accuracy_score(all_y, np.array(all_preds).flatten())\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, config, vocab_size, word_embeddings, norm='BN'):\n",
    "        super(RCNN, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n",
    "        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
    "        \n",
    "        # Bi-directional LSTM for RCNN\n",
    "        self.lstm = torch.nn.Sequential()\n",
    "\n",
    "        if norm == '':\n",
    "            self.lstm.add_module('bn', nn.B)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.config.dropout_keep)\n",
    "        \n",
    "        # Linear layer to get \"convolution output\" to be passed to Pooling Layer\n",
    "        self.W = nn.Linear(\n",
    "            self.config.embed_size + 2*self.config.hidden_size,\n",
    "            self.config.hidden_size_linear\n",
    "        )\n",
    "        \n",
    "        # Tanh non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Fully-Connected Layer\n",
    "        self.fc = nn.Linear(\n",
    "            self.config.hidden_size_linear,\n",
    "            self.config.output_size\n",
    "        )\n",
    "        \n",
    "        # Softmax non-linearity\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape = (seq_len, batch_size)\n",
    "        embedded_sent = self.embeddings(x)\n",
    "        # embedded_sent.shape = (seq_len, batch_size, embed_size)\n",
    "\n",
    "        lstm_out, (h_n,c_n) = self.lstm(embedded_sent)\n",
    "        # lstm_out.shape = (seq_len, batch_size, 2 * hidden_size)\n",
    "        \n",
    "        input_features = torch.cat([lstm_out,embedded_sent], 2).permute(1,0,2)\n",
    "        # final_features.shape = (batch_size, seq_len, embed_size + 2*hidden_size)\n",
    "        \n",
    "        linear_output = self.tanh(\n",
    "            self.W(input_features)\n",
    "        )\n",
    "        # linear_output.shape = (batch_size, seq_len, hidden_size_linear)\n",
    "        \n",
    "        linear_output = linear_output.permute(0,2,1) # Reshaping fot max_pool\n",
    "        \n",
    "        max_out_features = F.max_pool1d(linear_output, linear_output.shape[2]).squeeze(2)\n",
    "        # max_out_features.shape = (batch_size, hidden_size_linear)\n",
    "        \n",
    "        max_out_features = self.dropout(max_out_features)\n",
    "        final_out = self.fc(max_out_features)\n",
    "        return self.softmax(final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(config)\n",
    "dataset.load_data(w2v_file, train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model with specified optimizer and loss function\n",
    "##############################################################\n",
    "model = RCNN(config, len(dataset.vocab), dataset.word_embeddings)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.train()\n",
    "optimizer = optim.SGD(model.parameters(), lr=config.lr)\n",
    "NLLLoss = nn.NLLLoss()\n",
    "model.add_optimizer(optimizer)\n",
    "model.add_loss_op(NLLLoss)\n",
    "##############################################################\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for i in range(config.max_epochs):\n",
    "    print (\"Epoch: {}\".format(i))\n",
    "    train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n",
    "    train_losses.append(train_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "train_acc = evaluate_model(model, dataset.train_iterator)\n",
    "val_acc = evaluate_model(model, dataset.val_iterator)\n",
    "test_acc = evaluate_model(model, dataset.test_iterator)\n",
    "\n",
    "print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
    "print ('Final Validation Accuracy: {:.4f}'.format(val_acc))\n",
    "print ('Final Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
