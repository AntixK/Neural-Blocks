{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/robot-i9/Desktop/Anand/NeuralBlocks/notebooks', '/home/robot-i9/Desktop/Anand/NeuralBlocks/notebooks', '', '/home/robot-i9/Downloads/pynaoqi-python2.7-2.5.5.5-linux64/lib/python2.7/site-packages', '/home/robot-i9/miniconda3/envs/main/lib/python37.zip', '/home/robot-i9/miniconda3/envs/main/lib/python3.7', '/home/robot-i9/miniconda3/envs/main/lib/python3.7/lib-dynload', '/home/robot-i9/miniconda3/envs/main/lib/python3.7/site-packages', '/home/robot-i9/miniconda3/envs/main/lib/python3.7/site-packages/IPython/extensions', '/home/robot-i9/.ipython', '/home/robot-i9/Desktop/Anand/']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm_notebook\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns             \n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sys.path.append('/home/robot-i9/Desktop/Anand/')\n",
    "print(sys.path)\n",
    "\n",
    "from NeuralBlocks.models.densenet import DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2456)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "np.random.seed(2456)\n",
    "\n",
    "NUM_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "CHECKPOINT_INTERVAL = 100\n",
    "LRS = [0.0001, 0.001, 0.1]\n",
    "NORMS =[None,'BN', 'SN', 'WN', 'MSN', 'MSNTReLU', 'WNTReLU']\n",
    "DATA_PATH = \"/home/robot-i9/Desktop/Anand/NeuralBlocks/data_utils/datasets/CIFAR10/\"\n",
    "SAVE_PATH = \"/home/robot-i9/Desktop/Anand/NeuralBlocks/experiments/CIFAR10/\"\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        train_loss_log.append(train_loss/(batch_idx+1))\n",
    "        train_acc_log.append( 100.*correct/total)\n",
    "        \n",
    "        if(batch_idx%CHECKPOINT_INTERVAL==0):\n",
    "             print(\"Train Epoch [{:3d}/{:3d}]Batch [{:3d}/{:3d}] Loss: {:.3f} Acc {:.3f}%\".format(epoch, NUM_EPOCH,batch_idx, len(trainloader),\n",
    "                train_loss/(batch_idx+1), 100.*correct/total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            test_loss_log.append(test_loss/(batch_idx+1))\n",
    "            test_acc_log.append( 100.*correct/total)\n",
    "        \n",
    "            if(batch_idx%CHECKPOINT_INTERVAL==0):\n",
    "                print(\"Test Epoch [{:3d}/{:3d}]Batch [{:3d}/{:3d}] Loss: {:.3f} Acc {:.3f}%\".format(epoch, NUM_EPOCH,batch_idx, len(testloader),\n",
    "                test_loss/(batch_idx+1), 100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(SAVE_PATH+'checkpoint'):\n",
    "            os.mkdir(SAVE_PATH+'checkpoint')\n",
    "        torch.save(state, SAVE_PATH+'checkpoint/ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a31940348a4b358807d5821b8c563a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9113282ca44ffaac82b061aed9377b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795f4cb7439246fe8da21442fa21b8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [  0/200]Batch [  0/391] Loss: 2.305 Acc 10.156%\n",
      "Train Epoch [  0/200]Batch [100/391] Loss: 2.172 Acc 21.488%\n",
      "Train Epoch [  0/200]Batch [200/391] Loss: 2.082 Acc 25.606%\n",
      "Train Epoch [  0/200]Batch [300/391] Loss: 2.024 Acc 27.871%\n",
      "Test Epoch [  0/200]Batch [  0/ 79] Loss: 1.747 Acc 43.750%\n",
      "Saving..\n",
      "Train Epoch [  1/200]Batch [  0/391] Loss: 1.790 Acc 40.625%\n",
      "Train Epoch [  1/200]Batch [100/391] Loss: 1.797 Acc 36.231%\n",
      "Train Epoch [  1/200]Batch [200/391] Loss: 1.765 Acc 37.725%\n",
      "Train Epoch [  1/200]Batch [300/391] Loss: 1.745 Acc 38.312%\n",
      "Test Epoch [  1/200]Batch [  0/ 79] Loss: 1.576 Acc 46.094%\n",
      "Saving..\n",
      "Train Epoch [  2/200]Batch [  0/391] Loss: 1.751 Acc 39.844%\n",
      "Train Epoch [  2/200]Batch [100/391] Loss: 1.661 Acc 40.671%\n",
      "Train Epoch [  2/200]Batch [200/391] Loss: 1.648 Acc 41.476%\n",
      "Train Epoch [  2/200]Batch [300/391] Loss: 1.642 Acc 41.554%\n",
      "Test Epoch [  2/200]Batch [  0/ 79] Loss: 1.513 Acc 50.000%\n",
      "Saving..\n",
      "Train Epoch [  3/200]Batch [  0/391] Loss: 1.588 Acc 46.094%\n",
      "Train Epoch [  3/200]Batch [100/391] Loss: 1.585 Acc 43.580%\n",
      "Train Epoch [  3/200]Batch [200/391] Loss: 1.591 Acc 43.058%\n",
      "Train Epoch [  3/200]Batch [300/391] Loss: 1.587 Acc 43.540%\n",
      "Test Epoch [  3/200]Batch [  0/ 79] Loss: 1.458 Acc 53.125%\n",
      "Saving..\n",
      "Train Epoch [  4/200]Batch [  0/391] Loss: 1.432 Acc 46.094%\n",
      "Train Epoch [  4/200]Batch [100/391] Loss: 1.547 Acc 45.514%\n",
      "Train Epoch [  4/200]Batch [200/391] Loss: 1.552 Acc 45.324%\n",
      "Train Epoch [  4/200]Batch [300/391] Loss: 1.552 Acc 45.175%\n",
      "Test Epoch [  4/200]Batch [  0/ 79] Loss: 1.438 Acc 53.906%\n",
      "Saving..\n",
      "Train Epoch [  5/200]Batch [  0/391] Loss: 1.568 Acc 41.406%\n",
      "Train Epoch [  5/200]Batch [100/391] Loss: 1.532 Acc 46.627%\n",
      "Train Epoch [  5/200]Batch [200/391] Loss: 1.526 Acc 46.735%\n",
      "Train Epoch [  5/200]Batch [300/391] Loss: 1.522 Acc 46.789%\n",
      "Test Epoch [  5/200]Batch [  0/ 79] Loss: 1.393 Acc 56.250%\n",
      "Saving..\n",
      "Train Epoch [  6/200]Batch [  0/391] Loss: 1.517 Acc 42.969%\n",
      "Train Epoch [  6/200]Batch [100/391] Loss: 1.487 Acc 47.958%\n",
      "Train Epoch [  6/200]Batch [200/391] Loss: 1.494 Acc 47.606%\n",
      "Train Epoch [  6/200]Batch [300/391] Loss: 1.494 Acc 47.648%\n",
      "Test Epoch [  6/200]Batch [  0/ 79] Loss: 1.360 Acc 53.906%\n",
      "Saving..\n",
      "Train Epoch [  7/200]Batch [  0/391] Loss: 1.432 Acc 50.000%\n",
      "Train Epoch [  7/200]Batch [100/391] Loss: 1.483 Acc 48.399%\n",
      "Train Epoch [  7/200]Batch [200/391] Loss: 1.475 Acc 48.519%\n",
      "Train Epoch [  7/200]Batch [300/391] Loss: 1.470 Acc 48.713%\n",
      "Test Epoch [  7/200]Batch [  0/ 79] Loss: 1.368 Acc 55.469%\n",
      "Saving..\n",
      "Train Epoch [  8/200]Batch [  0/391] Loss: 1.441 Acc 48.438%\n",
      "Train Epoch [  8/200]Batch [100/391] Loss: 1.456 Acc 48.871%\n",
      "Train Epoch [  8/200]Batch [200/391] Loss: 1.454 Acc 49.304%\n",
      "Train Epoch [  8/200]Batch [300/391] Loss: 1.452 Acc 49.429%\n",
      "Test Epoch [  8/200]Batch [  0/ 79] Loss: 1.306 Acc 60.938%\n",
      "Saving..\n",
      "Train Epoch [  9/200]Batch [  0/391] Loss: 1.431 Acc 50.781%\n",
      "Train Epoch [  9/200]Batch [100/391] Loss: 1.436 Acc 50.077%\n",
      "Train Epoch [  9/200]Batch [200/391] Loss: 1.426 Acc 50.661%\n",
      "Train Epoch [  9/200]Batch [300/391] Loss: 1.430 Acc 50.298%\n",
      "Test Epoch [  9/200]Batch [  0/ 79] Loss: 1.312 Acc 61.719%\n",
      "Saving..\n",
      "Train Epoch [ 10/200]Batch [  0/391] Loss: 1.361 Acc 53.125%\n",
      "Train Epoch [ 10/200]Batch [100/391] Loss: 1.421 Acc 50.905%\n",
      "Train Epoch [ 10/200]Batch [200/391] Loss: 1.418 Acc 50.882%\n",
      "Train Epoch [ 10/200]Batch [300/391] Loss: 1.411 Acc 51.147%\n",
      "Test Epoch [ 10/200]Batch [  0/ 79] Loss: 1.262 Acc 60.938%\n",
      "Saving..\n",
      "Train Epoch [ 11/200]Batch [  0/391] Loss: 1.373 Acc 52.344%\n",
      "Train Epoch [ 11/200]Batch [100/391] Loss: 1.414 Acc 51.153%\n",
      "Train Epoch [ 11/200]Batch [200/391] Loss: 1.399 Acc 51.722%\n",
      "Train Epoch [ 11/200]Batch [300/391] Loss: 1.398 Acc 51.560%\n",
      "Test Epoch [ 11/200]Batch [  0/ 79] Loss: 1.242 Acc 62.500%\n",
      "Saving..\n",
      "Train Epoch [ 12/200]Batch [  0/391] Loss: 1.507 Acc 45.312%\n",
      "Train Epoch [ 12/200]Batch [100/391] Loss: 1.372 Acc 52.498%\n",
      "Train Epoch [ 12/200]Batch [200/391] Loss: 1.378 Acc 52.258%\n",
      "Train Epoch [ 12/200]Batch [300/391] Loss: 1.381 Acc 52.157%\n",
      "Test Epoch [ 12/200]Batch [  0/ 79] Loss: 1.247 Acc 62.500%\n",
      "Saving..\n",
      "Train Epoch [ 13/200]Batch [  0/391] Loss: 1.309 Acc 58.594%\n",
      "Train Epoch [ 13/200]Batch [100/391] Loss: 1.363 Acc 53.164%\n",
      "Train Epoch [ 13/200]Batch [200/391] Loss: 1.367 Acc 52.931%\n",
      "Train Epoch [ 13/200]Batch [300/391] Loss: 1.366 Acc 52.936%\n",
      "Test Epoch [ 13/200]Batch [  0/ 79] Loss: 1.238 Acc 62.500%\n",
      "Saving..\n",
      "Train Epoch [ 14/200]Batch [  0/391] Loss: 1.309 Acc 54.688%\n",
      "Train Epoch [ 14/200]Batch [100/391] Loss: 1.358 Acc 52.506%\n",
      "Train Epoch [ 14/200]Batch [200/391] Loss: 1.345 Acc 53.172%\n",
      "Train Epoch [ 14/200]Batch [300/391] Loss: 1.352 Acc 53.037%\n",
      "Test Epoch [ 14/200]Batch [  0/ 79] Loss: 1.202 Acc 64.062%\n",
      "Saving..\n",
      "Train Epoch [ 15/200]Batch [  0/391] Loss: 1.336 Acc 57.031%\n",
      "Train Epoch [ 15/200]Batch [100/391] Loss: 1.336 Acc 53.844%\n",
      "Train Epoch [ 15/200]Batch [200/391] Loss: 1.336 Acc 53.638%\n",
      "Train Epoch [ 15/200]Batch [300/391] Loss: 1.341 Acc 53.442%\n",
      "Test Epoch [ 15/200]Batch [  0/ 79] Loss: 1.220 Acc 64.844%\n",
      "Saving..\n",
      "Train Epoch [ 16/200]Batch [  0/391] Loss: 1.409 Acc 50.000%\n",
      "Train Epoch [ 16/200]Batch [100/391] Loss: 1.342 Acc 53.767%\n",
      "Train Epoch [ 16/200]Batch [200/391] Loss: 1.340 Acc 53.630%\n",
      "Train Epoch [ 16/200]Batch [300/391] Loss: 1.337 Acc 53.761%\n",
      "Test Epoch [ 16/200]Batch [  0/ 79] Loss: 1.185 Acc 66.406%\n",
      "Saving..\n",
      "Train Epoch [ 17/200]Batch [  0/391] Loss: 1.225 Acc 61.719%\n",
      "Train Epoch [ 17/200]Batch [100/391] Loss: 1.317 Acc 54.448%\n",
      "Train Epoch [ 17/200]Batch [200/391] Loss: 1.325 Acc 54.314%\n",
      "Train Epoch [ 17/200]Batch [300/391] Loss: 1.325 Acc 54.080%\n",
      "Test Epoch [ 17/200]Batch [  0/ 79] Loss: 1.168 Acc 65.625%\n",
      "Saving..\n",
      "Train Epoch [ 18/200]Batch [  0/391] Loss: 1.343 Acc 51.562%\n",
      "Train Epoch [ 18/200]Batch [100/391] Loss: 1.325 Acc 53.984%\n",
      "Train Epoch [ 18/200]Batch [200/391] Loss: 1.319 Acc 54.509%\n",
      "Train Epoch [ 18/200]Batch [300/391] Loss: 1.319 Acc 54.425%\n",
      "Test Epoch [ 18/200]Batch [  0/ 79] Loss: 1.175 Acc 63.281%\n",
      "Train Epoch [ 19/200]Batch [  0/391] Loss: 1.397 Acc 53.125%\n",
      "Train Epoch [ 19/200]Batch [100/391] Loss: 1.299 Acc 54.889%\n",
      "Train Epoch [ 19/200]Batch [200/391] Loss: 1.302 Acc 54.812%\n",
      "Train Epoch [ 19/200]Batch [300/391] Loss: 1.315 Acc 54.459%\n",
      "Test Epoch [ 19/200]Batch [  0/ 79] Loss: 1.149 Acc 64.062%\n",
      "Saving..\n",
      "Train Epoch [ 20/200]Batch [  0/391] Loss: 1.483 Acc 56.250%\n",
      "Train Epoch [ 20/200]Batch [100/391] Loss: 1.303 Acc 54.734%\n",
      "Train Epoch [ 20/200]Batch [200/391] Loss: 1.304 Acc 54.544%\n",
      "Train Epoch [ 20/200]Batch [300/391] Loss: 1.300 Acc 54.716%\n",
      "Test Epoch [ 20/200]Batch [  0/ 79] Loss: 1.152 Acc 64.844%\n",
      "Saving..\n",
      "Train Epoch [ 21/200]Batch [  0/391] Loss: 1.256 Acc 57.031%\n",
      "Train Epoch [ 21/200]Batch [100/391] Loss: 1.321 Acc 54.038%\n",
      "Train Epoch [ 21/200]Batch [200/391] Loss: 1.311 Acc 54.427%\n",
      "Train Epoch [ 21/200]Batch [300/391] Loss: 1.300 Acc 54.846%\n",
      "Test Epoch [ 21/200]Batch [  0/ 79] Loss: 1.138 Acc 66.406%\n",
      "Saving..\n",
      "Train Epoch [ 22/200]Batch [  0/391] Loss: 1.294 Acc 53.906%\n",
      "Train Epoch [ 22/200]Batch [100/391] Loss: 1.281 Acc 55.956%\n",
      "Train Epoch [ 22/200]Batch [200/391] Loss: 1.285 Acc 55.414%\n",
      "Train Epoch [ 22/200]Batch [300/391] Loss: 1.284 Acc 55.588%\n",
      "Test Epoch [ 22/200]Batch [  0/ 79] Loss: 1.153 Acc 63.281%\n",
      "Train Epoch [ 23/200]Batch [  0/391] Loss: 1.235 Acc 57.812%\n",
      "Train Epoch [ 23/200]Batch [100/391] Loss: 1.277 Acc 56.327%\n",
      "Train Epoch [ 23/200]Batch [200/391] Loss: 1.292 Acc 55.581%\n",
      "Train Epoch [ 23/200]Batch [300/391] Loss: 1.283 Acc 55.765%\n",
      "Test Epoch [ 23/200]Batch [  0/ 79] Loss: 1.173 Acc 60.938%\n",
      "Train Epoch [ 24/200]Batch [  0/391] Loss: 1.387 Acc 54.688%\n",
      "Train Epoch [ 24/200]Batch [100/391] Loss: 1.295 Acc 54.950%\n",
      "Train Epoch [ 24/200]Batch [200/391] Loss: 1.285 Acc 55.578%\n",
      "Train Epoch [ 24/200]Batch [300/391] Loss: 1.280 Acc 55.785%\n",
      "Test Epoch [ 24/200]Batch [  0/ 79] Loss: 1.138 Acc 66.406%\n",
      "Saving..\n",
      "Train Epoch [ 25/200]Batch [  0/391] Loss: 1.140 Acc 60.938%\n",
      "Train Epoch [ 25/200]Batch [100/391] Loss: 1.264 Acc 56.474%\n",
      "Train Epoch [ 25/200]Batch [200/391] Loss: 1.261 Acc 56.639%\n",
      "Train Epoch [ 25/200]Batch [300/391] Loss: 1.263 Acc 56.439%\n",
      "Test Epoch [ 25/200]Batch [  0/ 79] Loss: 1.135 Acc 62.500%\n",
      "Saving..\n",
      "Train Epoch [ 26/200]Batch [  0/391] Loss: 1.197 Acc 53.125%\n",
      "Train Epoch [ 26/200]Batch [100/391] Loss: 1.263 Acc 56.103%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [ 26/200]Batch [200/391] Loss: 1.267 Acc 56.269%\n",
      "Train Epoch [ 26/200]Batch [300/391] Loss: 1.263 Acc 56.263%\n",
      "Test Epoch [ 26/200]Batch [  0/ 79] Loss: 1.131 Acc 60.938%\n",
      "Saving..\n",
      "Train Epoch [ 27/200]Batch [  0/391] Loss: 1.181 Acc 58.594%\n",
      "Train Epoch [ 27/200]Batch [100/391] Loss: 1.256 Acc 56.660%\n",
      "Train Epoch [ 27/200]Batch [200/391] Loss: 1.257 Acc 56.713%\n",
      "Train Epoch [ 27/200]Batch [300/391] Loss: 1.256 Acc 56.494%\n",
      "Test Epoch [ 27/200]Batch [  0/ 79] Loss: 1.135 Acc 61.719%\n",
      "Train Epoch [ 28/200]Batch [  0/391] Loss: 1.341 Acc 50.000%\n",
      "Train Epoch [ 28/200]Batch [100/391] Loss: 1.257 Acc 56.490%\n",
      "Train Epoch [ 28/200]Batch [200/391] Loss: 1.252 Acc 56.627%\n",
      "Train Epoch [ 28/200]Batch [300/391] Loss: 1.252 Acc 56.632%\n",
      "Test Epoch [ 28/200]Batch [  0/ 79] Loss: 1.103 Acc 64.844%\n",
      "Saving..\n",
      "Train Epoch [ 29/200]Batch [  0/391] Loss: 1.147 Acc 62.500%\n",
      "Train Epoch [ 29/200]Batch [100/391] Loss: 1.247 Acc 57.109%\n",
      "Train Epoch [ 29/200]Batch [200/391] Loss: 1.250 Acc 56.860%\n",
      "Train Epoch [ 29/200]Batch [300/391] Loss: 1.247 Acc 56.974%\n",
      "Test Epoch [ 29/200]Batch [  0/ 79] Loss: 1.094 Acc 64.062%\n",
      "Saving..\n",
      "Train Epoch [ 30/200]Batch [  0/391] Loss: 1.237 Acc 57.812%\n",
      "Train Epoch [ 30/200]Batch [100/391] Loss: 1.258 Acc 56.884%\n",
      "Train Epoch [ 30/200]Batch [200/391] Loss: 1.252 Acc 57.051%\n",
      "Train Epoch [ 30/200]Batch [300/391] Loss: 1.249 Acc 56.992%\n",
      "Test Epoch [ 30/200]Batch [  0/ 79] Loss: 1.119 Acc 62.500%\n",
      "Train Epoch [ 31/200]Batch [  0/391] Loss: 1.301 Acc 48.438%\n",
      "Train Epoch [ 31/200]Batch [100/391] Loss: 1.230 Acc 57.433%\n",
      "Train Epoch [ 31/200]Batch [200/391] Loss: 1.236 Acc 57.389%\n",
      "Train Epoch [ 31/200]Batch [300/391] Loss: 1.237 Acc 57.452%\n",
      "Test Epoch [ 31/200]Batch [  0/ 79] Loss: 1.080 Acc 64.062%\n",
      "Saving..\n",
      "Train Epoch [ 32/200]Batch [  0/391] Loss: 1.180 Acc 62.500%\n",
      "Train Epoch [ 32/200]Batch [100/391] Loss: 1.237 Acc 57.480%\n",
      "Train Epoch [ 32/200]Batch [200/391] Loss: 1.228 Acc 57.789%\n",
      "Train Epoch [ 32/200]Batch [300/391] Loss: 1.227 Acc 57.846%\n",
      "Test Epoch [ 32/200]Batch [  0/ 79] Loss: 1.102 Acc 64.062%\n",
      "Train Epoch [ 33/200]Batch [  0/391] Loss: 1.277 Acc 53.906%\n",
      "Train Epoch [ 33/200]Batch [100/391] Loss: 1.226 Acc 57.519%\n",
      "Train Epoch [ 33/200]Batch [200/391] Loss: 1.228 Acc 57.455%\n",
      "Train Epoch [ 33/200]Batch [300/391] Loss: 1.232 Acc 57.472%\n",
      "Test Epoch [ 33/200]Batch [  0/ 79] Loss: 1.081 Acc 64.844%\n",
      "Saving..\n",
      "Train Epoch [ 34/200]Batch [  0/391] Loss: 1.231 Acc 59.375%\n",
      "Train Epoch [ 34/200]Batch [100/391] Loss: 1.226 Acc 57.859%\n",
      "Train Epoch [ 34/200]Batch [200/391] Loss: 1.235 Acc 57.552%\n",
      "Train Epoch [ 34/200]Batch [300/391] Loss: 1.231 Acc 57.618%\n",
      "Test Epoch [ 34/200]Batch [  0/ 79] Loss: 1.102 Acc 66.406%\n",
      "Train Epoch [ 35/200]Batch [  0/391] Loss: 1.226 Acc 56.250%\n",
      "Train Epoch [ 35/200]Batch [100/391] Loss: 1.206 Acc 58.106%\n",
      "Train Epoch [ 35/200]Batch [200/391] Loss: 1.221 Acc 57.634%\n",
      "Train Epoch [ 35/200]Batch [300/391] Loss: 1.222 Acc 57.859%\n",
      "Test Epoch [ 35/200]Batch [  0/ 79] Loss: 1.077 Acc 62.500%\n",
      "Train Epoch [ 36/200]Batch [  0/391] Loss: 1.213 Acc 54.688%\n",
      "Train Epoch [ 36/200]Batch [100/391] Loss: 1.222 Acc 57.201%\n",
      "Train Epoch [ 36/200]Batch [200/391] Loss: 1.224 Acc 57.630%\n",
      "Train Epoch [ 36/200]Batch [300/391] Loss: 1.225 Acc 57.607%\n",
      "Test Epoch [ 36/200]Batch [  0/ 79] Loss: 1.061 Acc 68.750%\n",
      "Saving..\n",
      "Train Epoch [ 37/200]Batch [  0/391] Loss: 1.168 Acc 63.281%\n",
      "Train Epoch [ 37/200]Batch [100/391] Loss: 1.222 Acc 57.890%\n",
      "Train Epoch [ 37/200]Batch [200/391] Loss: 1.217 Acc 58.069%\n",
      "Train Epoch [ 37/200]Batch [300/391] Loss: 1.217 Acc 58.176%\n",
      "Test Epoch [ 37/200]Batch [  0/ 79] Loss: 1.078 Acc 62.500%\n",
      "Train Epoch [ 38/200]Batch [  0/391] Loss: 1.188 Acc 57.812%\n",
      "Train Epoch [ 38/200]Batch [100/391] Loss: 1.210 Acc 58.230%\n",
      "Train Epoch [ 38/200]Batch [200/391] Loss: 1.209 Acc 58.357%\n",
      "Train Epoch [ 38/200]Batch [300/391] Loss: 1.214 Acc 58.158%\n",
      "Test Epoch [ 38/200]Batch [  0/ 79] Loss: 1.093 Acc 63.281%\n",
      "Train Epoch [ 39/200]Batch [  0/391] Loss: 1.186 Acc 64.844%\n",
      "Train Epoch [ 39/200]Batch [100/391] Loss: 1.226 Acc 57.201%\n",
      "Train Epoch [ 39/200]Batch [200/391] Loss: 1.221 Acc 57.746%\n",
      "Train Epoch [ 39/200]Batch [300/391] Loss: 1.211 Acc 58.085%\n",
      "Test Epoch [ 39/200]Batch [  0/ 79] Loss: 1.057 Acc 64.844%\n",
      "Saving..\n",
      "Train Epoch [ 40/200]Batch [  0/391] Loss: 1.110 Acc 64.844%\n",
      "Train Epoch [ 40/200]Batch [100/391] Loss: 1.212 Acc 59.050%\n",
      "Train Epoch [ 40/200]Batch [200/391] Loss: 1.215 Acc 58.419%\n",
      "Train Epoch [ 40/200]Batch [300/391] Loss: 1.215 Acc 58.469%\n",
      "Test Epoch [ 40/200]Batch [  0/ 79] Loss: 1.055 Acc 66.406%\n",
      "Saving..\n",
      "Train Epoch [ 41/200]Batch [  0/391] Loss: 1.111 Acc 61.719%\n",
      "Train Epoch [ 41/200]Batch [100/391] Loss: 1.206 Acc 58.431%\n",
      "Train Epoch [ 41/200]Batch [200/391] Loss: 1.211 Acc 58.469%\n",
      "Train Epoch [ 41/200]Batch [300/391] Loss: 1.209 Acc 58.646%\n",
      "Test Epoch [ 41/200]Batch [  0/ 79] Loss: 1.071 Acc 66.406%\n",
      "Train Epoch [ 42/200]Batch [  0/391] Loss: 1.229 Acc 60.156%\n",
      "Train Epoch [ 42/200]Batch [100/391] Loss: 1.191 Acc 58.996%\n",
      "Train Epoch [ 42/200]Batch [200/391] Loss: 1.199 Acc 58.874%\n",
      "Train Epoch [ 42/200]Batch [300/391] Loss: 1.199 Acc 58.944%\n",
      "Test Epoch [ 42/200]Batch [  0/ 79] Loss: 1.075 Acc 64.844%\n",
      "Saving..\n",
      "Train Epoch [ 43/200]Batch [  0/391] Loss: 1.180 Acc 64.844%\n",
      "Train Epoch [ 43/200]Batch [100/391] Loss: 1.182 Acc 59.522%\n",
      "Train Epoch [ 43/200]Batch [200/391] Loss: 1.201 Acc 58.928%\n",
      "Train Epoch [ 43/200]Batch [300/391] Loss: 1.202 Acc 58.773%\n",
      "Test Epoch [ 43/200]Batch [  0/ 79] Loss: 1.042 Acc 65.625%\n",
      "Saving..\n",
      "Train Epoch [ 44/200]Batch [  0/391] Loss: 1.118 Acc 62.500%\n",
      "Train Epoch [ 44/200]Batch [100/391] Loss: 1.194 Acc 59.398%\n",
      "Train Epoch [ 44/200]Batch [200/391] Loss: 1.197 Acc 59.126%\n",
      "Train Epoch [ 44/200]Batch [300/391] Loss: 1.196 Acc 58.877%\n",
      "Test Epoch [ 44/200]Batch [  0/ 79] Loss: 1.062 Acc 65.625%\n",
      "Saving..\n",
      "Train Epoch [ 45/200]Batch [  0/391] Loss: 1.193 Acc 60.156%\n",
      "Train Epoch [ 45/200]Batch [100/391] Loss: 1.196 Acc 58.594%\n",
      "Train Epoch [ 45/200]Batch [200/391] Loss: 1.201 Acc 58.535%\n",
      "Train Epoch [ 45/200]Batch [300/391] Loss: 1.196 Acc 58.853%\n",
      "Test Epoch [ 45/200]Batch [  0/ 79] Loss: 1.056 Acc 64.062%\n",
      "Train Epoch [ 46/200]Batch [  0/391] Loss: 1.214 Acc 59.375%\n",
      "Train Epoch [ 46/200]Batch [100/391] Loss: 1.181 Acc 59.398%\n",
      "Train Epoch [ 46/200]Batch [200/391] Loss: 1.188 Acc 58.932%\n",
      "Train Epoch [ 46/200]Batch [300/391] Loss: 1.188 Acc 58.851%\n",
      "Test Epoch [ 46/200]Batch [  0/ 79] Loss: 1.026 Acc 64.062%\n",
      "Train Epoch [ 47/200]Batch [  0/391] Loss: 1.217 Acc 53.125%\n",
      "Train Epoch [ 47/200]Batch [100/391] Loss: 1.188 Acc 59.290%\n",
      "Train Epoch [ 47/200]Batch [200/391] Loss: 1.187 Acc 58.920%\n",
      "Train Epoch [ 47/200]Batch [300/391] Loss: 1.186 Acc 59.058%\n",
      "Test Epoch [ 47/200]Batch [  0/ 79] Loss: 1.048 Acc 67.188%\n",
      "Saving..\n",
      "Train Epoch [ 48/200]Batch [  0/391] Loss: 1.148 Acc 59.375%\n",
      "Train Epoch [ 48/200]Batch [100/391] Loss: 1.183 Acc 59.205%\n",
      "Train Epoch [ 48/200]Batch [200/391] Loss: 1.183 Acc 59.445%\n",
      "Train Epoch [ 48/200]Batch [300/391] Loss: 1.187 Acc 59.121%\n",
      "Test Epoch [ 48/200]Batch [  0/ 79] Loss: 1.034 Acc 67.188%\n",
      "Saving..\n",
      "Train Epoch [ 49/200]Batch [  0/391] Loss: 1.170 Acc 58.594%\n",
      "Train Epoch [ 49/200]Batch [100/391] Loss: 1.175 Acc 59.638%\n",
      "Train Epoch [ 49/200]Batch [200/391] Loss: 1.180 Acc 59.332%\n",
      "Train Epoch [ 49/200]Batch [300/391] Loss: 1.178 Acc 59.349%\n",
      "Test Epoch [ 49/200]Batch [  0/ 79] Loss: 1.066 Acc 63.281%\n",
      "Train Epoch [ 50/200]Batch [  0/391] Loss: 1.234 Acc 57.031%\n",
      "Train Epoch [ 50/200]Batch [100/391] Loss: 1.174 Acc 59.352%\n",
      "Train Epoch [ 50/200]Batch [200/391] Loss: 1.181 Acc 59.422%\n",
      "Train Epoch [ 50/200]Batch [300/391] Loss: 1.177 Acc 59.590%\n",
      "Test Epoch [ 50/200]Batch [  0/ 79] Loss: 1.033 Acc 65.625%\n",
      "Train Epoch [ 51/200]Batch [  0/391] Loss: 1.136 Acc 62.500%\n",
      "Train Epoch [ 51/200]Batch [100/391] Loss: 1.166 Acc 59.847%\n",
      "Train Epoch [ 51/200]Batch [200/391] Loss: 1.177 Acc 59.356%\n",
      "Train Epoch [ 51/200]Batch [300/391] Loss: 1.175 Acc 59.458%\n",
      "Test Epoch [ 51/200]Batch [  0/ 79] Loss: 1.047 Acc 65.625%\n",
      "Train Epoch [ 52/200]Batch [  0/391] Loss: 1.248 Acc 62.500%\n",
      "Train Epoch [ 52/200]Batch [100/391] Loss: 1.167 Acc 60.179%\n",
      "Train Epoch [ 52/200]Batch [200/391] Loss: 1.174 Acc 59.538%\n",
      "Train Epoch [ 52/200]Batch [300/391] Loss: 1.174 Acc 59.583%\n",
      "Test Epoch [ 52/200]Batch [  0/ 79] Loss: 1.006 Acc 66.406%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Train Epoch [ 53/200]Batch [  0/391] Loss: 1.135 Acc 60.156%\n",
      "Train Epoch [ 53/200]Batch [100/391] Loss: 1.163 Acc 59.793%\n",
      "Train Epoch [ 53/200]Batch [200/391] Loss: 1.172 Acc 59.527%\n",
      "Train Epoch [ 53/200]Batch [300/391] Loss: 1.172 Acc 59.346%\n",
      "Test Epoch [ 53/200]Batch [  0/ 79] Loss: 1.026 Acc 66.406%\n",
      "Train Epoch [ 54/200]Batch [  0/391] Loss: 1.163 Acc 56.250%\n",
      "Train Epoch [ 54/200]Batch [100/391] Loss: 1.170 Acc 59.994%\n",
      "Train Epoch [ 54/200]Batch [200/391] Loss: 1.167 Acc 59.857%\n",
      "Train Epoch [ 54/200]Batch [300/391] Loss: 1.165 Acc 59.702%\n",
      "Test Epoch [ 54/200]Batch [  0/ 79] Loss: 1.025 Acc 67.188%\n",
      "Saving..\n",
      "Train Epoch [ 55/200]Batch [  0/391] Loss: 1.270 Acc 52.344%\n",
      "Train Epoch [ 55/200]Batch [100/391] Loss: 1.161 Acc 60.350%\n",
      "Train Epoch [ 55/200]Batch [200/391] Loss: 1.168 Acc 59.534%\n",
      "Train Epoch [ 55/200]Batch [300/391] Loss: 1.166 Acc 59.850%\n",
      "Test Epoch [ 55/200]Batch [  0/ 79] Loss: 1.000 Acc 64.844%\n",
      "Saving..\n",
      "Train Epoch [ 56/200]Batch [  0/391] Loss: 0.982 Acc 75.781%\n",
      "Train Epoch [ 56/200]Batch [100/391] Loss: 1.170 Acc 59.839%\n",
      "Train Epoch [ 56/200]Batch [200/391] Loss: 1.168 Acc 59.822%\n",
      "Train Epoch [ 56/200]Batch [300/391] Loss: 1.168 Acc 59.936%\n",
      "Test Epoch [ 56/200]Batch [  0/ 79] Loss: 0.999 Acc 67.188%\n",
      "Saving..\n",
      "Train Epoch [ 57/200]Batch [  0/391] Loss: 1.177 Acc 58.594%\n",
      "Train Epoch [ 57/200]Batch [100/391] Loss: 1.176 Acc 59.089%\n",
      "Train Epoch [ 57/200]Batch [200/391] Loss: 1.163 Acc 59.674%\n",
      "Train Epoch [ 57/200]Batch [300/391] Loss: 1.165 Acc 59.681%\n",
      "Test Epoch [ 57/200]Batch [  0/ 79] Loss: 1.003 Acc 65.625%\n",
      "Train Epoch [ 58/200]Batch [  0/391] Loss: 1.216 Acc 61.719%\n",
      "Train Epoch [ 58/200]Batch [100/391] Loss: 1.165 Acc 60.172%\n",
      "Train Epoch [ 58/200]Batch [200/391] Loss: 1.157 Acc 60.242%\n",
      "Train Epoch [ 58/200]Batch [300/391] Loss: 1.151 Acc 60.501%\n",
      "Test Epoch [ 58/200]Batch [  0/ 79] Loss: 0.995 Acc 67.188%\n",
      "Train Epoch [ 59/200]Batch [  0/391] Loss: 1.279 Acc 60.156%\n",
      "Train Epoch [ 59/200]Batch [100/391] Loss: 1.150 Acc 60.435%\n",
      "Train Epoch [ 59/200]Batch [200/391] Loss: 1.153 Acc 60.471%\n",
      "Train Epoch [ 59/200]Batch [300/391] Loss: 1.159 Acc 60.294%\n",
      "Test Epoch [ 59/200]Batch [  0/ 79] Loss: 0.998 Acc 66.406%\n",
      "Train Epoch [ 60/200]Batch [  0/391] Loss: 1.214 Acc 60.938%\n",
      "Train Epoch [ 60/200]Batch [100/391] Loss: 1.156 Acc 60.520%\n",
      "Train Epoch [ 60/200]Batch [200/391] Loss: 1.157 Acc 60.292%\n",
      "Train Epoch [ 60/200]Batch [300/391] Loss: 1.151 Acc 60.348%\n",
      "Test Epoch [ 60/200]Batch [  0/ 79] Loss: 1.005 Acc 66.406%\n",
      "Train Epoch [ 61/200]Batch [  0/391] Loss: 1.001 Acc 65.625%\n",
      "Train Epoch [ 61/200]Batch [100/391] Loss: 1.159 Acc 60.342%\n",
      "Train Epoch [ 61/200]Batch [200/391] Loss: 1.151 Acc 60.580%\n",
      "Train Epoch [ 61/200]Batch [300/391] Loss: 1.150 Acc 60.457%\n",
      "Test Epoch [ 61/200]Batch [  0/ 79] Loss: 1.012 Acc 67.969%\n",
      "Train Epoch [ 62/200]Batch [  0/391] Loss: 1.083 Acc 57.812%\n",
      "Train Epoch [ 62/200]Batch [100/391] Loss: 1.152 Acc 60.442%\n",
      "Train Epoch [ 62/200]Batch [200/391] Loss: 1.153 Acc 60.261%\n",
      "Train Epoch [ 62/200]Batch [300/391] Loss: 1.151 Acc 60.268%\n",
      "Test Epoch [ 62/200]Batch [  0/ 79] Loss: 0.969 Acc 66.406%\n",
      "Train Epoch [ 63/200]Batch [  0/391] Loss: 1.141 Acc 62.500%\n",
      "Train Epoch [ 63/200]Batch [100/391] Loss: 1.148 Acc 60.427%\n",
      "Train Epoch [ 63/200]Batch [200/391] Loss: 1.146 Acc 60.549%\n",
      "Train Epoch [ 63/200]Batch [300/391] Loss: 1.143 Acc 60.527%\n",
      "Test Epoch [ 63/200]Batch [  0/ 79] Loss: 1.008 Acc 67.969%\n",
      "Train Epoch [ 64/200]Batch [  0/391] Loss: 1.105 Acc 58.594%\n",
      "Train Epoch [ 64/200]Batch [100/391] Loss: 1.148 Acc 60.280%\n",
      "Train Epoch [ 64/200]Batch [200/391] Loss: 1.145 Acc 60.401%\n",
      "Train Epoch [ 64/200]Batch [300/391] Loss: 1.149 Acc 60.385%\n",
      "Test Epoch [ 64/200]Batch [  0/ 79] Loss: 0.984 Acc 67.969%\n",
      "Train Epoch [ 65/200]Batch [  0/391] Loss: 1.193 Acc 56.250%\n",
      "Train Epoch [ 65/200]Batch [100/391] Loss: 1.157 Acc 59.808%\n",
      "Train Epoch [ 65/200]Batch [200/391] Loss: 1.152 Acc 60.362%\n",
      "Train Epoch [ 65/200]Batch [300/391] Loss: 1.149 Acc 60.424%\n",
      "Test Epoch [ 65/200]Batch [  0/ 79] Loss: 0.997 Acc 66.406%\n",
      "Train Epoch [ 66/200]Batch [  0/391] Loss: 1.112 Acc 56.250%\n",
      "Train Epoch [ 66/200]Batch [100/391] Loss: 1.142 Acc 60.504%\n",
      "Train Epoch [ 66/200]Batch [200/391] Loss: 1.141 Acc 60.747%\n",
      "Train Epoch [ 66/200]Batch [300/391] Loss: 1.143 Acc 60.823%\n",
      "Test Epoch [ 66/200]Batch [  0/ 79] Loss: 0.998 Acc 66.406%\n",
      "Train Epoch [ 67/200]Batch [  0/391] Loss: 1.189 Acc 57.031%\n",
      "Train Epoch [ 67/200]Batch [100/391] Loss: 1.141 Acc 60.628%\n",
      "Train Epoch [ 67/200]Batch [200/391] Loss: 1.147 Acc 60.553%\n",
      "Train Epoch [ 67/200]Batch [300/391] Loss: 1.147 Acc 60.491%\n",
      "Test Epoch [ 67/200]Batch [  0/ 79] Loss: 0.995 Acc 67.188%\n",
      "Saving..\n",
      "Train Epoch [ 68/200]Batch [  0/391] Loss: 1.101 Acc 65.625%\n",
      "Train Epoch [ 68/200]Batch [100/391] Loss: 1.141 Acc 60.404%\n",
      "Train Epoch [ 68/200]Batch [200/391] Loss: 1.141 Acc 60.743%\n",
      "Train Epoch [ 68/200]Batch [300/391] Loss: 1.138 Acc 60.943%\n",
      "Test Epoch [ 68/200]Batch [  0/ 79] Loss: 1.003 Acc 69.531%\n",
      "Train Epoch [ 69/200]Batch [  0/391] Loss: 1.052 Acc 57.031%\n",
      "Train Epoch [ 69/200]Batch [100/391] Loss: 1.137 Acc 61.185%\n",
      "Train Epoch [ 69/200]Batch [200/391] Loss: 1.143 Acc 60.957%\n",
      "Train Epoch [ 69/200]Batch [300/391] Loss: 1.142 Acc 60.831%\n",
      "Test Epoch [ 69/200]Batch [  0/ 79] Loss: 0.970 Acc 65.625%\n",
      "Train Epoch [ 70/200]Batch [  0/391] Loss: 1.296 Acc 61.719%\n",
      "Train Epoch [ 70/200]Batch [100/391] Loss: 1.144 Acc 60.156%\n",
      "Train Epoch [ 70/200]Batch [200/391] Loss: 1.139 Acc 60.615%\n",
      "Train Epoch [ 70/200]Batch [300/391] Loss: 1.134 Acc 60.803%\n",
      "Test Epoch [ 70/200]Batch [  0/ 79] Loss: 0.975 Acc 67.188%\n",
      "Train Epoch [ 71/200]Batch [  0/391] Loss: 0.903 Acc 71.875%\n",
      "Train Epoch [ 71/200]Batch [100/391] Loss: 1.128 Acc 61.262%\n",
      "Train Epoch [ 71/200]Batch [200/391] Loss: 1.136 Acc 61.136%\n",
      "Train Epoch [ 71/200]Batch [300/391] Loss: 1.134 Acc 61.223%\n",
      "Test Epoch [ 71/200]Batch [  0/ 79] Loss: 0.985 Acc 66.406%\n",
      "Train Epoch [ 72/200]Batch [  0/391] Loss: 1.149 Acc 63.281%\n",
      "Train Epoch [ 72/200]Batch [100/391] Loss: 1.130 Acc 61.989%\n",
      "Train Epoch [ 72/200]Batch [200/391] Loss: 1.129 Acc 61.517%\n",
      "Train Epoch [ 72/200]Batch [300/391] Loss: 1.128 Acc 61.449%\n",
      "Test Epoch [ 72/200]Batch [  0/ 79] Loss: 0.964 Acc 65.625%\n",
      "Train Epoch [ 73/200]Batch [  0/391] Loss: 1.046 Acc 61.719%\n",
      "Train Epoch [ 73/200]Batch [100/391] Loss: 1.131 Acc 61.030%\n",
      "Train Epoch [ 73/200]Batch [200/391] Loss: 1.135 Acc 60.918%\n",
      "Train Epoch [ 73/200]Batch [300/391] Loss: 1.136 Acc 61.018%\n",
      "Test Epoch [ 73/200]Batch [  0/ 79] Loss: 0.969 Acc 67.969%\n",
      "Saving..\n",
      "Train Epoch [ 74/200]Batch [  0/391] Loss: 1.277 Acc 55.469%\n",
      "Train Epoch [ 74/200]Batch [100/391] Loss: 1.136 Acc 60.504%\n",
      "Train Epoch [ 74/200]Batch [200/391] Loss: 1.132 Acc 60.961%\n",
      "Train Epoch [ 74/200]Batch [300/391] Loss: 1.126 Acc 61.231%\n",
      "Test Epoch [ 74/200]Batch [  0/ 79] Loss: 0.976 Acc 64.844%\n",
      "Saving..\n",
      "Train Epoch [ 75/200]Batch [  0/391] Loss: 1.077 Acc 64.062%\n",
      "Train Epoch [ 75/200]Batch [100/391] Loss: 1.130 Acc 60.783%\n",
      "Train Epoch [ 75/200]Batch [200/391] Loss: 1.130 Acc 60.903%\n",
      "Train Epoch [ 75/200]Batch [300/391] Loss: 1.123 Acc 61.303%\n",
      "Test Epoch [ 75/200]Batch [  0/ 79] Loss: 0.991 Acc 67.969%\n",
      "Train Epoch [ 76/200]Batch [  0/391] Loss: 0.985 Acc 64.844%\n",
      "Train Epoch [ 76/200]Batch [100/391] Loss: 1.117 Acc 61.417%\n",
      "Train Epoch [ 76/200]Batch [200/391] Loss: 1.125 Acc 61.151%\n",
      "Train Epoch [ 76/200]Batch [300/391] Loss: 1.129 Acc 60.971%\n",
      "Test Epoch [ 76/200]Batch [  0/ 79] Loss: 0.985 Acc 65.625%\n",
      "Train Epoch [ 77/200]Batch [  0/391] Loss: 1.138 Acc 60.156%\n",
      "Train Epoch [ 77/200]Batch [100/391] Loss: 1.133 Acc 60.961%\n",
      "Train Epoch [ 77/200]Batch [200/391] Loss: 1.131 Acc 61.015%\n",
      "Train Epoch [ 77/200]Batch [300/391] Loss: 1.131 Acc 60.950%\n",
      "Test Epoch [ 77/200]Batch [  0/ 79] Loss: 0.958 Acc 67.188%\n",
      "Train Epoch [ 78/200]Batch [  0/391] Loss: 1.030 Acc 64.062%\n",
      "Train Epoch [ 78/200]Batch [100/391] Loss: 1.104 Acc 61.920%\n",
      "Train Epoch [ 78/200]Batch [200/391] Loss: 1.117 Acc 61.556%\n",
      "Train Epoch [ 78/200]Batch [300/391] Loss: 1.122 Acc 61.472%\n",
      "Test Epoch [ 78/200]Batch [  0/ 79] Loss: 0.961 Acc 68.750%\n",
      "Train Epoch [ 79/200]Batch [  0/391] Loss: 1.166 Acc 57.031%\n",
      "Train Epoch [ 79/200]Batch [100/391] Loss: 1.121 Acc 61.030%\n",
      "Train Epoch [ 79/200]Batch [200/391] Loss: 1.122 Acc 61.346%\n",
      "Train Epoch [ 79/200]Batch [300/391] Loss: 1.124 Acc 61.485%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [ 79/200]Batch [  0/ 79] Loss: 0.960 Acc 66.406%\n",
      "Train Epoch [ 80/200]Batch [  0/391] Loss: 1.133 Acc 57.812%\n",
      "Train Epoch [ 80/200]Batch [100/391] Loss: 1.147 Acc 60.783%\n",
      "Train Epoch [ 80/200]Batch [200/391] Loss: 1.131 Acc 61.140%\n",
      "Train Epoch [ 80/200]Batch [300/391] Loss: 1.124 Acc 61.363%\n",
      "Test Epoch [ 80/200]Batch [  0/ 79] Loss: 0.985 Acc 64.844%\n",
      "Train Epoch [ 81/200]Batch [  0/391] Loss: 1.052 Acc 65.625%\n",
      "Train Epoch [ 81/200]Batch [100/391] Loss: 1.120 Acc 61.781%\n",
      "Train Epoch [ 81/200]Batch [200/391] Loss: 1.118 Acc 61.738%\n",
      "Train Epoch [ 81/200]Batch [300/391] Loss: 1.117 Acc 61.625%\n",
      "Test Epoch [ 81/200]Batch [  0/ 79] Loss: 0.953 Acc 66.406%\n",
      "Saving..\n",
      "Train Epoch [ 82/200]Batch [  0/391] Loss: 1.066 Acc 62.500%\n",
      "Train Epoch [ 82/200]Batch [100/391] Loss: 1.119 Acc 61.951%\n",
      "Train Epoch [ 82/200]Batch [200/391] Loss: 1.119 Acc 61.559%\n",
      "Train Epoch [ 82/200]Batch [300/391] Loss: 1.117 Acc 61.599%\n",
      "Test Epoch [ 82/200]Batch [  0/ 79] Loss: 0.967 Acc 66.406%\n",
      "Train Epoch [ 83/200]Batch [  0/391] Loss: 1.163 Acc 62.500%\n",
      "Train Epoch [ 83/200]Batch [100/391] Loss: 1.116 Acc 61.843%\n",
      "Train Epoch [ 83/200]Batch [200/391] Loss: 1.117 Acc 61.645%\n",
      "Train Epoch [ 83/200]Batch [300/391] Loss: 1.115 Acc 61.703%\n",
      "Test Epoch [ 83/200]Batch [  0/ 79] Loss: 0.973 Acc 67.969%\n",
      "Train Epoch [ 84/200]Batch [  0/391] Loss: 1.210 Acc 62.500%\n",
      "Train Epoch [ 84/200]Batch [100/391] Loss: 1.120 Acc 61.580%\n",
      "Train Epoch [ 84/200]Batch [200/391] Loss: 1.114 Acc 61.451%\n",
      "Train Epoch [ 84/200]Batch [300/391] Loss: 1.119 Acc 61.361%\n",
      "Test Epoch [ 84/200]Batch [  0/ 79] Loss: 0.963 Acc 66.406%\n",
      "Train Epoch [ 85/200]Batch [  0/391] Loss: 1.213 Acc 58.594%\n",
      "Train Epoch [ 85/200]Batch [100/391] Loss: 1.110 Acc 62.392%\n",
      "Train Epoch [ 85/200]Batch [200/391] Loss: 1.116 Acc 61.664%\n",
      "Train Epoch [ 85/200]Batch [300/391] Loss: 1.112 Acc 61.716%\n",
      "Test Epoch [ 85/200]Batch [  0/ 79] Loss: 0.954 Acc 67.969%\n",
      "Saving..\n",
      "Train Epoch [ 86/200]Batch [  0/391] Loss: 1.107 Acc 58.594%\n",
      "Train Epoch [ 86/200]Batch [100/391] Loss: 1.106 Acc 61.850%\n",
      "Train Epoch [ 86/200]Batch [200/391] Loss: 1.103 Acc 61.843%\n",
      "Train Epoch [ 86/200]Batch [300/391] Loss: 1.107 Acc 61.784%\n",
      "Test Epoch [ 86/200]Batch [  0/ 79] Loss: 0.927 Acc 68.750%\n",
      "Saving..\n",
      "Train Epoch [ 87/200]Batch [  0/391] Loss: 0.952 Acc 67.188%\n",
      "Train Epoch [ 87/200]Batch [100/391] Loss: 1.089 Acc 62.608%\n",
      "Train Epoch [ 87/200]Batch [200/391] Loss: 1.106 Acc 61.956%\n",
      "Train Epoch [ 87/200]Batch [300/391] Loss: 1.110 Acc 61.851%\n",
      "Test Epoch [ 87/200]Batch [  0/ 79] Loss: 0.947 Acc 67.969%\n",
      "Train Epoch [ 88/200]Batch [  0/391] Loss: 1.240 Acc 57.812%\n",
      "Train Epoch [ 88/200]Batch [100/391] Loss: 1.117 Acc 61.587%\n",
      "Train Epoch [ 88/200]Batch [200/391] Loss: 1.110 Acc 61.851%\n",
      "Train Epoch [ 88/200]Batch [300/391] Loss: 1.110 Acc 61.877%\n",
      "Test Epoch [ 88/200]Batch [  0/ 79] Loss: 0.948 Acc 68.750%\n",
      "Train Epoch [ 89/200]Batch [  0/391] Loss: 1.145 Acc 64.062%\n",
      "Train Epoch [ 89/200]Batch [100/391] Loss: 1.120 Acc 61.309%\n",
      "Train Epoch [ 89/200]Batch [200/391] Loss: 1.113 Acc 61.824%\n",
      "Train Epoch [ 89/200]Batch [300/391] Loss: 1.107 Acc 61.887%\n",
      "Test Epoch [ 89/200]Batch [  0/ 79] Loss: 0.958 Acc 67.969%\n",
      "Train Epoch [ 90/200]Batch [  0/391] Loss: 0.997 Acc 67.188%\n",
      "Train Epoch [ 90/200]Batch [100/391] Loss: 1.097 Acc 62.152%\n",
      "Train Epoch [ 90/200]Batch [200/391] Loss: 1.098 Acc 62.119%\n",
      "Train Epoch [ 90/200]Batch [300/391] Loss: 1.105 Acc 61.895%\n",
      "Test Epoch [ 90/200]Batch [  0/ 79] Loss: 0.971 Acc 65.625%\n",
      "Train Epoch [ 91/200]Batch [  0/391] Loss: 1.189 Acc 55.469%\n",
      "Train Epoch [ 91/200]Batch [100/391] Loss: 1.113 Acc 61.850%\n",
      "Train Epoch [ 91/200]Batch [200/391] Loss: 1.115 Acc 61.796%\n",
      "Train Epoch [ 91/200]Batch [300/391] Loss: 1.108 Acc 62.098%\n",
      "Test Epoch [ 91/200]Batch [  0/ 79] Loss: 0.929 Acc 67.188%\n",
      "Saving..\n",
      "Train Epoch [ 92/200]Batch [  0/391] Loss: 1.216 Acc 57.812%\n",
      "Train Epoch [ 92/200]Batch [100/391] Loss: 1.107 Acc 61.827%\n",
      "Train Epoch [ 92/200]Batch [200/391] Loss: 1.105 Acc 61.828%\n",
      "Train Epoch [ 92/200]Batch [300/391] Loss: 1.103 Acc 62.077%\n",
      "Test Epoch [ 92/200]Batch [  0/ 79] Loss: 0.965 Acc 64.844%\n",
      "Train Epoch [ 93/200]Batch [  0/391] Loss: 1.033 Acc 62.500%\n",
      "Train Epoch [ 93/200]Batch [100/391] Loss: 1.110 Acc 61.827%\n",
      "Train Epoch [ 93/200]Batch [200/391] Loss: 1.110 Acc 61.828%\n",
      "Train Epoch [ 93/200]Batch [300/391] Loss: 1.103 Acc 62.007%\n",
      "Test Epoch [ 93/200]Batch [  0/ 79] Loss: 0.948 Acc 67.188%\n",
      "Train Epoch [ 94/200]Batch [  0/391] Loss: 1.063 Acc 61.719%\n",
      "Train Epoch [ 94/200]Batch [100/391] Loss: 1.094 Acc 62.283%\n",
      "Train Epoch [ 94/200]Batch [200/391] Loss: 1.094 Acc 62.473%\n",
      "Train Epoch [ 94/200]Batch [300/391] Loss: 1.099 Acc 62.264%\n",
      "Test Epoch [ 94/200]Batch [  0/ 79] Loss: 0.949 Acc 68.750%\n",
      "Train Epoch [ 95/200]Batch [  0/391] Loss: 1.031 Acc 63.281%\n",
      "Train Epoch [ 95/200]Batch [100/391] Loss: 1.087 Acc 62.740%\n",
      "Train Epoch [ 95/200]Batch [200/391] Loss: 1.099 Acc 62.527%\n",
      "Train Epoch [ 95/200]Batch [300/391] Loss: 1.098 Acc 62.471%\n",
      "Test Epoch [ 95/200]Batch [  0/ 79] Loss: 0.935 Acc 67.969%\n",
      "Train Epoch [ 96/200]Batch [  0/391] Loss: 0.950 Acc 66.406%\n",
      "Train Epoch [ 96/200]Batch [100/391] Loss: 1.095 Acc 62.090%\n",
      "Train Epoch [ 96/200]Batch [200/391] Loss: 1.099 Acc 62.006%\n",
      "Train Epoch [ 96/200]Batch [300/391] Loss: 1.096 Acc 62.451%\n",
      "Test Epoch [ 96/200]Batch [  0/ 79] Loss: 0.914 Acc 67.969%\n",
      "Train Epoch [ 97/200]Batch [  0/391] Loss: 1.028 Acc 67.188%\n",
      "Train Epoch [ 97/200]Batch [100/391] Loss: 1.114 Acc 61.688%\n",
      "Train Epoch [ 97/200]Batch [200/391] Loss: 1.105 Acc 62.100%\n",
      "Train Epoch [ 97/200]Batch [300/391] Loss: 1.102 Acc 62.282%\n",
      "Test Epoch [ 97/200]Batch [  0/ 79] Loss: 0.945 Acc 67.969%\n",
      "Train Epoch [ 98/200]Batch [  0/391] Loss: 1.106 Acc 60.938%\n",
      "Train Epoch [ 98/200]Batch [100/391] Loss: 1.101 Acc 62.183%\n",
      "Train Epoch [ 98/200]Batch [200/391] Loss: 1.098 Acc 62.523%\n",
      "Train Epoch [ 98/200]Batch [300/391] Loss: 1.098 Acc 62.456%\n",
      "Test Epoch [ 98/200]Batch [  0/ 79] Loss: 0.951 Acc 67.188%\n",
      "Train Epoch [ 99/200]Batch [  0/391] Loss: 1.092 Acc 57.031%\n",
      "Train Epoch [ 99/200]Batch [100/391] Loss: 1.080 Acc 63.297%\n",
      "Train Epoch [ 99/200]Batch [200/391] Loss: 1.087 Acc 62.935%\n",
      "Train Epoch [ 99/200]Batch [300/391] Loss: 1.098 Acc 62.326%\n",
      "Test Epoch [ 99/200]Batch [  0/ 79] Loss: 0.945 Acc 67.969%\n",
      "Train Epoch [100/200]Batch [  0/391] Loss: 0.980 Acc 70.312%\n",
      "Train Epoch [100/200]Batch [100/391] Loss: 1.101 Acc 62.508%\n",
      "Train Epoch [100/200]Batch [200/391] Loss: 1.102 Acc 62.080%\n",
      "Train Epoch [100/200]Batch [300/391] Loss: 1.103 Acc 62.144%\n",
      "Test Epoch [100/200]Batch [  0/ 79] Loss: 0.960 Acc 66.406%\n",
      "Train Epoch [101/200]Batch [  0/391] Loss: 1.128 Acc 57.031%\n",
      "Train Epoch [101/200]Batch [100/391] Loss: 1.095 Acc 62.585%\n",
      "Train Epoch [101/200]Batch [200/391] Loss: 1.093 Acc 62.644%\n",
      "Train Epoch [101/200]Batch [300/391] Loss: 1.095 Acc 62.718%\n",
      "Test Epoch [101/200]Batch [  0/ 79] Loss: 0.936 Acc 69.531%\n",
      "Train Epoch [102/200]Batch [  0/391] Loss: 1.185 Acc 63.281%\n",
      "Train Epoch [102/200]Batch [100/391] Loss: 1.101 Acc 61.804%\n",
      "Train Epoch [102/200]Batch [200/391] Loss: 1.095 Acc 62.368%\n",
      "Train Epoch [102/200]Batch [300/391] Loss: 1.094 Acc 62.225%\n",
      "Test Epoch [102/200]Batch [  0/ 79] Loss: 0.934 Acc 70.312%\n",
      "Train Epoch [103/200]Batch [  0/391] Loss: 1.103 Acc 66.406%\n",
      "Train Epoch [103/200]Batch [100/391] Loss: 1.088 Acc 62.840%\n",
      "Train Epoch [103/200]Batch [200/391] Loss: 1.092 Acc 62.846%\n",
      "Train Epoch [103/200]Batch [300/391] Loss: 1.085 Acc 62.889%\n",
      "Test Epoch [103/200]Batch [  0/ 79] Loss: 0.951 Acc 68.750%\n",
      "Train Epoch [104/200]Batch [  0/391] Loss: 0.951 Acc 65.625%\n",
      "Train Epoch [104/200]Batch [100/391] Loss: 1.084 Acc 63.157%\n",
      "Train Epoch [104/200]Batch [200/391] Loss: 1.089 Acc 62.815%\n",
      "Train Epoch [104/200]Batch [300/391] Loss: 1.088 Acc 62.822%\n",
      "Test Epoch [104/200]Batch [  0/ 79] Loss: 0.945 Acc 68.750%\n",
      "Train Epoch [105/200]Batch [  0/391] Loss: 1.259 Acc 58.594%\n",
      "Train Epoch [105/200]Batch [100/391] Loss: 1.085 Acc 62.446%\n",
      "Train Epoch [105/200]Batch [200/391] Loss: 1.091 Acc 62.127%\n",
      "Train Epoch [105/200]Batch [300/391] Loss: 1.089 Acc 62.370%\n",
      "Test Epoch [105/200]Batch [  0/ 79] Loss: 0.934 Acc 69.531%\n",
      "Train Epoch [106/200]Batch [  0/391] Loss: 1.197 Acc 57.812%\n",
      "Train Epoch [106/200]Batch [100/391] Loss: 1.098 Acc 62.376%\n",
      "Train Epoch [106/200]Batch [200/391] Loss: 1.089 Acc 62.519%\n",
      "Train Epoch [106/200]Batch [300/391] Loss: 1.088 Acc 62.606%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [106/200]Batch [  0/ 79] Loss: 0.948 Acc 70.312%\n",
      "Train Epoch [107/200]Batch [  0/391] Loss: 1.118 Acc 64.844%\n",
      "Train Epoch [107/200]Batch [100/391] Loss: 1.093 Acc 62.028%\n",
      "Train Epoch [107/200]Batch [200/391] Loss: 1.089 Acc 62.574%\n",
      "Train Epoch [107/200]Batch [300/391] Loss: 1.089 Acc 62.664%\n",
      "Test Epoch [107/200]Batch [  0/ 79] Loss: 0.921 Acc 70.312%\n",
      "Saving..\n",
      "Train Epoch [108/200]Batch [  0/391] Loss: 1.224 Acc 57.031%\n",
      "Train Epoch [108/200]Batch [100/391] Loss: 1.071 Acc 63.041%\n",
      "Train Epoch [108/200]Batch [200/391] Loss: 1.081 Acc 62.683%\n",
      "Train Epoch [108/200]Batch [300/391] Loss: 1.081 Acc 62.775%\n",
      "Test Epoch [108/200]Batch [  0/ 79] Loss: 0.930 Acc 67.188%\n",
      "Train Epoch [109/200]Batch [  0/391] Loss: 1.096 Acc 63.281%\n",
      "Train Epoch [109/200]Batch [100/391] Loss: 1.087 Acc 62.330%\n",
      "Train Epoch [109/200]Batch [200/391] Loss: 1.077 Acc 62.970%\n",
      "Train Epoch [109/200]Batch [300/391] Loss: 1.076 Acc 62.965%\n",
      "Test Epoch [109/200]Batch [  0/ 79] Loss: 0.911 Acc 70.312%\n",
      "Train Epoch [110/200]Batch [  0/391] Loss: 1.005 Acc 64.844%\n",
      "Train Epoch [110/200]Batch [100/391] Loss: 1.087 Acc 62.763%\n",
      "Train Epoch [110/200]Batch [200/391] Loss: 1.091 Acc 62.683%\n",
      "Train Epoch [110/200]Batch [300/391] Loss: 1.087 Acc 62.676%\n",
      "Test Epoch [110/200]Batch [  0/ 79] Loss: 0.911 Acc 68.750%\n",
      "Saving..\n",
      "Train Epoch [111/200]Batch [  0/391] Loss: 1.189 Acc 57.031%\n",
      "Train Epoch [111/200]Batch [100/391] Loss: 1.078 Acc 63.034%\n",
      "Train Epoch [111/200]Batch [200/391] Loss: 1.085 Acc 62.912%\n",
      "Train Epoch [111/200]Batch [300/391] Loss: 1.079 Acc 62.863%\n",
      "Test Epoch [111/200]Batch [  0/ 79] Loss: 0.938 Acc 64.844%\n",
      "Train Epoch [112/200]Batch [  0/391] Loss: 1.056 Acc 61.719%\n",
      "Train Epoch [112/200]Batch [100/391] Loss: 1.064 Acc 63.297%\n",
      "Train Epoch [112/200]Batch [200/391] Loss: 1.076 Acc 63.250%\n",
      "Train Epoch [112/200]Batch [300/391] Loss: 1.080 Acc 63.138%\n",
      "Test Epoch [112/200]Batch [  0/ 79] Loss: 0.921 Acc 70.312%\n",
      "Train Epoch [113/200]Batch [  0/391] Loss: 0.982 Acc 65.625%\n",
      "Train Epoch [113/200]Batch [100/391] Loss: 1.078 Acc 63.413%\n",
      "Train Epoch [113/200]Batch [200/391] Loss: 1.072 Acc 63.375%\n",
      "Train Epoch [113/200]Batch [300/391] Loss: 1.079 Acc 63.045%\n",
      "Test Epoch [113/200]Batch [  0/ 79] Loss: 0.915 Acc 70.312%\n",
      "Train Epoch [114/200]Batch [  0/391] Loss: 1.068 Acc 64.844%\n",
      "Train Epoch [114/200]Batch [100/391] Loss: 1.086 Acc 62.562%\n",
      "Train Epoch [114/200]Batch [200/391] Loss: 1.076 Acc 63.312%\n",
      "Train Epoch [114/200]Batch [300/391] Loss: 1.079 Acc 63.011%\n",
      "Test Epoch [114/200]Batch [  0/ 79] Loss: 0.933 Acc 67.969%\n",
      "Train Epoch [115/200]Batch [  0/391] Loss: 1.131 Acc 65.625%\n",
      "Train Epoch [115/200]Batch [100/391] Loss: 1.093 Acc 62.840%\n",
      "Train Epoch [115/200]Batch [200/391] Loss: 1.087 Acc 62.998%\n",
      "Train Epoch [115/200]Batch [300/391] Loss: 1.080 Acc 63.157%\n",
      "Test Epoch [115/200]Batch [  0/ 79] Loss: 0.902 Acc 71.094%\n",
      "Saving..\n",
      "Train Epoch [116/200]Batch [  0/391] Loss: 1.024 Acc 64.844%\n",
      "Train Epoch [116/200]Batch [100/391] Loss: 1.078 Acc 63.506%\n",
      "Train Epoch [116/200]Batch [200/391] Loss: 1.080 Acc 62.978%\n",
      "Train Epoch [116/200]Batch [300/391] Loss: 1.075 Acc 63.089%\n",
      "Test Epoch [116/200]Batch [  0/ 79] Loss: 0.914 Acc 71.875%\n",
      "Train Epoch [117/200]Batch [  0/391] Loss: 1.308 Acc 55.469%\n",
      "Train Epoch [117/200]Batch [100/391] Loss: 1.086 Acc 62.794%\n",
      "Train Epoch [117/200]Batch [200/391] Loss: 1.082 Acc 62.920%\n",
      "Train Epoch [117/200]Batch [300/391] Loss: 1.075 Acc 63.266%\n",
      "Test Epoch [117/200]Batch [  0/ 79] Loss: 0.920 Acc 68.750%\n",
      "Train Epoch [118/200]Batch [  0/391] Loss: 1.129 Acc 57.812%\n",
      "Train Epoch [118/200]Batch [100/391] Loss: 1.066 Acc 62.647%\n",
      "Train Epoch [118/200]Batch [200/391] Loss: 1.070 Acc 62.725%\n",
      "Train Epoch [118/200]Batch [300/391] Loss: 1.070 Acc 62.967%\n",
      "Test Epoch [118/200]Batch [  0/ 79] Loss: 0.939 Acc 71.094%\n",
      "Train Epoch [119/200]Batch [  0/391] Loss: 1.105 Acc 60.156%\n",
      "Train Epoch [119/200]Batch [100/391] Loss: 1.082 Acc 62.724%\n",
      "Train Epoch [119/200]Batch [200/391] Loss: 1.079 Acc 63.134%\n",
      "Train Epoch [119/200]Batch [300/391] Loss: 1.076 Acc 63.219%\n",
      "Test Epoch [119/200]Batch [  0/ 79] Loss: 0.895 Acc 67.188%\n",
      "Saving..\n",
      "Train Epoch [120/200]Batch [  0/391] Loss: 1.106 Acc 62.500%\n",
      "Train Epoch [120/200]Batch [100/391] Loss: 1.086 Acc 62.972%\n",
      "Train Epoch [120/200]Batch [200/391] Loss: 1.079 Acc 62.838%\n",
      "Train Epoch [120/200]Batch [300/391] Loss: 1.074 Acc 63.133%\n",
      "Test Epoch [120/200]Batch [  0/ 79] Loss: 0.899 Acc 69.531%\n",
      "Saving..\n",
      "Train Epoch [121/200]Batch [  0/391] Loss: 1.033 Acc 68.750%\n",
      "Train Epoch [121/200]Batch [100/391] Loss: 1.082 Acc 62.376%\n",
      "Train Epoch [121/200]Batch [200/391] Loss: 1.076 Acc 63.207%\n",
      "Train Epoch [121/200]Batch [300/391] Loss: 1.073 Acc 63.107%\n",
      "Test Epoch [121/200]Batch [  0/ 79] Loss: 0.905 Acc 69.531%\n",
      "Saving..\n",
      "Train Epoch [122/200]Batch [  0/391] Loss: 1.076 Acc 67.188%\n",
      "Train Epoch [122/200]Batch [100/391] Loss: 1.086 Acc 62.639%\n",
      "Train Epoch [122/200]Batch [200/391] Loss: 1.086 Acc 62.628%\n",
      "Train Epoch [122/200]Batch [300/391] Loss: 1.081 Acc 62.773%\n",
      "Test Epoch [122/200]Batch [  0/ 79] Loss: 0.924 Acc 67.969%\n",
      "Train Epoch [123/200]Batch [  0/391] Loss: 1.053 Acc 64.844%\n",
      "Train Epoch [123/200]Batch [100/391] Loss: 1.062 Acc 64.055%\n",
      "Train Epoch [123/200]Batch [200/391] Loss: 1.068 Acc 63.748%\n",
      "Train Epoch [123/200]Batch [300/391] Loss: 1.067 Acc 63.712%\n",
      "Test Epoch [123/200]Batch [  0/ 79] Loss: 0.905 Acc 71.094%\n",
      "Saving..\n",
      "Train Epoch [124/200]Batch [  0/391] Loss: 1.149 Acc 62.500%\n",
      "Train Epoch [124/200]Batch [100/391] Loss: 1.053 Acc 64.062%\n",
      "Train Epoch [124/200]Batch [200/391] Loss: 1.062 Acc 63.868%\n",
      "Train Epoch [124/200]Batch [300/391] Loss: 1.068 Acc 63.507%\n",
      "Test Epoch [124/200]Batch [  0/ 79] Loss: 0.904 Acc 67.188%\n",
      "Train Epoch [125/200]Batch [  0/391] Loss: 0.886 Acc 72.656%\n",
      "Train Epoch [125/200]Batch [100/391] Loss: 1.066 Acc 63.498%\n",
      "Train Epoch [125/200]Batch [200/391] Loss: 1.068 Acc 63.518%\n",
      "Train Epoch [125/200]Batch [300/391] Loss: 1.068 Acc 63.523%\n",
      "Test Epoch [125/200]Batch [  0/ 79] Loss: 0.915 Acc 68.750%\n",
      "Train Epoch [126/200]Batch [  0/391] Loss: 1.064 Acc 65.625%\n",
      "Train Epoch [126/200]Batch [100/391] Loss: 1.067 Acc 64.062%\n",
      "Train Epoch [126/200]Batch [200/391] Loss: 1.071 Acc 63.619%\n",
      "Train Epoch [126/200]Batch [300/391] Loss: 1.068 Acc 63.543%\n",
      "Test Epoch [126/200]Batch [  0/ 79] Loss: 0.896 Acc 69.531%\n",
      "Train Epoch [127/200]Batch [  0/391] Loss: 1.112 Acc 64.062%\n",
      "Train Epoch [127/200]Batch [100/391] Loss: 1.059 Acc 63.328%\n",
      "Train Epoch [127/200]Batch [200/391] Loss: 1.064 Acc 63.200%\n",
      "Train Epoch [127/200]Batch [300/391] Loss: 1.067 Acc 63.289%\n",
      "Test Epoch [127/200]Batch [  0/ 79] Loss: 0.919 Acc 67.969%\n",
      "Saving..\n",
      "Train Epoch [128/200]Batch [  0/391] Loss: 1.214 Acc 57.031%\n",
      "Train Epoch [128/200]Batch [100/391] Loss: 1.069 Acc 63.397%\n",
      "Train Epoch [128/200]Batch [200/391] Loss: 1.065 Acc 63.351%\n",
      "Train Epoch [128/200]Batch [300/391] Loss: 1.069 Acc 63.271%\n",
      "Test Epoch [128/200]Batch [  0/ 79] Loss: 0.920 Acc 67.969%\n",
      "Train Epoch [129/200]Batch [  0/391] Loss: 1.112 Acc 67.969%\n",
      "Train Epoch [129/200]Batch [100/391] Loss: 1.071 Acc 63.328%\n",
      "Train Epoch [129/200]Batch [200/391] Loss: 1.061 Acc 63.736%\n",
      "Train Epoch [129/200]Batch [300/391] Loss: 1.069 Acc 63.471%\n",
      "Test Epoch [129/200]Batch [  0/ 79] Loss: 0.913 Acc 68.750%\n",
      "Saving..\n",
      "Train Epoch [130/200]Batch [  0/391] Loss: 1.016 Acc 65.625%\n",
      "Train Epoch [130/200]Batch [100/391] Loss: 1.075 Acc 63.591%\n",
      "Train Epoch [130/200]Batch [200/391] Loss: 1.064 Acc 63.596%\n",
      "Train Epoch [130/200]Batch [300/391] Loss: 1.065 Acc 63.437%\n",
      "Test Epoch [130/200]Batch [  0/ 79] Loss: 0.896 Acc 71.094%\n",
      "Train Epoch [131/200]Batch [  0/391] Loss: 1.112 Acc 60.156%\n",
      "Train Epoch [131/200]Batch [100/391] Loss: 1.063 Acc 63.861%\n",
      "Train Epoch [131/200]Batch [200/391] Loss: 1.066 Acc 63.604%\n",
      "Train Epoch [131/200]Batch [300/391] Loss: 1.064 Acc 63.699%\n",
      "Test Epoch [131/200]Batch [  0/ 79] Loss: 0.920 Acc 70.312%\n",
      "Train Epoch [132/200]Batch [  0/391] Loss: 1.084 Acc 63.281%\n",
      "Train Epoch [132/200]Batch [100/391] Loss: 1.062 Acc 64.008%\n",
      "Train Epoch [132/200]Batch [200/391] Loss: 1.057 Acc 63.895%\n",
      "Train Epoch [132/200]Batch [300/391] Loss: 1.064 Acc 63.525%\n",
      "Test Epoch [132/200]Batch [  0/ 79] Loss: 0.878 Acc 68.750%\n",
      "Train Epoch [133/200]Batch [  0/391] Loss: 1.055 Acc 62.500%\n",
      "Train Epoch [133/200]Batch [100/391] Loss: 1.067 Acc 63.343%\n",
      "Train Epoch [133/200]Batch [200/391] Loss: 1.061 Acc 63.678%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [133/200]Batch [300/391] Loss: 1.061 Acc 63.538%\n",
      "Test Epoch [133/200]Batch [  0/ 79] Loss: 0.919 Acc 69.531%\n",
      "Train Epoch [134/200]Batch [  0/391] Loss: 0.993 Acc 64.844%\n",
      "Train Epoch [134/200]Batch [100/391] Loss: 1.058 Acc 63.900%\n",
      "Train Epoch [134/200]Batch [200/391] Loss: 1.057 Acc 64.000%\n",
      "Train Epoch [134/200]Batch [300/391] Loss: 1.061 Acc 63.697%\n",
      "Test Epoch [134/200]Batch [  0/ 79] Loss: 0.899 Acc 70.312%\n",
      "Train Epoch [135/200]Batch [  0/391] Loss: 1.023 Acc 64.062%\n",
      "Train Epoch [135/200]Batch [100/391] Loss: 1.072 Acc 63.266%\n",
      "Train Epoch [135/200]Batch [200/391] Loss: 1.062 Acc 63.359%\n",
      "Train Epoch [135/200]Batch [300/391] Loss: 1.060 Acc 63.528%\n",
      "Test Epoch [135/200]Batch [  0/ 79] Loss: 0.897 Acc 72.656%\n",
      "Train Epoch [136/200]Batch [  0/391] Loss: 0.848 Acc 72.656%\n",
      "Train Epoch [136/200]Batch [100/391] Loss: 1.072 Acc 63.444%\n",
      "Train Epoch [136/200]Batch [200/391] Loss: 1.064 Acc 64.000%\n",
      "Train Epoch [136/200]Batch [300/391] Loss: 1.061 Acc 63.951%\n",
      "Test Epoch [136/200]Batch [  0/ 79] Loss: 0.907 Acc 69.531%\n",
      "Saving..\n",
      "Train Epoch [137/200]Batch [  0/391] Loss: 1.020 Acc 63.281%\n",
      "Train Epoch [137/200]Batch [100/391] Loss: 1.052 Acc 64.186%\n",
      "Train Epoch [137/200]Batch [200/391] Loss: 1.063 Acc 63.557%\n",
      "Train Epoch [137/200]Batch [300/391] Loss: 1.057 Acc 63.543%\n",
      "Test Epoch [137/200]Batch [  0/ 79] Loss: 0.887 Acc 71.875%\n",
      "Saving..\n",
      "Train Epoch [138/200]Batch [  0/391] Loss: 0.997 Acc 64.844%\n",
      "Train Epoch [138/200]Batch [100/391] Loss: 1.059 Acc 63.977%\n",
      "Train Epoch [138/200]Batch [200/391] Loss: 1.060 Acc 63.713%\n",
      "Train Epoch [138/200]Batch [300/391] Loss: 1.060 Acc 63.507%\n",
      "Test Epoch [138/200]Batch [  0/ 79] Loss: 0.908 Acc 70.312%\n",
      "Train Epoch [139/200]Batch [  0/391] Loss: 1.129 Acc 59.375%\n",
      "Train Epoch [139/200]Batch [100/391] Loss: 1.050 Acc 64.217%\n",
      "Train Epoch [139/200]Batch [200/391] Loss: 1.046 Acc 64.129%\n",
      "Train Epoch [139/200]Batch [300/391] Loss: 1.050 Acc 64.068%\n",
      "Test Epoch [139/200]Batch [  0/ 79] Loss: 0.907 Acc 70.312%\n",
      "Train Epoch [140/200]Batch [  0/391] Loss: 1.034 Acc 63.281%\n",
      "Train Epoch [140/200]Batch [100/391] Loss: 1.062 Acc 63.382%\n",
      "Train Epoch [140/200]Batch [200/391] Loss: 1.058 Acc 63.596%\n",
      "Train Epoch [140/200]Batch [300/391] Loss: 1.056 Acc 63.606%\n",
      "Test Epoch [140/200]Batch [  0/ 79] Loss: 0.915 Acc 68.750%\n",
      "Train Epoch [141/200]Batch [  0/391] Loss: 0.954 Acc 63.281%\n",
      "Train Epoch [141/200]Batch [100/391] Loss: 1.064 Acc 63.776%\n",
      "Train Epoch [141/200]Batch [200/391] Loss: 1.058 Acc 63.775%\n",
      "Train Epoch [141/200]Batch [300/391] Loss: 1.062 Acc 63.626%\n",
      "Test Epoch [141/200]Batch [  0/ 79] Loss: 0.904 Acc 70.312%\n",
      "Train Epoch [142/200]Batch [  0/391] Loss: 1.158 Acc 63.281%\n",
      "Train Epoch [142/200]Batch [100/391] Loss: 1.043 Acc 64.867%\n",
      "Train Epoch [142/200]Batch [200/391] Loss: 1.054 Acc 64.066%\n",
      "Train Epoch [142/200]Batch [300/391] Loss: 1.053 Acc 63.902%\n",
      "Test Epoch [142/200]Batch [  0/ 79] Loss: 0.894 Acc 71.094%\n",
      "Train Epoch [143/200]Batch [  0/391] Loss: 1.000 Acc 67.969%\n",
      "Train Epoch [143/200]Batch [100/391] Loss: 1.065 Acc 63.498%\n",
      "Train Epoch [143/200]Batch [200/391] Loss: 1.064 Acc 63.538%\n",
      "Train Epoch [143/200]Batch [300/391] Loss: 1.058 Acc 63.842%\n",
      "Test Epoch [143/200]Batch [  0/ 79] Loss: 0.888 Acc 72.656%\n",
      "Train Epoch [144/200]Batch [  0/391] Loss: 1.018 Acc 64.844%\n",
      "Train Epoch [144/200]Batch [100/391] Loss: 1.045 Acc 64.109%\n",
      "Train Epoch [144/200]Batch [200/391] Loss: 1.041 Acc 64.265%\n",
      "Train Epoch [144/200]Batch [300/391] Loss: 1.043 Acc 64.000%\n",
      "Test Epoch [144/200]Batch [  0/ 79] Loss: 0.902 Acc 71.094%\n",
      "Train Epoch [145/200]Batch [  0/391] Loss: 0.982 Acc 67.969%\n",
      "Train Epoch [145/200]Batch [100/391] Loss: 1.038 Acc 64.411%\n",
      "Train Epoch [145/200]Batch [200/391] Loss: 1.041 Acc 64.272%\n",
      "Train Epoch [145/200]Batch [300/391] Loss: 1.037 Acc 64.436%\n",
      "Test Epoch [145/200]Batch [  0/ 79] Loss: 0.908 Acc 71.875%\n",
      "Train Epoch [146/200]Batch [  0/391] Loss: 1.006 Acc 67.969%\n",
      "Train Epoch [146/200]Batch [100/391] Loss: 1.044 Acc 64.356%\n",
      "Train Epoch [146/200]Batch [200/391] Loss: 1.042 Acc 64.296%\n",
      "Train Epoch [146/200]Batch [300/391] Loss: 1.043 Acc 64.060%\n",
      "Test Epoch [146/200]Batch [  0/ 79] Loss: 0.883 Acc 71.094%\n",
      "Saving..\n",
      "Train Epoch [147/200]Batch [  0/391] Loss: 1.121 Acc 60.938%\n",
      "Train Epoch [147/200]Batch [100/391] Loss: 1.042 Acc 64.256%\n",
      "Train Epoch [147/200]Batch [200/391] Loss: 1.054 Acc 63.923%\n",
      "Train Epoch [147/200]Batch [300/391] Loss: 1.049 Acc 63.935%\n",
      "Test Epoch [147/200]Batch [  0/ 79] Loss: 0.900 Acc 70.312%\n",
      "Train Epoch [148/200]Batch [  0/391] Loss: 1.231 Acc 54.688%\n",
      "Train Epoch [148/200]Batch [100/391] Loss: 1.062 Acc 62.848%\n",
      "Train Epoch [148/200]Batch [200/391] Loss: 1.057 Acc 63.223%\n",
      "Train Epoch [148/200]Batch [300/391] Loss: 1.052 Acc 63.686%\n",
      "Test Epoch [148/200]Batch [  0/ 79] Loss: 0.892 Acc 71.875%\n",
      "Train Epoch [149/200]Batch [  0/391] Loss: 0.995 Acc 67.969%\n",
      "Train Epoch [149/200]Batch [100/391] Loss: 1.056 Acc 63.954%\n",
      "Train Epoch [149/200]Batch [200/391] Loss: 1.051 Acc 64.257%\n",
      "Train Epoch [149/200]Batch [300/391] Loss: 1.051 Acc 64.037%\n",
      "Test Epoch [149/200]Batch [  0/ 79] Loss: 0.878 Acc 71.094%\n",
      "Train Epoch [150/200]Batch [  0/391] Loss: 1.004 Acc 63.281%\n",
      "Train Epoch [150/200]Batch [100/391] Loss: 1.025 Acc 65.006%\n",
      "Train Epoch [150/200]Batch [200/391] Loss: 1.034 Acc 64.428%\n",
      "Train Epoch [150/200]Batch [300/391] Loss: 1.043 Acc 64.133%\n",
      "Test Epoch [150/200]Batch [  0/ 79] Loss: 0.903 Acc 67.969%\n",
      "Train Epoch [151/200]Batch [  0/391] Loss: 1.120 Acc 57.031%\n",
      "Train Epoch [151/200]Batch [100/391] Loss: 1.033 Acc 64.558%\n",
      "Train Epoch [151/200]Batch [200/391] Loss: 1.046 Acc 64.101%\n",
      "Train Epoch [151/200]Batch [300/391] Loss: 1.043 Acc 63.987%\n",
      "Test Epoch [151/200]Batch [  0/ 79] Loss: 0.899 Acc 72.656%\n",
      "Train Epoch [152/200]Batch [  0/391] Loss: 1.136 Acc 61.719%\n",
      "Train Epoch [152/200]Batch [100/391] Loss: 1.042 Acc 64.318%\n",
      "Train Epoch [152/200]Batch [200/391] Loss: 1.045 Acc 64.269%\n",
      "Train Epoch [152/200]Batch [300/391] Loss: 1.045 Acc 64.138%\n",
      "Test Epoch [152/200]Batch [  0/ 79] Loss: 0.903 Acc 68.750%\n",
      "Train Epoch [153/200]Batch [  0/391] Loss: 0.945 Acc 69.531%\n",
      "Train Epoch [153/200]Batch [100/391] Loss: 1.037 Acc 64.596%\n",
      "Train Epoch [153/200]Batch [200/391] Loss: 1.042 Acc 64.160%\n",
      "Train Epoch [153/200]Batch [300/391] Loss: 1.043 Acc 64.083%\n",
      "Test Epoch [153/200]Batch [  0/ 79] Loss: 0.893 Acc 71.875%\n",
      "Train Epoch [154/200]Batch [  0/391] Loss: 0.966 Acc 65.625%\n",
      "Train Epoch [154/200]Batch [100/391] Loss: 1.049 Acc 63.730%\n",
      "Train Epoch [154/200]Batch [200/391] Loss: 1.053 Acc 63.682%\n",
      "Train Epoch [154/200]Batch [300/391] Loss: 1.051 Acc 63.761%\n",
      "Test Epoch [154/200]Batch [  0/ 79] Loss: 0.915 Acc 68.750%\n",
      "Train Epoch [155/200]Batch [  0/391] Loss: 1.087 Acc 59.375%\n",
      "Train Epoch [155/200]Batch [100/391] Loss: 1.046 Acc 64.001%\n",
      "Train Epoch [155/200]Batch [200/391] Loss: 1.041 Acc 63.938%\n",
      "Train Epoch [155/200]Batch [300/391] Loss: 1.039 Acc 64.153%\n",
      "Test Epoch [155/200]Batch [  0/ 79] Loss: 0.906 Acc 68.750%\n",
      "Train Epoch [156/200]Batch [  0/391] Loss: 0.856 Acc 67.969%\n",
      "Train Epoch [156/200]Batch [100/391] Loss: 1.030 Acc 64.596%\n",
      "Train Epoch [156/200]Batch [200/391] Loss: 1.041 Acc 64.187%\n",
      "Train Epoch [156/200]Batch [300/391] Loss: 1.040 Acc 64.159%\n",
      "Test Epoch [156/200]Batch [  0/ 79] Loss: 0.879 Acc 72.656%\n",
      "Train Epoch [157/200]Batch [  0/391] Loss: 1.365 Acc 55.469%\n",
      "Train Epoch [157/200]Batch [100/391] Loss: 1.044 Acc 64.055%\n",
      "Train Epoch [157/200]Batch [200/391] Loss: 1.043 Acc 64.249%\n",
      "Train Epoch [157/200]Batch [300/391] Loss: 1.049 Acc 63.990%\n",
      "Test Epoch [157/200]Batch [  0/ 79] Loss: 0.872 Acc 71.094%\n",
      "Saving..\n",
      "Train Epoch [158/200]Batch [  0/391] Loss: 1.186 Acc 60.938%\n",
      "Train Epoch [158/200]Batch [100/391] Loss: 1.025 Acc 64.983%\n",
      "Train Epoch [158/200]Batch [200/391] Loss: 1.038 Acc 64.350%\n",
      "Train Epoch [158/200]Batch [300/391] Loss: 1.041 Acc 64.418%\n",
      "Test Epoch [158/200]Batch [  0/ 79] Loss: 0.901 Acc 70.312%\n",
      "Train Epoch [159/200]Batch [  0/391] Loss: 1.087 Acc 60.938%\n",
      "Train Epoch [159/200]Batch [100/391] Loss: 1.037 Acc 63.916%\n",
      "Train Epoch [159/200]Batch [200/391] Loss: 1.036 Acc 64.249%\n",
      "Train Epoch [159/200]Batch [300/391] Loss: 1.035 Acc 64.221%\n",
      "Test Epoch [159/200]Batch [  0/ 79] Loss: 0.904 Acc 71.094%\n",
      "Train Epoch [160/200]Batch [  0/391] Loss: 1.044 Acc 61.719%\n",
      "Train Epoch [160/200]Batch [100/391] Loss: 1.031 Acc 64.712%\n",
      "Train Epoch [160/200]Batch [200/391] Loss: 1.037 Acc 64.327%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [160/200]Batch [300/391] Loss: 1.039 Acc 64.252%\n",
      "Test Epoch [160/200]Batch [  0/ 79] Loss: 0.900 Acc 70.312%\n",
      "Train Epoch [161/200]Batch [  0/391] Loss: 1.122 Acc 60.938%\n",
      "Train Epoch [161/200]Batch [100/391] Loss: 1.035 Acc 64.341%\n",
      "Train Epoch [161/200]Batch [200/391] Loss: 1.041 Acc 64.269%\n",
      "Train Epoch [161/200]Batch [300/391] Loss: 1.038 Acc 64.499%\n",
      "Test Epoch [161/200]Batch [  0/ 79] Loss: 0.897 Acc 71.094%\n",
      "Train Epoch [162/200]Batch [  0/391] Loss: 0.914 Acc 64.062%\n",
      "Train Epoch [162/200]Batch [100/391] Loss: 1.045 Acc 63.668%\n",
      "Train Epoch [162/200]Batch [200/391] Loss: 1.040 Acc 64.148%\n",
      "Train Epoch [162/200]Batch [300/391] Loss: 1.039 Acc 64.288%\n",
      "Test Epoch [162/200]Batch [  0/ 79] Loss: 0.872 Acc 74.219%\n",
      "Train Epoch [163/200]Batch [  0/391] Loss: 0.954 Acc 64.062%\n",
      "Train Epoch [163/200]Batch [100/391] Loss: 1.041 Acc 64.186%\n",
      "Train Epoch [163/200]Batch [200/391] Loss: 1.046 Acc 63.993%\n",
      "Train Epoch [163/200]Batch [300/391] Loss: 1.043 Acc 63.928%\n",
      "Test Epoch [163/200]Batch [  0/ 79] Loss: 0.885 Acc 72.656%\n",
      "Train Epoch [164/200]Batch [  0/391] Loss: 1.091 Acc 61.719%\n",
      "Train Epoch [164/200]Batch [100/391] Loss: 1.042 Acc 64.627%\n",
      "Train Epoch [164/200]Batch [200/391] Loss: 1.036 Acc 64.478%\n",
      "Train Epoch [164/200]Batch [300/391] Loss: 1.038 Acc 64.213%\n",
      "Test Epoch [164/200]Batch [  0/ 79] Loss: 0.906 Acc 70.312%\n",
      "Train Epoch [165/200]Batch [  0/391] Loss: 0.972 Acc 64.062%\n",
      "Train Epoch [165/200]Batch [100/391] Loss: 1.040 Acc 64.271%\n",
      "Train Epoch [165/200]Batch [200/391] Loss: 1.037 Acc 64.451%\n",
      "Train Epoch [165/200]Batch [300/391] Loss: 1.032 Acc 64.621%\n",
      "Test Epoch [165/200]Batch [  0/ 79] Loss: 0.885 Acc 71.094%\n",
      "Train Epoch [166/200]Batch [  0/391] Loss: 1.052 Acc 62.500%\n",
      "Train Epoch [166/200]Batch [100/391] Loss: 1.031 Acc 64.821%\n",
      "Train Epoch [166/200]Batch [200/391] Loss: 1.032 Acc 64.719%\n",
      "Train Epoch [166/200]Batch [300/391] Loss: 1.035 Acc 64.470%\n",
      "Test Epoch [166/200]Batch [  0/ 79] Loss: 0.900 Acc 73.438%\n",
      "Train Epoch [167/200]Batch [  0/391] Loss: 0.888 Acc 69.531%\n",
      "Train Epoch [167/200]Batch [100/391] Loss: 1.047 Acc 64.387%\n",
      "Train Epoch [167/200]Batch [200/391] Loss: 1.036 Acc 64.358%\n",
      "Train Epoch [167/200]Batch [300/391] Loss: 1.036 Acc 64.457%\n",
      "Test Epoch [167/200]Batch [  0/ 79] Loss: 0.891 Acc 71.094%\n",
      "Train Epoch [168/200]Batch [  0/391] Loss: 0.911 Acc 66.406%\n",
      "Train Epoch [168/200]Batch [100/391] Loss: 1.026 Acc 64.851%\n",
      "Train Epoch [168/200]Batch [200/391] Loss: 1.038 Acc 64.560%\n",
      "Train Epoch [168/200]Batch [300/391] Loss: 1.039 Acc 64.576%\n",
      "Test Epoch [168/200]Batch [  0/ 79] Loss: 0.877 Acc 71.875%\n",
      "Train Epoch [169/200]Batch [  0/391] Loss: 1.279 Acc 58.594%\n",
      "Train Epoch [169/200]Batch [100/391] Loss: 1.028 Acc 64.619%\n",
      "Train Epoch [169/200]Batch [200/391] Loss: 1.034 Acc 64.342%\n",
      "Train Epoch [169/200]Batch [300/391] Loss: 1.039 Acc 64.122%\n",
      "Test Epoch [169/200]Batch [  0/ 79] Loss: 0.879 Acc 71.875%\n",
      "Train Epoch [170/200]Batch [  0/391] Loss: 1.031 Acc 59.375%\n",
      "Train Epoch [170/200]Batch [100/391] Loss: 1.046 Acc 63.830%\n",
      "Train Epoch [170/200]Batch [200/391] Loss: 1.037 Acc 64.125%\n",
      "Train Epoch [170/200]Batch [300/391] Loss: 1.031 Acc 64.415%\n",
      "Test Epoch [170/200]Batch [  0/ 79] Loss: 0.890 Acc 72.656%\n",
      "Train Epoch [171/200]Batch [  0/391] Loss: 1.028 Acc 61.719%\n",
      "Train Epoch [171/200]Batch [100/391] Loss: 1.045 Acc 64.318%\n",
      "Train Epoch [171/200]Batch [200/391] Loss: 1.036 Acc 64.646%\n",
      "Train Epoch [171/200]Batch [300/391] Loss: 1.037 Acc 64.566%\n",
      "Test Epoch [171/200]Batch [  0/ 79] Loss: 0.884 Acc 71.094%\n",
      "Train Epoch [172/200]Batch [  0/391] Loss: 1.001 Acc 71.094%\n",
      "Train Epoch [172/200]Batch [100/391] Loss: 1.032 Acc 64.705%\n",
      "Train Epoch [172/200]Batch [200/391] Loss: 1.027 Acc 64.855%\n",
      "Train Epoch [172/200]Batch [300/391] Loss: 1.031 Acc 64.540%\n",
      "Test Epoch [172/200]Batch [  0/ 79] Loss: 0.889 Acc 70.312%\n",
      "Train Epoch [173/200]Batch [  0/391] Loss: 0.964 Acc 70.312%\n",
      "Train Epoch [173/200]Batch [100/391] Loss: 1.031 Acc 64.209%\n",
      "Train Epoch [173/200]Batch [200/391] Loss: 1.037 Acc 64.016%\n",
      "Train Epoch [173/200]Batch [300/391] Loss: 1.032 Acc 64.101%\n",
      "Test Epoch [173/200]Batch [  0/ 79] Loss: 0.903 Acc 71.875%\n",
      "Train Epoch [174/200]Batch [  0/391] Loss: 0.978 Acc 65.625%\n",
      "Train Epoch [174/200]Batch [100/391] Loss: 1.038 Acc 64.588%\n",
      "Train Epoch [174/200]Batch [200/391] Loss: 1.038 Acc 64.560%\n",
      "Train Epoch [174/200]Batch [300/391] Loss: 1.033 Acc 64.563%\n",
      "Test Epoch [174/200]Batch [  0/ 79] Loss: 0.896 Acc 70.312%\n",
      "Train Epoch [175/200]Batch [  0/391] Loss: 0.950 Acc 65.625%\n",
      "Train Epoch [175/200]Batch [100/391] Loss: 1.030 Acc 64.983%\n",
      "Train Epoch [175/200]Batch [200/391] Loss: 1.031 Acc 64.747%\n",
      "Train Epoch [175/200]Batch [300/391] Loss: 1.033 Acc 64.576%\n",
      "Test Epoch [175/200]Batch [  0/ 79] Loss: 0.890 Acc 69.531%\n",
      "Train Epoch [176/200]Batch [  0/391] Loss: 0.928 Acc 65.625%\n",
      "Train Epoch [176/200]Batch [100/391] Loss: 1.019 Acc 64.898%\n",
      "Train Epoch [176/200]Batch [200/391] Loss: 1.033 Acc 64.389%\n",
      "Train Epoch [176/200]Batch [300/391] Loss: 1.034 Acc 64.496%\n",
      "Test Epoch [176/200]Batch [  0/ 79] Loss: 0.885 Acc 72.656%\n",
      "Train Epoch [177/200]Batch [  0/391] Loss: 1.067 Acc 63.281%\n",
      "Train Epoch [177/200]Batch [100/391] Loss: 1.040 Acc 64.124%\n",
      "Train Epoch [177/200]Batch [200/391] Loss: 1.031 Acc 64.587%\n",
      "Train Epoch [177/200]Batch [300/391] Loss: 1.033 Acc 64.600%\n",
      "Test Epoch [177/200]Batch [  0/ 79] Loss: 0.894 Acc 69.531%\n",
      "Train Epoch [178/200]Batch [  0/391] Loss: 0.905 Acc 71.875%\n",
      "Train Epoch [178/200]Batch [100/391] Loss: 1.038 Acc 64.480%\n",
      "Train Epoch [178/200]Batch [200/391] Loss: 1.033 Acc 64.463%\n",
      "Train Epoch [178/200]Batch [300/391] Loss: 1.030 Acc 64.566%\n",
      "Test Epoch [178/200]Batch [  0/ 79] Loss: 0.886 Acc 70.312%\n",
      "Train Epoch [179/200]Batch [  0/391] Loss: 1.039 Acc 65.625%\n",
      "Train Epoch [179/200]Batch [100/391] Loss: 1.018 Acc 65.014%\n",
      "Train Epoch [179/200]Batch [200/391] Loss: 1.021 Acc 64.918%\n",
      "Train Epoch [179/200]Batch [300/391] Loss: 1.028 Acc 64.649%\n",
      "Test Epoch [179/200]Batch [  0/ 79] Loss: 0.893 Acc 70.312%\n",
      "Train Epoch [180/200]Batch [  0/391] Loss: 1.078 Acc 68.750%\n",
      "Train Epoch [180/200]Batch [100/391] Loss: 1.047 Acc 64.387%\n",
      "Train Epoch [180/200]Batch [200/391] Loss: 1.038 Acc 64.428%\n",
      "Train Epoch [180/200]Batch [300/391] Loss: 1.034 Acc 64.548%\n",
      "Test Epoch [180/200]Batch [  0/ 79] Loss: 0.885 Acc 69.531%\n",
      "Train Epoch [181/200]Batch [  0/391] Loss: 0.961 Acc 64.062%\n",
      "Train Epoch [181/200]Batch [100/391] Loss: 1.007 Acc 65.671%\n",
      "Train Epoch [181/200]Batch [200/391] Loss: 1.029 Acc 65.034%\n",
      "Train Epoch [181/200]Batch [300/391] Loss: 1.030 Acc 64.883%\n",
      "Test Epoch [181/200]Batch [  0/ 79] Loss: 0.895 Acc 69.531%\n",
      "Train Epoch [182/200]Batch [  0/391] Loss: 0.877 Acc 67.188%\n",
      "Train Epoch [182/200]Batch [100/391] Loss: 1.035 Acc 64.519%\n",
      "Train Epoch [182/200]Batch [200/391] Loss: 1.030 Acc 64.681%\n",
      "Train Epoch [182/200]Batch [300/391] Loss: 1.034 Acc 64.579%\n",
      "Test Epoch [182/200]Batch [  0/ 79] Loss: 0.871 Acc 73.438%\n",
      "Train Epoch [183/200]Batch [  0/391] Loss: 0.962 Acc 71.094%\n",
      "Train Epoch [183/200]Batch [100/391] Loss: 1.033 Acc 64.558%\n",
      "Train Epoch [183/200]Batch [200/391] Loss: 1.030 Acc 64.595%\n",
      "Train Epoch [183/200]Batch [300/391] Loss: 1.027 Acc 64.737%\n",
      "Test Epoch [183/200]Batch [  0/ 79] Loss: 0.893 Acc 71.094%\n",
      "Train Epoch [184/200]Batch [  0/391] Loss: 1.011 Acc 61.719%\n",
      "Train Epoch [184/200]Batch [100/391] Loss: 1.013 Acc 65.145%\n",
      "Train Epoch [184/200]Batch [200/391] Loss: 1.022 Acc 64.809%\n",
      "Train Epoch [184/200]Batch [300/391] Loss: 1.028 Acc 64.621%\n",
      "Test Epoch [184/200]Batch [  0/ 79] Loss: 0.895 Acc 69.531%\n",
      "Train Epoch [185/200]Batch [  0/391] Loss: 0.924 Acc 71.875%\n",
      "Train Epoch [185/200]Batch [100/391] Loss: 1.014 Acc 64.944%\n",
      "Train Epoch [185/200]Batch [200/391] Loss: 1.021 Acc 64.820%\n",
      "Train Epoch [185/200]Batch [300/391] Loss: 1.027 Acc 64.750%\n",
      "Test Epoch [185/200]Batch [  0/ 79] Loss: 0.895 Acc 71.094%\n",
      "Train Epoch [186/200]Batch [  0/391] Loss: 0.923 Acc 69.531%\n",
      "Train Epoch [186/200]Batch [100/391] Loss: 1.038 Acc 64.380%\n",
      "Train Epoch [186/200]Batch [200/391] Loss: 1.028 Acc 65.081%\n",
      "Train Epoch [186/200]Batch [300/391] Loss: 1.024 Acc 65.129%\n",
      "Test Epoch [186/200]Batch [  0/ 79] Loss: 0.901 Acc 72.656%\n",
      "Train Epoch [187/200]Batch [  0/391] Loss: 0.996 Acc 64.844%\n",
      "Train Epoch [187/200]Batch [100/391] Loss: 1.009 Acc 65.617%\n",
      "Train Epoch [187/200]Batch [200/391] Loss: 1.017 Acc 65.306%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [187/200]Batch [300/391] Loss: 1.020 Acc 65.207%\n",
      "Test Epoch [187/200]Batch [  0/ 79] Loss: 0.886 Acc 69.531%\n",
      "Train Epoch [188/200]Batch [  0/391] Loss: 1.238 Acc 57.812%\n",
      "Train Epoch [188/200]Batch [100/391] Loss: 1.031 Acc 64.619%\n",
      "Train Epoch [188/200]Batch [200/391] Loss: 1.033 Acc 64.471%\n",
      "Train Epoch [188/200]Batch [300/391] Loss: 1.034 Acc 64.545%\n",
      "Test Epoch [188/200]Batch [  0/ 79] Loss: 0.893 Acc 71.875%\n",
      "Train Epoch [189/200]Batch [  0/391] Loss: 0.998 Acc 65.625%\n",
      "Train Epoch [189/200]Batch [100/391] Loss: 1.008 Acc 65.602%\n",
      "Train Epoch [189/200]Batch [200/391] Loss: 1.017 Acc 65.306%\n",
      "Train Epoch [189/200]Batch [300/391] Loss: 1.021 Acc 64.929%\n",
      "Test Epoch [189/200]Batch [  0/ 79] Loss: 0.866 Acc 74.219%\n",
      "Saving..\n",
      "Train Epoch [190/200]Batch [  0/391] Loss: 0.997 Acc 66.406%\n",
      "Train Epoch [190/200]Batch [100/391] Loss: 1.020 Acc 65.331%\n",
      "Train Epoch [190/200]Batch [200/391] Loss: 1.028 Acc 64.770%\n",
      "Train Epoch [190/200]Batch [300/391] Loss: 1.026 Acc 64.805%\n",
      "Test Epoch [190/200]Batch [  0/ 79] Loss: 0.895 Acc 72.656%\n",
      "Train Epoch [191/200]Batch [  0/391] Loss: 0.982 Acc 66.406%\n",
      "Train Epoch [191/200]Batch [100/391] Loss: 1.010 Acc 65.617%\n",
      "Train Epoch [191/200]Batch [200/391] Loss: 1.019 Acc 65.023%\n",
      "Train Epoch [191/200]Batch [300/391] Loss: 1.017 Acc 65.142%\n",
      "Test Epoch [191/200]Batch [  0/ 79] Loss: 0.888 Acc 71.875%\n",
      "Train Epoch [192/200]Batch [  0/391] Loss: 0.940 Acc 68.750%\n",
      "Train Epoch [192/200]Batch [100/391] Loss: 1.025 Acc 65.045%\n",
      "Train Epoch [192/200]Batch [200/391] Loss: 1.019 Acc 65.034%\n",
      "Train Epoch [192/200]Batch [300/391] Loss: 1.024 Acc 64.711%\n",
      "Test Epoch [192/200]Batch [  0/ 79] Loss: 0.903 Acc 70.312%\n",
      "Train Epoch [193/200]Batch [  0/391] Loss: 0.975 Acc 67.188%\n",
      "Train Epoch [193/200]Batch [100/391] Loss: 1.021 Acc 65.176%\n",
      "Train Epoch [193/200]Batch [200/391] Loss: 1.023 Acc 65.127%\n",
      "Train Epoch [193/200]Batch [300/391] Loss: 1.025 Acc 64.940%\n",
      "Test Epoch [193/200]Batch [  0/ 79] Loss: 0.894 Acc 70.312%\n",
      "Train Epoch [194/200]Batch [  0/391] Loss: 1.089 Acc 63.281%\n",
      "Train Epoch [194/200]Batch [100/391] Loss: 1.015 Acc 65.671%\n",
      "Train Epoch [194/200]Batch [200/391] Loss: 1.016 Acc 65.427%\n",
      "Train Epoch [194/200]Batch [300/391] Loss: 1.020 Acc 65.189%\n",
      "Test Epoch [194/200]Batch [  0/ 79] Loss: 0.890 Acc 71.875%\n",
      "Train Epoch [195/200]Batch [  0/391] Loss: 1.080 Acc 62.500%\n",
      "Train Epoch [195/200]Batch [100/391] Loss: 1.011 Acc 65.099%\n",
      "Train Epoch [195/200]Batch [200/391] Loss: 1.013 Acc 65.213%\n",
      "Train Epoch [195/200]Batch [300/391] Loss: 1.017 Acc 65.155%\n",
      "Test Epoch [195/200]Batch [  0/ 79] Loss: 0.880 Acc 72.656%\n",
      "Train Epoch [196/200]Batch [  0/391] Loss: 0.906 Acc 60.156%\n",
      "Train Epoch [196/200]Batch [100/391] Loss: 1.020 Acc 64.867%\n",
      "Train Epoch [196/200]Batch [200/391] Loss: 1.021 Acc 64.863%\n",
      "Train Epoch [196/200]Batch [300/391] Loss: 1.016 Acc 64.966%\n",
      "Test Epoch [196/200]Batch [  0/ 79] Loss: 0.882 Acc 72.656%\n",
      "Train Epoch [197/200]Batch [  0/391] Loss: 1.001 Acc 67.969%\n",
      "Train Epoch [197/200]Batch [100/391] Loss: 1.010 Acc 65.548%\n",
      "Train Epoch [197/200]Batch [200/391] Loss: 1.018 Acc 65.314%\n",
      "Train Epoch [197/200]Batch [300/391] Loss: 1.018 Acc 65.116%\n",
      "Test Epoch [197/200]Batch [  0/ 79] Loss: 0.887 Acc 69.531%\n",
      "Train Epoch [198/200]Batch [  0/391] Loss: 0.864 Acc 75.000%\n",
      "Train Epoch [198/200]Batch [100/391] Loss: 1.005 Acc 65.524%\n",
      "Train Epoch [198/200]Batch [200/391] Loss: 1.017 Acc 65.232%\n",
      "Train Epoch [198/200]Batch [300/391] Loss: 1.024 Acc 65.088%\n",
      "Test Epoch [198/200]Batch [  0/ 79] Loss: 0.868 Acc 70.312%\n",
      "Train Epoch [199/200]Batch [  0/391] Loss: 1.238 Acc 57.812%\n",
      "Train Epoch [199/200]Batch [100/391] Loss: 1.012 Acc 64.882%\n",
      "Train Epoch [199/200]Batch [200/391] Loss: 1.019 Acc 64.793%\n",
      "Train Epoch [199/200]Batch [300/391] Loss: 1.019 Acc 64.883%\n",
      "Test Epoch [199/200]Batch [  0/ 79] Loss: 0.904 Acc 67.969%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c744f71ab36144279d998ea1f5eb41b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [  0/200]Batch [  0/391] Loss: 2.336 Acc 10.938%\n",
      "Train Epoch [  0/200]Batch [100/391] Loss: 2.106 Acc 24.335%\n",
      "Train Epoch [  0/200]Batch [200/391] Loss: 2.009 Acc 28.677%\n",
      "Train Epoch [  0/200]Batch [300/391] Loss: 1.951 Acc 31.027%\n",
      "Test Epoch [  0/200]Batch [  0/ 79] Loss: 1.678 Acc 45.312%\n",
      "Train Epoch [  1/200]Batch [  0/391] Loss: 1.810 Acc 41.406%\n",
      "Train Epoch [  1/200]Batch [100/391] Loss: 1.743 Acc 39.341%\n",
      "Train Epoch [  1/200]Batch [200/391] Loss: 1.719 Acc 40.256%\n",
      "Train Epoch [  1/200]Batch [300/391] Loss: 1.698 Acc 41.030%\n",
      "Test Epoch [  1/200]Batch [  0/ 79] Loss: 1.542 Acc 49.219%\n",
      "Train Epoch [  2/200]Batch [  0/391] Loss: 1.668 Acc 40.625%\n",
      "Train Epoch [  2/200]Batch [100/391] Loss: 1.611 Acc 44.075%\n",
      "Train Epoch [  2/200]Batch [200/391] Loss: 1.603 Acc 44.337%\n",
      "Train Epoch [  2/200]Batch [300/391] Loss: 1.594 Acc 44.671%\n",
      "Test Epoch [  2/200]Batch [  0/ 79] Loss: 1.429 Acc 60.156%\n",
      "Train Epoch [  3/200]Batch [  0/391] Loss: 1.630 Acc 42.188%\n",
      "Train Epoch [  3/200]Batch [100/391] Loss: 1.533 Acc 46.767%\n",
      "Train Epoch [  3/200]Batch [200/391] Loss: 1.530 Acc 46.941%\n",
      "Train Epoch [  3/200]Batch [300/391] Loss: 1.518 Acc 47.384%\n",
      "Test Epoch [  3/200]Batch [  0/ 79] Loss: 1.367 Acc 57.031%\n",
      "Train Epoch [  4/200]Batch [  0/391] Loss: 1.467 Acc 50.000%\n",
      "Train Epoch [  4/200]Batch [100/391] Loss: 1.489 Acc 48.352%\n",
      "Train Epoch [  4/200]Batch [200/391] Loss: 1.475 Acc 49.172%\n",
      "Train Epoch [  4/200]Batch [300/391] Loss: 1.469 Acc 49.496%\n",
      "Test Epoch [  4/200]Batch [  0/ 79] Loss: 1.318 Acc 58.594%\n",
      "Train Epoch [  5/200]Batch [  0/391] Loss: 1.471 Acc 56.250%\n",
      "Train Epoch [  5/200]Batch [100/391] Loss: 1.433 Acc 50.920%\n",
      "Train Epoch [  5/200]Batch [200/391] Loss: 1.430 Acc 50.878%\n",
      "Train Epoch [  5/200]Batch [300/391] Loss: 1.424 Acc 51.062%\n",
      "Test Epoch [  5/200]Batch [  0/ 79] Loss: 1.287 Acc 57.031%\n",
      "Train Epoch [  6/200]Batch [  0/391] Loss: 1.396 Acc 47.656%\n",
      "Train Epoch [  6/200]Batch [100/391] Loss: 1.382 Acc 52.235%\n",
      "Train Epoch [  6/200]Batch [200/391] Loss: 1.390 Acc 51.850%\n",
      "Train Epoch [  6/200]Batch [300/391] Loss: 1.384 Acc 52.271%\n",
      "Test Epoch [  6/200]Batch [  0/ 79] Loss: 1.214 Acc 61.719%\n",
      "Train Epoch [  7/200]Batch [  0/391] Loss: 1.386 Acc 56.250%\n",
      "Train Epoch [  7/200]Batch [100/391] Loss: 1.368 Acc 53.156%\n",
      "Train Epoch [  7/200]Batch [200/391] Loss: 1.366 Acc 53.280%\n",
      "Train Epoch [  7/200]Batch [300/391] Loss: 1.362 Acc 53.512%\n",
      "Test Epoch [  7/200]Batch [  0/ 79] Loss: 1.203 Acc 59.375%\n",
      "Train Epoch [  8/200]Batch [  0/391] Loss: 1.295 Acc 53.906%\n",
      "Train Epoch [  8/200]Batch [100/391] Loss: 1.341 Acc 54.602%\n",
      "Train Epoch [  8/200]Batch [200/391] Loss: 1.341 Acc 54.256%\n",
      "Train Epoch [  8/200]Batch [300/391] Loss: 1.338 Acc 54.462%\n",
      "Test Epoch [  8/200]Batch [  0/ 79] Loss: 1.198 Acc 60.156%\n",
      "Train Epoch [  9/200]Batch [  0/391] Loss: 1.312 Acc 52.344%\n",
      "Train Epoch [  9/200]Batch [100/391] Loss: 1.338 Acc 54.455%\n",
      "Train Epoch [  9/200]Batch [200/391] Loss: 1.320 Acc 55.131%\n",
      "Train Epoch [  9/200]Batch [300/391] Loss: 1.314 Acc 55.222%\n",
      "Test Epoch [  9/200]Batch [  0/ 79] Loss: 1.174 Acc 63.281%\n",
      "Train Epoch [ 10/200]Batch [  0/391] Loss: 1.433 Acc 51.562%\n",
      "Train Epoch [ 10/200]Batch [100/391] Loss: 1.295 Acc 55.654%\n",
      "Train Epoch [ 10/200]Batch [200/391] Loss: 1.298 Acc 55.947%\n",
      "Train Epoch [ 10/200]Batch [300/391] Loss: 1.296 Acc 55.793%\n",
      "Test Epoch [ 10/200]Batch [  0/ 79] Loss: 1.153 Acc 58.594%\n",
      "Train Epoch [ 11/200]Batch [  0/391] Loss: 1.355 Acc 45.312%\n",
      "Train Epoch [ 11/200]Batch [100/391] Loss: 1.278 Acc 55.817%\n",
      "Train Epoch [ 11/200]Batch [200/391] Loss: 1.276 Acc 56.324%\n",
      "Train Epoch [ 11/200]Batch [300/391] Loss: 1.277 Acc 56.494%\n",
      "Test Epoch [ 11/200]Batch [  0/ 79] Loss: 1.141 Acc 64.844%\n",
      "Train Epoch [ 12/200]Batch [  0/391] Loss: 1.232 Acc 60.156%\n",
      "Train Epoch [ 12/200]Batch [100/391] Loss: 1.253 Acc 57.093%\n",
      "Train Epoch [ 12/200]Batch [200/391] Loss: 1.260 Acc 56.926%\n",
      "Train Epoch [ 12/200]Batch [300/391] Loss: 1.262 Acc 56.912%\n",
      "Test Epoch [ 12/200]Batch [  0/ 79] Loss: 1.079 Acc 63.281%\n",
      "Train Epoch [ 13/200]Batch [  0/391] Loss: 1.213 Acc 60.156%\n",
      "Train Epoch [ 13/200]Batch [100/391] Loss: 1.253 Acc 57.735%\n",
      "Train Epoch [ 13/200]Batch [200/391] Loss: 1.254 Acc 57.447%\n",
      "Train Epoch [ 13/200]Batch [300/391] Loss: 1.249 Acc 57.509%\n",
      "Test Epoch [ 13/200]Batch [  0/ 79] Loss: 1.086 Acc 63.281%\n",
      "Train Epoch [ 14/200]Batch [  0/391] Loss: 1.303 Acc 53.125%\n",
      "Train Epoch [ 14/200]Batch [100/391] Loss: 1.233 Acc 58.323%\n",
      "Train Epoch [ 14/200]Batch [200/391] Loss: 1.234 Acc 57.964%\n",
      "Train Epoch [ 14/200]Batch [300/391] Loss: 1.238 Acc 57.766%\n",
      "Test Epoch [ 14/200]Batch [  0/ 79] Loss: 1.071 Acc 61.719%\n",
      "Train Epoch [ 15/200]Batch [  0/391] Loss: 1.269 Acc 58.594%\n",
      "Train Epoch [ 15/200]Batch [100/391] Loss: 1.242 Acc 57.433%\n",
      "Train Epoch [ 15/200]Batch [200/391] Loss: 1.231 Acc 57.984%\n",
      "Train Epoch [ 15/200]Batch [300/391] Loss: 1.226 Acc 58.132%\n",
      "Test Epoch [ 15/200]Batch [  0/ 79] Loss: 1.098 Acc 63.281%\n",
      "Train Epoch [ 16/200]Batch [  0/391] Loss: 1.238 Acc 52.344%\n",
      "Train Epoch [ 16/200]Batch [100/391] Loss: 1.219 Acc 58.246%\n",
      "Train Epoch [ 16/200]Batch [200/391] Loss: 1.215 Acc 58.625%\n",
      "Train Epoch [ 16/200]Batch [300/391] Loss: 1.213 Acc 58.643%\n",
      "Test Epoch [ 16/200]Batch [  0/ 79] Loss: 1.055 Acc 67.188%\n",
      "Train Epoch [ 17/200]Batch [  0/391] Loss: 1.094 Acc 60.156%\n",
      "Train Epoch [ 17/200]Batch [100/391] Loss: 1.222 Acc 59.027%\n",
      "Train Epoch [ 17/200]Batch [200/391] Loss: 1.219 Acc 58.761%\n",
      "Train Epoch [ 17/200]Batch [300/391] Loss: 1.215 Acc 58.778%\n",
      "Test Epoch [ 17/200]Batch [  0/ 79] Loss: 1.024 Acc 67.188%\n",
      "Train Epoch [ 18/200]Batch [  0/391] Loss: 1.228 Acc 54.688%\n",
      "Train Epoch [ 18/200]Batch [100/391] Loss: 1.203 Acc 59.259%\n",
      "Train Epoch [ 18/200]Batch [200/391] Loss: 1.203 Acc 58.967%\n",
      "Train Epoch [ 18/200]Batch [300/391] Loss: 1.205 Acc 58.942%\n",
      "Test Epoch [ 18/200]Batch [  0/ 79] Loss: 1.033 Acc 67.188%\n",
      "Train Epoch [ 19/200]Batch [  0/391] Loss: 1.202 Acc 54.688%\n",
      "Train Epoch [ 19/200]Batch [100/391] Loss: 1.211 Acc 58.787%\n",
      "Train Epoch [ 19/200]Batch [200/391] Loss: 1.196 Acc 59.600%\n",
      "Train Epoch [ 19/200]Batch [300/391] Loss: 1.195 Acc 59.551%\n",
      "Test Epoch [ 19/200]Batch [  0/ 79] Loss: 1.003 Acc 69.531%\n",
      "Train Epoch [ 20/200]Batch [  0/391] Loss: 1.201 Acc 56.250%\n",
      "Train Epoch [ 20/200]Batch [100/391] Loss: 1.185 Acc 59.646%\n",
      "Train Epoch [ 20/200]Batch [200/391] Loss: 1.189 Acc 59.857%\n",
      "Train Epoch [ 20/200]Batch [300/391] Loss: 1.184 Acc 59.886%\n",
      "Test Epoch [ 20/200]Batch [  0/ 79] Loss: 1.002 Acc 67.969%\n",
      "Train Epoch [ 21/200]Batch [  0/391] Loss: 1.036 Acc 61.719%\n",
      "Train Epoch [ 21/200]Batch [100/391] Loss: 1.177 Acc 59.599%\n",
      "Train Epoch [ 21/200]Batch [200/391] Loss: 1.175 Acc 59.876%\n",
      "Train Epoch [ 21/200]Batch [300/391] Loss: 1.178 Acc 59.775%\n",
      "Test Epoch [ 21/200]Batch [  0/ 79] Loss: 1.001 Acc 67.188%\n",
      "Train Epoch [ 22/200]Batch [  0/391] Loss: 1.112 Acc 60.938%\n",
      "Train Epoch [ 22/200]Batch [100/391] Loss: 1.179 Acc 59.537%\n",
      "Train Epoch [ 22/200]Batch [200/391] Loss: 1.176 Acc 60.047%\n",
      "Train Epoch [ 22/200]Batch [300/391] Loss: 1.178 Acc 59.964%\n",
      "Test Epoch [ 22/200]Batch [  0/ 79] Loss: 0.996 Acc 67.188%\n",
      "Train Epoch [ 23/200]Batch [  0/391] Loss: 1.158 Acc 62.500%\n",
      "Train Epoch [ 23/200]Batch [100/391] Loss: 1.174 Acc 60.133%\n",
      "Train Epoch [ 23/200]Batch [200/391] Loss: 1.171 Acc 60.413%\n",
      "Train Epoch [ 23/200]Batch [300/391] Loss: 1.169 Acc 60.304%\n",
      "Test Epoch [ 23/200]Batch [  0/ 79] Loss: 0.989 Acc 71.094%\n",
      "Train Epoch [ 24/200]Batch [  0/391] Loss: 1.224 Acc 57.812%\n",
      "Train Epoch [ 24/200]Batch [100/391] Loss: 1.162 Acc 60.125%\n",
      "Train Epoch [ 24/200]Batch [200/391] Loss: 1.166 Acc 60.024%\n",
      "Train Epoch [ 24/200]Batch [300/391] Loss: 1.168 Acc 60.016%\n",
      "Test Epoch [ 24/200]Batch [  0/ 79] Loss: 0.992 Acc 67.969%\n",
      "Train Epoch [ 25/200]Batch [  0/391] Loss: 1.032 Acc 66.406%\n",
      "Train Epoch [ 25/200]Batch [100/391] Loss: 1.160 Acc 60.551%\n",
      "Train Epoch [ 25/200]Batch [200/391] Loss: 1.159 Acc 60.557%\n",
      "Train Epoch [ 25/200]Batch [300/391] Loss: 1.159 Acc 60.538%\n",
      "Test Epoch [ 25/200]Batch [  0/ 79] Loss: 0.983 Acc 69.531%\n",
      "Train Epoch [ 26/200]Batch [  0/391] Loss: 1.133 Acc 65.625%\n",
      "Train Epoch [ 26/200]Batch [100/391] Loss: 1.158 Acc 60.597%\n",
      "Train Epoch [ 26/200]Batch [200/391] Loss: 1.155 Acc 60.533%\n",
      "Train Epoch [ 26/200]Batch [300/391] Loss: 1.153 Acc 60.722%\n",
      "Test Epoch [ 26/200]Batch [  0/ 79] Loss: 0.952 Acc 70.312%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [ 27/200]Batch [  0/391] Loss: 1.077 Acc 63.281%\n",
      "Train Epoch [ 27/200]Batch [100/391] Loss: 1.156 Acc 60.497%\n",
      "Train Epoch [ 27/200]Batch [200/391] Loss: 1.158 Acc 60.428%\n",
      "Train Epoch [ 27/200]Batch [300/391] Loss: 1.156 Acc 60.582%\n",
      "Test Epoch [ 27/200]Batch [  0/ 79] Loss: 0.965 Acc 69.531%\n",
      "Train Epoch [ 28/200]Batch [  0/391] Loss: 0.920 Acc 68.750%\n",
      "Train Epoch [ 28/200]Batch [100/391] Loss: 1.150 Acc 60.953%\n",
      "Train Epoch [ 28/200]Batch [200/391] Loss: 1.150 Acc 60.914%\n",
      "Train Epoch [ 28/200]Batch [300/391] Loss: 1.143 Acc 61.220%\n",
      "Test Epoch [ 28/200]Batch [  0/ 79] Loss: 0.954 Acc 66.406%\n",
      "Train Epoch [ 29/200]Batch [  0/391] Loss: 0.992 Acc 67.188%\n",
      "Train Epoch [ 29/200]Batch [100/391] Loss: 1.138 Acc 61.502%\n",
      "Train Epoch [ 29/200]Batch [200/391] Loss: 1.134 Acc 61.548%\n",
      "Train Epoch [ 29/200]Batch [300/391] Loss: 1.134 Acc 61.656%\n",
      "Test Epoch [ 29/200]Batch [  0/ 79] Loss: 0.942 Acc 71.875%\n",
      "Train Epoch [ 30/200]Batch [  0/391] Loss: 1.060 Acc 64.062%\n",
      "Train Epoch [ 30/200]Batch [100/391] Loss: 1.133 Acc 61.309%\n",
      "Train Epoch [ 30/200]Batch [200/391] Loss: 1.132 Acc 61.248%\n",
      "Train Epoch [ 30/200]Batch [300/391] Loss: 1.136 Acc 61.252%\n",
      "Test Epoch [ 30/200]Batch [  0/ 79] Loss: 0.939 Acc 67.969%\n",
      "Train Epoch [ 31/200]Batch [  0/391] Loss: 1.070 Acc 63.281%\n",
      "Train Epoch [ 31/200]Batch [100/391] Loss: 1.145 Acc 60.721%\n",
      "Train Epoch [ 31/200]Batch [200/391] Loss: 1.138 Acc 61.276%\n",
      "Train Epoch [ 31/200]Batch [300/391] Loss: 1.138 Acc 61.298%\n",
      "Test Epoch [ 31/200]Batch [  0/ 79] Loss: 0.949 Acc 71.875%\n",
      "Train Epoch [ 32/200]Batch [  0/391] Loss: 1.099 Acc 53.125%\n",
      "Train Epoch [ 32/200]Batch [100/391] Loss: 1.132 Acc 60.968%\n",
      "Train Epoch [ 32/200]Batch [200/391] Loss: 1.131 Acc 61.412%\n",
      "Train Epoch [ 32/200]Batch [300/391] Loss: 1.126 Acc 61.854%\n",
      "Test Epoch [ 32/200]Batch [  0/ 79] Loss: 0.920 Acc 71.094%\n",
      "Train Epoch [ 33/200]Batch [  0/391] Loss: 1.022 Acc 69.531%\n",
      "Train Epoch [ 33/200]Batch [100/391] Loss: 1.138 Acc 61.518%\n",
      "Train Epoch [ 33/200]Batch [200/391] Loss: 1.133 Acc 61.727%\n",
      "Train Epoch [ 33/200]Batch [300/391] Loss: 1.129 Acc 61.672%\n",
      "Test Epoch [ 33/200]Batch [  0/ 79] Loss: 0.920 Acc 69.531%\n",
      "Train Epoch [ 34/200]Batch [  0/391] Loss: 1.233 Acc 59.375%\n",
      "Train Epoch [ 34/200]Batch [100/391] Loss: 1.110 Acc 62.407%\n",
      "Train Epoch [ 34/200]Batch [200/391] Loss: 1.111 Acc 62.477%\n",
      "Train Epoch [ 34/200]Batch [300/391] Loss: 1.122 Acc 62.041%\n",
      "Test Epoch [ 34/200]Batch [  0/ 79] Loss: 0.938 Acc 69.531%\n",
      "Train Epoch [ 35/200]Batch [  0/391] Loss: 1.252 Acc 61.719%\n",
      "Train Epoch [ 35/200]Batch [100/391] Loss: 1.122 Acc 62.028%\n",
      "Train Epoch [ 35/200]Batch [200/391] Loss: 1.120 Acc 61.983%\n",
      "Train Epoch [ 35/200]Batch [300/391] Loss: 1.120 Acc 61.934%\n",
      "Test Epoch [ 35/200]Batch [  0/ 79] Loss: 0.918 Acc 72.656%\n",
      "Train Epoch [ 36/200]Batch [  0/391] Loss: 1.073 Acc 61.719%\n",
      "Train Epoch [ 36/200]Batch [100/391] Loss: 1.125 Acc 61.858%\n",
      "Train Epoch [ 36/200]Batch [200/391] Loss: 1.125 Acc 61.820%\n",
      "Train Epoch [ 36/200]Batch [300/391] Loss: 1.123 Acc 61.926%\n",
      "Test Epoch [ 36/200]Batch [  0/ 79] Loss: 0.924 Acc 68.750%\n",
      "Train Epoch [ 37/200]Batch [  0/391] Loss: 1.071 Acc 62.500%\n",
      "Train Epoch [ 37/200]Batch [100/391] Loss: 1.118 Acc 62.229%\n",
      "Train Epoch [ 37/200]Batch [200/391] Loss: 1.113 Acc 62.267%\n",
      "Train Epoch [ 37/200]Batch [300/391] Loss: 1.116 Acc 62.163%\n",
      "Test Epoch [ 37/200]Batch [  0/ 79] Loss: 0.936 Acc 71.875%\n",
      "Train Epoch [ 38/200]Batch [  0/391] Loss: 1.257 Acc 64.062%\n",
      "Train Epoch [ 38/200]Batch [100/391] Loss: 1.117 Acc 62.129%\n",
      "Train Epoch [ 38/200]Batch [200/391] Loss: 1.113 Acc 62.442%\n",
      "Train Epoch [ 38/200]Batch [300/391] Loss: 1.114 Acc 62.391%\n",
      "Test Epoch [ 38/200]Batch [  0/ 79] Loss: 0.936 Acc 71.875%\n",
      "Train Epoch [ 39/200]Batch [  0/391] Loss: 1.190 Acc 59.375%\n",
      "Train Epoch [ 39/200]Batch [100/391] Loss: 1.119 Acc 61.580%\n",
      "Train Epoch [ 39/200]Batch [200/391] Loss: 1.110 Acc 62.205%\n",
      "Train Epoch [ 39/200]Batch [300/391] Loss: 1.108 Acc 62.285%\n",
      "Test Epoch [ 39/200]Batch [  0/ 79] Loss: 0.919 Acc 74.219%\n",
      "Train Epoch [ 40/200]Batch [  0/391] Loss: 1.035 Acc 68.750%\n",
      "Train Epoch [ 40/200]Batch [100/391] Loss: 1.099 Acc 63.026%\n",
      "Train Epoch [ 40/200]Batch [200/391] Loss: 1.106 Acc 62.407%\n",
      "Train Epoch [ 40/200]Batch [300/391] Loss: 1.107 Acc 62.365%\n",
      "Test Epoch [ 40/200]Batch [  0/ 79] Loss: 0.911 Acc 73.438%\n",
      "Train Epoch [ 41/200]Batch [  0/391] Loss: 1.046 Acc 62.500%\n",
      "Train Epoch [ 41/200]Batch [100/391] Loss: 1.101 Acc 62.392%\n",
      "Train Epoch [ 41/200]Batch [200/391] Loss: 1.100 Acc 62.858%\n",
      "Train Epoch [ 41/200]Batch [300/391] Loss: 1.100 Acc 62.861%\n",
      "Test Epoch [ 41/200]Batch [  0/ 79] Loss: 0.912 Acc 70.312%\n",
      "Train Epoch [ 42/200]Batch [  0/391] Loss: 1.086 Acc 60.156%\n",
      "Train Epoch [ 42/200]Batch [100/391] Loss: 1.099 Acc 62.763%\n",
      "Train Epoch [ 42/200]Batch [200/391] Loss: 1.102 Acc 62.881%\n",
      "Train Epoch [ 42/200]Batch [300/391] Loss: 1.099 Acc 62.858%\n",
      "Test Epoch [ 42/200]Batch [  0/ 79] Loss: 0.908 Acc 70.312%\n",
      "Train Epoch [ 43/200]Batch [  0/391] Loss: 1.110 Acc 66.406%\n",
      "Train Epoch [ 43/200]Batch [100/391] Loss: 1.090 Acc 63.049%\n",
      "Train Epoch [ 43/200]Batch [200/391] Loss: 1.105 Acc 62.345%\n",
      "Train Epoch [ 43/200]Batch [300/391] Loss: 1.102 Acc 62.544%\n",
      "Test Epoch [ 43/200]Batch [  0/ 79] Loss: 0.924 Acc 69.531%\n",
      "Train Epoch [ 44/200]Batch [  0/391] Loss: 1.061 Acc 61.719%\n",
      "Train Epoch [ 44/200]Batch [100/391] Loss: 1.089 Acc 62.469%\n",
      "Train Epoch [ 44/200]Batch [200/391] Loss: 1.091 Acc 62.671%\n",
      "Train Epoch [ 44/200]Batch [300/391] Loss: 1.097 Acc 62.477%\n",
      "Test Epoch [ 44/200]Batch [  0/ 79] Loss: 0.905 Acc 74.219%\n",
      "Train Epoch [ 45/200]Batch [  0/391] Loss: 0.950 Acc 72.656%\n",
      "Train Epoch [ 45/200]Batch [100/391] Loss: 1.093 Acc 63.181%\n",
      "Train Epoch [ 45/200]Batch [200/391] Loss: 1.094 Acc 63.231%\n",
      "Train Epoch [ 45/200]Batch [300/391] Loss: 1.099 Acc 63.040%\n",
      "Test Epoch [ 45/200]Batch [  0/ 79] Loss: 0.903 Acc 73.438%\n",
      "Train Epoch [ 46/200]Batch [  0/391] Loss: 1.117 Acc 62.500%\n",
      "Train Epoch [ 46/200]Batch [100/391] Loss: 1.092 Acc 62.655%\n",
      "Train Epoch [ 46/200]Batch [200/391] Loss: 1.087 Acc 62.877%\n",
      "Train Epoch [ 46/200]Batch [300/391] Loss: 1.090 Acc 62.767%\n",
      "Test Epoch [ 46/200]Batch [  0/ 79] Loss: 0.891 Acc 74.219%\n",
      "Saving..\n",
      "Train Epoch [ 47/200]Batch [  0/391] Loss: 1.135 Acc 64.062%\n",
      "Train Epoch [ 47/200]Batch [100/391] Loss: 1.095 Acc 63.119%\n",
      "Train Epoch [ 47/200]Batch [200/391] Loss: 1.093 Acc 63.060%\n",
      "Train Epoch [ 47/200]Batch [300/391] Loss: 1.092 Acc 62.933%\n",
      "Test Epoch [ 47/200]Batch [  0/ 79] Loss: 0.905 Acc 72.656%\n",
      "Train Epoch [ 48/200]Batch [  0/391] Loss: 1.138 Acc 62.500%\n",
      "Train Epoch [ 48/200]Batch [100/391] Loss: 1.086 Acc 63.065%\n",
      "Train Epoch [ 48/200]Batch [200/391] Loss: 1.081 Acc 63.305%\n",
      "Train Epoch [ 48/200]Batch [300/391] Loss: 1.083 Acc 63.354%\n",
      "Test Epoch [ 48/200]Batch [  0/ 79] Loss: 0.925 Acc 72.656%\n",
      "Train Epoch [ 49/200]Batch [  0/391] Loss: 1.047 Acc 63.281%\n",
      "Train Epoch [ 49/200]Batch [100/391] Loss: 1.083 Acc 63.003%\n",
      "Train Epoch [ 49/200]Batch [200/391] Loss: 1.075 Acc 63.402%\n",
      "Train Epoch [ 49/200]Batch [300/391] Loss: 1.080 Acc 63.183%\n",
      "Test Epoch [ 49/200]Batch [  0/ 79] Loss: 0.887 Acc 74.219%\n",
      "Train Epoch [ 50/200]Batch [  0/391] Loss: 1.102 Acc 64.062%\n",
      "Train Epoch [ 50/200]Batch [100/391] Loss: 1.080 Acc 63.683%\n",
      "Train Epoch [ 50/200]Batch [200/391] Loss: 1.079 Acc 63.573%\n",
      "Train Epoch [ 50/200]Batch [300/391] Loss: 1.082 Acc 63.393%\n",
      "Test Epoch [ 50/200]Batch [  0/ 79] Loss: 0.890 Acc 73.438%\n",
      "Saving..\n",
      "Train Epoch [ 51/200]Batch [  0/391] Loss: 0.984 Acc 67.969%\n",
      "Train Epoch [ 51/200]Batch [100/391] Loss: 1.087 Acc 63.335%\n",
      "Train Epoch [ 51/200]Batch [200/391] Loss: 1.077 Acc 63.522%\n",
      "Train Epoch [ 51/200]Batch [300/391] Loss: 1.082 Acc 63.375%\n",
      "Test Epoch [ 51/200]Batch [  0/ 79] Loss: 0.879 Acc 76.562%\n",
      "Train Epoch [ 52/200]Batch [  0/391] Loss: 1.244 Acc 55.469%\n",
      "Train Epoch [ 52/200]Batch [100/391] Loss: 1.089 Acc 63.498%\n",
      "Train Epoch [ 52/200]Batch [200/391] Loss: 1.084 Acc 63.343%\n",
      "Train Epoch [ 52/200]Batch [300/391] Loss: 1.079 Acc 63.639%\n",
      "Test Epoch [ 52/200]Batch [  0/ 79] Loss: 0.884 Acc 72.656%\n",
      "Train Epoch [ 53/200]Batch [  0/391] Loss: 1.120 Acc 57.812%\n",
      "Train Epoch [ 53/200]Batch [100/391] Loss: 1.075 Acc 63.436%\n",
      "Train Epoch [ 53/200]Batch [200/391] Loss: 1.076 Acc 63.417%\n",
      "Train Epoch [ 53/200]Batch [300/391] Loss: 1.073 Acc 63.385%\n",
      "Test Epoch [ 53/200]Batch [  0/ 79] Loss: 0.875 Acc 74.219%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [ 54/200]Batch [  0/391] Loss: 1.123 Acc 63.281%\n",
      "Train Epoch [ 54/200]Batch [100/391] Loss: 1.067 Acc 63.923%\n",
      "Train Epoch [ 54/200]Batch [200/391] Loss: 1.075 Acc 63.775%\n",
      "Train Epoch [ 54/200]Batch [300/391] Loss: 1.076 Acc 63.629%\n",
      "Test Epoch [ 54/200]Batch [  0/ 79] Loss: 0.889 Acc 73.438%\n",
      "Train Epoch [ 55/200]Batch [  0/391] Loss: 1.152 Acc 60.938%\n",
      "Train Epoch [ 55/200]Batch [100/391] Loss: 1.060 Acc 64.310%\n",
      "Train Epoch [ 55/200]Batch [200/391] Loss: 1.069 Acc 63.806%\n",
      "Train Epoch [ 55/200]Batch [300/391] Loss: 1.071 Acc 63.761%\n",
      "Test Epoch [ 55/200]Batch [  0/ 79] Loss: 0.882 Acc 73.438%\n",
      "Saving..\n",
      "Train Epoch [ 56/200]Batch [  0/391] Loss: 1.102 Acc 60.156%\n",
      "Train Epoch [ 56/200]Batch [100/391] Loss: 1.076 Acc 63.312%\n",
      "Train Epoch [ 56/200]Batch [200/391] Loss: 1.082 Acc 63.056%\n",
      "Train Epoch [ 56/200]Batch [300/391] Loss: 1.075 Acc 63.414%\n",
      "Test Epoch [ 56/200]Batch [  0/ 79] Loss: 0.877 Acc 74.219%\n",
      "Train Epoch [ 57/200]Batch [  0/391] Loss: 0.908 Acc 69.531%\n",
      "Train Epoch [ 57/200]Batch [100/391] Loss: 1.060 Acc 64.225%\n",
      "Train Epoch [ 57/200]Batch [200/391] Loss: 1.067 Acc 63.693%\n",
      "Train Epoch [ 57/200]Batch [300/391] Loss: 1.072 Acc 63.735%\n",
      "Test Epoch [ 57/200]Batch [  0/ 79] Loss: 0.868 Acc 73.438%\n",
      "Train Epoch [ 58/200]Batch [  0/391] Loss: 1.038 Acc 62.500%\n",
      "Train Epoch [ 58/200]Batch [100/391] Loss: 1.068 Acc 63.312%\n",
      "Train Epoch [ 58/200]Batch [200/391] Loss: 1.069 Acc 63.623%\n",
      "Train Epoch [ 58/200]Batch [300/391] Loss: 1.067 Acc 63.678%\n",
      "Test Epoch [ 58/200]Batch [  0/ 79] Loss: 0.889 Acc 72.656%\n",
      "Train Epoch [ 59/200]Batch [  0/391] Loss: 1.150 Acc 58.594%\n",
      "Train Epoch [ 59/200]Batch [100/391] Loss: 1.077 Acc 63.637%\n",
      "Train Epoch [ 59/200]Batch [200/391] Loss: 1.070 Acc 63.783%\n",
      "Train Epoch [ 59/200]Batch [300/391] Loss: 1.070 Acc 63.819%\n",
      "Test Epoch [ 59/200]Batch [  0/ 79] Loss: 0.897 Acc 73.438%\n",
      "Train Epoch [ 60/200]Batch [  0/391] Loss: 0.915 Acc 66.406%\n",
      "Train Epoch [ 60/200]Batch [100/391] Loss: 1.053 Acc 63.908%\n",
      "Train Epoch [ 60/200]Batch [200/391] Loss: 1.061 Acc 63.705%\n",
      "Train Epoch [ 60/200]Batch [300/391] Loss: 1.059 Acc 63.808%\n",
      "Test Epoch [ 60/200]Batch [  0/ 79] Loss: 0.877 Acc 74.219%\n",
      "Train Epoch [ 61/200]Batch [  0/391] Loss: 1.194 Acc 65.625%\n",
      "Train Epoch [ 61/200]Batch [100/391] Loss: 1.071 Acc 63.544%\n",
      "Train Epoch [ 61/200]Batch [200/391] Loss: 1.064 Acc 63.685%\n",
      "Train Epoch [ 61/200]Batch [300/391] Loss: 1.064 Acc 63.767%\n",
      "Test Epoch [ 61/200]Batch [  0/ 79] Loss: 0.872 Acc 75.000%\n",
      "Train Epoch [ 62/200]Batch [  0/391] Loss: 1.052 Acc 66.406%\n",
      "Train Epoch [ 62/200]Batch [100/391] Loss: 1.058 Acc 63.583%\n",
      "Train Epoch [ 62/200]Batch [200/391] Loss: 1.062 Acc 63.755%\n",
      "Train Epoch [ 62/200]Batch [300/391] Loss: 1.058 Acc 63.907%\n",
      "Test Epoch [ 62/200]Batch [  0/ 79] Loss: 0.868 Acc 75.000%\n",
      "Train Epoch [ 63/200]Batch [  0/391] Loss: 0.987 Acc 68.750%\n",
      "Train Epoch [ 63/200]Batch [100/391] Loss: 1.056 Acc 63.869%\n",
      "Train Epoch [ 63/200]Batch [200/391] Loss: 1.060 Acc 63.654%\n",
      "Train Epoch [ 63/200]Batch [300/391] Loss: 1.061 Acc 63.793%\n",
      "Test Epoch [ 63/200]Batch [  0/ 79] Loss: 0.890 Acc 74.219%\n",
      "Train Epoch [ 64/200]Batch [  0/391] Loss: 1.261 Acc 58.594%\n",
      "Train Epoch [ 64/200]Batch [100/391] Loss: 1.084 Acc 63.227%\n",
      "Train Epoch [ 64/200]Batch [200/391] Loss: 1.064 Acc 63.689%\n",
      "Train Epoch [ 64/200]Batch [300/391] Loss: 1.061 Acc 63.722%\n",
      "Test Epoch [ 64/200]Batch [  0/ 79] Loss: 0.863 Acc 72.656%\n",
      "Train Epoch [ 65/200]Batch [  0/391] Loss: 1.128 Acc 61.719%\n",
      "Train Epoch [ 65/200]Batch [100/391] Loss: 1.055 Acc 64.712%\n",
      "Train Epoch [ 65/200]Batch [200/391] Loss: 1.058 Acc 64.062%\n",
      "Train Epoch [ 65/200]Batch [300/391] Loss: 1.056 Acc 64.226%\n",
      "Test Epoch [ 65/200]Batch [  0/ 79] Loss: 0.868 Acc 75.000%\n",
      "Saving..\n",
      "Train Epoch [ 66/200]Batch [  0/391] Loss: 0.948 Acc 65.625%\n",
      "Train Epoch [ 66/200]Batch [100/391] Loss: 1.067 Acc 63.823%\n",
      "Train Epoch [ 66/200]Batch [200/391] Loss: 1.053 Acc 64.257%\n",
      "Train Epoch [ 66/200]Batch [300/391] Loss: 1.053 Acc 64.218%\n",
      "Test Epoch [ 66/200]Batch [  0/ 79] Loss: 0.876 Acc 75.000%\n",
      "Train Epoch [ 67/200]Batch [  0/391] Loss: 0.931 Acc 63.281%\n",
      "Train Epoch [ 67/200]Batch [100/391] Loss: 1.050 Acc 64.209%\n",
      "Train Epoch [ 67/200]Batch [200/391] Loss: 1.049 Acc 64.136%\n",
      "Train Epoch [ 67/200]Batch [300/391] Loss: 1.049 Acc 64.306%\n",
      "Test Epoch [ 67/200]Batch [  0/ 79] Loss: 0.863 Acc 71.875%\n",
      "Train Epoch [ 68/200]Batch [  0/391] Loss: 1.156 Acc 64.062%\n",
      "Train Epoch [ 68/200]Batch [100/391] Loss: 1.068 Acc 63.931%\n",
      "Train Epoch [ 68/200]Batch [200/391] Loss: 1.058 Acc 64.253%\n",
      "Train Epoch [ 68/200]Batch [300/391] Loss: 1.055 Acc 64.281%\n",
      "Test Epoch [ 68/200]Batch [  0/ 79] Loss: 0.855 Acc 75.000%\n",
      "Saving..\n",
      "Train Epoch [ 69/200]Batch [  0/391] Loss: 1.167 Acc 60.156%\n",
      "Train Epoch [ 69/200]Batch [100/391] Loss: 1.050 Acc 64.426%\n",
      "Train Epoch [ 69/200]Batch [200/391] Loss: 1.054 Acc 64.214%\n",
      "Train Epoch [ 69/200]Batch [300/391] Loss: 1.051 Acc 64.405%\n",
      "Test Epoch [ 69/200]Batch [  0/ 79] Loss: 0.843 Acc 73.438%\n",
      "Train Epoch [ 70/200]Batch [  0/391] Loss: 1.112 Acc 59.375%\n",
      "Train Epoch [ 70/200]Batch [100/391] Loss: 1.057 Acc 64.109%\n",
      "Train Epoch [ 70/200]Batch [200/391] Loss: 1.058 Acc 64.012%\n",
      "Train Epoch [ 70/200]Batch [300/391] Loss: 1.052 Acc 64.192%\n",
      "Test Epoch [ 70/200]Batch [  0/ 79] Loss: 0.871 Acc 71.875%\n",
      "Train Epoch [ 71/200]Batch [  0/391] Loss: 1.006 Acc 69.531%\n",
      "Train Epoch [ 71/200]Batch [100/391] Loss: 1.054 Acc 64.341%\n",
      "Train Epoch [ 71/200]Batch [200/391] Loss: 1.052 Acc 64.191%\n",
      "Train Epoch [ 71/200]Batch [300/391] Loss: 1.051 Acc 64.322%\n",
      "Test Epoch [ 71/200]Batch [  0/ 79] Loss: 0.850 Acc 75.781%\n",
      "Train Epoch [ 72/200]Batch [  0/391] Loss: 0.996 Acc 66.406%\n",
      "Train Epoch [ 72/200]Batch [100/391] Loss: 1.042 Acc 64.171%\n",
      "Train Epoch [ 72/200]Batch [200/391] Loss: 1.048 Acc 64.405%\n",
      "Train Epoch [ 72/200]Batch [300/391] Loss: 1.044 Acc 64.792%\n",
      "Test Epoch [ 72/200]Batch [  0/ 79] Loss: 0.875 Acc 74.219%\n",
      "Train Epoch [ 73/200]Batch [  0/391] Loss: 1.180 Acc 55.469%\n",
      "Train Epoch [ 73/200]Batch [100/391] Loss: 1.041 Acc 64.743%\n",
      "Train Epoch [ 73/200]Batch [200/391] Loss: 1.044 Acc 64.793%\n",
      "Train Epoch [ 73/200]Batch [300/391] Loss: 1.043 Acc 64.532%\n",
      "Test Epoch [ 73/200]Batch [  0/ 79] Loss: 0.844 Acc 75.000%\n",
      "Train Epoch [ 74/200]Batch [  0/391] Loss: 1.104 Acc 67.188%\n",
      "Train Epoch [ 74/200]Batch [100/391] Loss: 1.054 Acc 64.681%\n",
      "Train Epoch [ 74/200]Batch [200/391] Loss: 1.048 Acc 64.475%\n",
      "Train Epoch [ 74/200]Batch [300/391] Loss: 1.042 Acc 64.574%\n",
      "Test Epoch [ 74/200]Batch [  0/ 79] Loss: 0.853 Acc 75.781%\n",
      "Train Epoch [ 75/200]Batch [  0/391] Loss: 1.054 Acc 64.844%\n",
      "Train Epoch [ 75/200]Batch [100/391] Loss: 1.040 Acc 64.921%\n",
      "Train Epoch [ 75/200]Batch [200/391] Loss: 1.039 Acc 64.661%\n",
      "Train Epoch [ 75/200]Batch [300/391] Loss: 1.041 Acc 64.672%\n",
      "Test Epoch [ 75/200]Batch [  0/ 79] Loss: 0.848 Acc 74.219%\n",
      "Train Epoch [ 76/200]Batch [  0/391] Loss: 1.133 Acc 60.938%\n",
      "Train Epoch [ 76/200]Batch [100/391] Loss: 1.052 Acc 64.442%\n",
      "Train Epoch [ 76/200]Batch [200/391] Loss: 1.044 Acc 64.451%\n",
      "Train Epoch [ 76/200]Batch [300/391] Loss: 1.042 Acc 64.613%\n",
      "Test Epoch [ 76/200]Batch [  0/ 79] Loss: 0.842 Acc 74.219%\n",
      "Saving..\n",
      "Train Epoch [ 77/200]Batch [  0/391] Loss: 1.093 Acc 63.281%\n",
      "Train Epoch [ 77/200]Batch [100/391] Loss: 1.038 Acc 64.588%\n",
      "Train Epoch [ 77/200]Batch [200/391] Loss: 1.048 Acc 64.292%\n",
      "Train Epoch [ 77/200]Batch [300/391] Loss: 1.045 Acc 64.348%\n",
      "Test Epoch [ 77/200]Batch [  0/ 79] Loss: 0.867 Acc 74.219%\n",
      "Train Epoch [ 78/200]Batch [  0/391] Loss: 1.027 Acc 61.719%\n",
      "Train Epoch [ 78/200]Batch [100/391] Loss: 1.036 Acc 65.014%\n",
      "Train Epoch [ 78/200]Batch [200/391] Loss: 1.039 Acc 64.809%\n",
      "Train Epoch [ 78/200]Batch [300/391] Loss: 1.039 Acc 64.732%\n",
      "Test Epoch [ 78/200]Batch [  0/ 79] Loss: 0.855 Acc 75.000%\n",
      "Train Epoch [ 79/200]Batch [  0/391] Loss: 0.923 Acc 69.531%\n",
      "Train Epoch [ 79/200]Batch [100/391] Loss: 1.047 Acc 64.163%\n",
      "Train Epoch [ 79/200]Batch [200/391] Loss: 1.042 Acc 64.513%\n",
      "Train Epoch [ 79/200]Batch [300/391] Loss: 1.044 Acc 64.366%\n",
      "Test Epoch [ 79/200]Batch [  0/ 79] Loss: 0.851 Acc 74.219%\n",
      "Train Epoch [ 80/200]Batch [  0/391] Loss: 1.120 Acc 64.844%\n",
      "Train Epoch [ 80/200]Batch [100/391] Loss: 1.052 Acc 64.310%\n",
      "Train Epoch [ 80/200]Batch [200/391] Loss: 1.040 Acc 64.568%\n",
      "Train Epoch [ 80/200]Batch [300/391] Loss: 1.031 Acc 64.719%\n",
      "Test Epoch [ 80/200]Batch [  0/ 79] Loss: 0.861 Acc 72.656%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [ 81/200]Batch [  0/391] Loss: 0.952 Acc 64.062%\n",
      "Train Epoch [ 81/200]Batch [100/391] Loss: 1.067 Acc 63.660%\n",
      "Train Epoch [ 81/200]Batch [200/391] Loss: 1.050 Acc 64.354%\n",
      "Train Epoch [ 81/200]Batch [300/391] Loss: 1.044 Acc 64.423%\n",
      "Test Epoch [ 81/200]Batch [  0/ 79] Loss: 0.858 Acc 75.000%\n",
      "Saving..\n",
      "Train Epoch [ 82/200]Batch [  0/391] Loss: 1.062 Acc 60.156%\n",
      "Train Epoch [ 82/200]Batch [100/391] Loss: 1.028 Acc 64.991%\n",
      "Train Epoch [ 82/200]Batch [200/391] Loss: 1.035 Acc 64.797%\n",
      "Train Epoch [ 82/200]Batch [300/391] Loss: 1.037 Acc 64.883%\n",
      "Test Epoch [ 82/200]Batch [  0/ 79] Loss: 0.858 Acc 76.562%\n",
      "Train Epoch [ 83/200]Batch [  0/391] Loss: 0.966 Acc 68.750%\n",
      "Train Epoch [ 83/200]Batch [100/391] Loss: 1.034 Acc 65.022%\n",
      "Train Epoch [ 83/200]Batch [200/391] Loss: 1.037 Acc 64.859%\n",
      "Train Epoch [ 83/200]Batch [300/391] Loss: 1.030 Acc 64.974%\n",
      "Test Epoch [ 83/200]Batch [  0/ 79] Loss: 0.849 Acc 74.219%\n",
      "Train Epoch [ 84/200]Batch [  0/391] Loss: 0.892 Acc 69.531%\n",
      "Train Epoch [ 84/200]Batch [100/391] Loss: 1.028 Acc 65.099%\n",
      "Train Epoch [ 84/200]Batch [200/391] Loss: 1.030 Acc 65.077%\n",
      "Train Epoch [ 84/200]Batch [300/391] Loss: 1.031 Acc 64.898%\n",
      "Test Epoch [ 84/200]Batch [  0/ 79] Loss: 0.833 Acc 75.781%\n",
      "Train Epoch [ 85/200]Batch [  0/391] Loss: 1.133 Acc 55.469%\n",
      "Train Epoch [ 85/200]Batch [100/391] Loss: 1.029 Acc 64.387%\n",
      "Train Epoch [ 85/200]Batch [200/391] Loss: 1.028 Acc 64.704%\n",
      "Train Epoch [ 85/200]Batch [300/391] Loss: 1.032 Acc 64.717%\n",
      "Test Epoch [ 85/200]Batch [  0/ 79] Loss: 0.834 Acc 75.781%\n",
      "Train Epoch [ 86/200]Batch [  0/391] Loss: 1.046 Acc 63.281%\n",
      "Train Epoch [ 86/200]Batch [100/391] Loss: 1.001 Acc 66.058%\n",
      "Train Epoch [ 86/200]Batch [200/391] Loss: 1.019 Acc 65.481%\n",
      "Train Epoch [ 86/200]Batch [300/391] Loss: 1.023 Acc 65.339%\n",
      "Test Epoch [ 86/200]Batch [  0/ 79] Loss: 0.830 Acc 75.000%\n",
      "Saving..\n",
      "Train Epoch [ 87/200]Batch [  0/391] Loss: 1.019 Acc 63.281%\n",
      "Train Epoch [ 87/200]Batch [100/391] Loss: 1.022 Acc 65.192%\n",
      "Train Epoch [ 87/200]Batch [200/391] Loss: 1.025 Acc 65.252%\n",
      "Train Epoch [ 87/200]Batch [300/391] Loss: 1.029 Acc 65.137%\n",
      "Test Epoch [ 87/200]Batch [  0/ 79] Loss: 0.854 Acc 76.562%\n",
      "Train Epoch [ 88/200]Batch [  0/391] Loss: 1.021 Acc 60.156%\n",
      "Train Epoch [ 88/200]Batch [100/391] Loss: 1.020 Acc 65.238%\n",
      "Train Epoch [ 88/200]Batch [200/391] Loss: 1.032 Acc 64.898%\n",
      "Train Epoch [ 88/200]Batch [300/391] Loss: 1.028 Acc 65.051%\n",
      "Test Epoch [ 88/200]Batch [  0/ 79] Loss: 0.846 Acc 74.219%\n",
      "Train Epoch [ 89/200]Batch [  0/391] Loss: 0.988 Acc 73.438%\n",
      "Train Epoch [ 89/200]Batch [100/391] Loss: 1.024 Acc 65.640%\n",
      "Train Epoch [ 89/200]Batch [200/391] Loss: 1.024 Acc 65.442%\n",
      "Train Epoch [ 89/200]Batch [300/391] Loss: 1.024 Acc 65.363%\n",
      "Test Epoch [ 89/200]Batch [  0/ 79] Loss: 0.835 Acc 78.125%\n",
      "Train Epoch [ 90/200]Batch [  0/391] Loss: 0.975 Acc 67.188%\n",
      "Train Epoch [ 90/200]Batch [100/391] Loss: 1.048 Acc 64.217%\n",
      "Train Epoch [ 90/200]Batch [200/391] Loss: 1.033 Acc 65.003%\n",
      "Train Epoch [ 90/200]Batch [300/391] Loss: 1.033 Acc 65.038%\n",
      "Test Epoch [ 90/200]Batch [  0/ 79] Loss: 0.841 Acc 75.000%\n",
      "Train Epoch [ 91/200]Batch [  0/391] Loss: 1.031 Acc 67.188%\n",
      "Train Epoch [ 91/200]Batch [100/391] Loss: 1.027 Acc 64.882%\n",
      "Train Epoch [ 91/200]Batch [200/391] Loss: 1.025 Acc 65.085%\n",
      "Train Epoch [ 91/200]Batch [300/391] Loss: 1.027 Acc 65.163%\n",
      "Test Epoch [ 91/200]Batch [  0/ 79] Loss: 0.850 Acc 74.219%\n",
      "Train Epoch [ 92/200]Batch [  0/391] Loss: 1.019 Acc 67.188%\n",
      "Train Epoch [ 92/200]Batch [100/391] Loss: 1.029 Acc 65.308%\n",
      "Train Epoch [ 92/200]Batch [200/391] Loss: 1.028 Acc 65.112%\n",
      "Train Epoch [ 92/200]Batch [300/391] Loss: 1.026 Acc 65.342%\n",
      "Test Epoch [ 92/200]Batch [  0/ 79] Loss: 0.834 Acc 75.781%\n",
      "Train Epoch [ 93/200]Batch [  0/391] Loss: 0.947 Acc 70.312%\n",
      "Train Epoch [ 93/200]Batch [100/391] Loss: 1.021 Acc 65.107%\n",
      "Train Epoch [ 93/200]Batch [200/391] Loss: 1.024 Acc 65.454%\n",
      "Train Epoch [ 93/200]Batch [300/391] Loss: 1.023 Acc 65.430%\n",
      "Test Epoch [ 93/200]Batch [  0/ 79] Loss: 0.844 Acc 73.438%\n",
      "Train Epoch [ 94/200]Batch [  0/391] Loss: 0.973 Acc 67.969%\n",
      "Train Epoch [ 94/200]Batch [100/391] Loss: 1.016 Acc 65.548%\n",
      "Train Epoch [ 94/200]Batch [200/391] Loss: 1.026 Acc 65.407%\n",
      "Train Epoch [ 94/200]Batch [300/391] Loss: 1.027 Acc 65.365%\n",
      "Test Epoch [ 94/200]Batch [  0/ 79] Loss: 0.853 Acc 74.219%\n",
      "Train Epoch [ 95/200]Batch [  0/391] Loss: 1.016 Acc 65.625%\n",
      "Train Epoch [ 95/200]Batch [100/391] Loss: 1.034 Acc 64.735%\n",
      "Train Epoch [ 95/200]Batch [200/391] Loss: 1.033 Acc 64.898%\n",
      "Train Epoch [ 95/200]Batch [300/391] Loss: 1.029 Acc 64.961%\n",
      "Test Epoch [ 95/200]Batch [  0/ 79] Loss: 0.830 Acc 74.219%\n",
      "Train Epoch [ 96/200]Batch [  0/391] Loss: 0.893 Acc 69.531%\n",
      "Train Epoch [ 96/200]Batch [100/391] Loss: 1.017 Acc 66.174%\n",
      "Train Epoch [ 96/200]Batch [200/391] Loss: 1.012 Acc 66.130%\n",
      "Train Epoch [ 96/200]Batch [300/391] Loss: 1.016 Acc 65.840%\n",
      "Test Epoch [ 96/200]Batch [  0/ 79] Loss: 0.836 Acc 73.438%\n",
      "Train Epoch [ 97/200]Batch [  0/391] Loss: 1.001 Acc 68.750%\n",
      "Train Epoch [ 97/200]Batch [100/391] Loss: 1.032 Acc 65.091%\n",
      "Train Epoch [ 97/200]Batch [200/391] Loss: 1.031 Acc 64.949%\n",
      "Train Epoch [ 97/200]Batch [300/391] Loss: 1.023 Acc 65.194%\n",
      "Test Epoch [ 97/200]Batch [  0/ 79] Loss: 0.839 Acc 78.125%\n",
      "Saving..\n",
      "Train Epoch [ 98/200]Batch [  0/391] Loss: 0.987 Acc 67.188%\n",
      "Train Epoch [ 98/200]Batch [100/391] Loss: 1.017 Acc 65.880%\n",
      "Train Epoch [ 98/200]Batch [200/391] Loss: 1.027 Acc 65.330%\n",
      "Train Epoch [ 98/200]Batch [300/391] Loss: 1.026 Acc 65.251%\n",
      "Test Epoch [ 98/200]Batch [  0/ 79] Loss: 0.844 Acc 74.219%\n",
      "Train Epoch [ 99/200]Batch [  0/391] Loss: 1.116 Acc 62.500%\n",
      "Train Epoch [ 99/200]Batch [100/391] Loss: 1.020 Acc 65.169%\n",
      "Train Epoch [ 99/200]Batch [200/391] Loss: 1.015 Acc 65.551%\n",
      "Train Epoch [ 99/200]Batch [300/391] Loss: 1.016 Acc 65.539%\n",
      "Test Epoch [ 99/200]Batch [  0/ 79] Loss: 0.826 Acc 73.438%\n",
      "Saving..\n",
      "Train Epoch [100/200]Batch [  0/391] Loss: 1.096 Acc 61.719%\n",
      "Train Epoch [100/200]Batch [100/391] Loss: 1.017 Acc 65.571%\n",
      "Train Epoch [100/200]Batch [200/391] Loss: 1.022 Acc 65.314%\n",
      "Train Epoch [100/200]Batch [300/391] Loss: 1.019 Acc 65.332%\n",
      "Test Epoch [100/200]Batch [  0/ 79] Loss: 0.827 Acc 75.781%\n",
      "Train Epoch [101/200]Batch [  0/391] Loss: 0.972 Acc 71.094%\n",
      "Train Epoch [101/200]Batch [100/391] Loss: 1.033 Acc 64.650%\n",
      "Train Epoch [101/200]Batch [200/391] Loss: 1.019 Acc 65.209%\n",
      "Train Epoch [101/200]Batch [300/391] Loss: 1.019 Acc 65.308%\n",
      "Test Epoch [101/200]Batch [  0/ 79] Loss: 0.849 Acc 75.000%\n",
      "Saving..\n",
      "Train Epoch [102/200]Batch [  0/391] Loss: 1.006 Acc 62.500%\n",
      "Train Epoch [102/200]Batch [100/391] Loss: 1.025 Acc 65.068%\n",
      "Train Epoch [102/200]Batch [200/391] Loss: 1.020 Acc 65.147%\n",
      "Train Epoch [102/200]Batch [300/391] Loss: 1.023 Acc 65.036%\n",
      "Test Epoch [102/200]Batch [  0/ 79] Loss: 0.832 Acc 77.344%\n",
      "Train Epoch [103/200]Batch [  0/391] Loss: 1.042 Acc 68.750%\n",
      "Train Epoch [103/200]Batch [100/391] Loss: 0.999 Acc 66.445%\n",
      "Train Epoch [103/200]Batch [200/391] Loss: 1.000 Acc 66.426%\n",
      "Train Epoch [103/200]Batch [300/391] Loss: 1.007 Acc 66.014%\n",
      "Test Epoch [103/200]Batch [  0/ 79] Loss: 0.822 Acc 74.219%\n",
      "Train Epoch [104/200]Batch [  0/391] Loss: 1.020 Acc 67.969%\n",
      "Train Epoch [104/200]Batch [100/391] Loss: 1.009 Acc 65.826%\n",
      "Train Epoch [104/200]Batch [200/391] Loss: 1.009 Acc 65.777%\n",
      "Train Epoch [104/200]Batch [300/391] Loss: 1.009 Acc 65.729%\n",
      "Test Epoch [104/200]Batch [  0/ 79] Loss: 0.833 Acc 75.000%\n",
      "Train Epoch [105/200]Batch [  0/391] Loss: 1.007 Acc 64.844%\n",
      "Train Epoch [105/200]Batch [100/391] Loss: 1.008 Acc 65.370%\n",
      "Train Epoch [105/200]Batch [200/391] Loss: 1.009 Acc 65.722%\n",
      "Train Epoch [105/200]Batch [300/391] Loss: 1.015 Acc 65.485%\n",
      "Test Epoch [105/200]Batch [  0/ 79] Loss: 0.809 Acc 75.781%\n",
      "Train Epoch [106/200]Batch [  0/391] Loss: 1.097 Acc 61.719%\n",
      "Train Epoch [106/200]Batch [100/391] Loss: 1.012 Acc 65.238%\n",
      "Train Epoch [106/200]Batch [200/391] Loss: 1.020 Acc 65.108%\n",
      "Train Epoch [106/200]Batch [300/391] Loss: 1.019 Acc 65.205%\n",
      "Test Epoch [106/200]Batch [  0/ 79] Loss: 0.846 Acc 75.000%\n",
      "Train Epoch [107/200]Batch [  0/391] Loss: 1.155 Acc 65.625%\n",
      "Train Epoch [107/200]Batch [100/391] Loss: 1.010 Acc 65.803%\n",
      "Train Epoch [107/200]Batch [200/391] Loss: 1.007 Acc 65.986%\n",
      "Train Epoch [107/200]Batch [300/391] Loss: 1.011 Acc 65.776%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [107/200]Batch [  0/ 79] Loss: 0.829 Acc 75.000%\n",
      "Train Epoch [108/200]Batch [  0/391] Loss: 0.969 Acc 64.062%\n",
      "Train Epoch [108/200]Batch [100/391] Loss: 1.012 Acc 65.973%\n",
      "Train Epoch [108/200]Batch [200/391] Loss: 1.016 Acc 65.827%\n",
      "Train Epoch [108/200]Batch [300/391] Loss: 1.012 Acc 65.840%\n",
      "Test Epoch [108/200]Batch [  0/ 79] Loss: 0.823 Acc 74.219%\n",
      "Train Epoch [109/200]Batch [  0/391] Loss: 1.081 Acc 65.625%\n",
      "Train Epoch [109/200]Batch [100/391] Loss: 1.017 Acc 65.323%\n",
      "Train Epoch [109/200]Batch [200/391] Loss: 1.009 Acc 65.574%\n",
      "Train Epoch [109/200]Batch [300/391] Loss: 1.010 Acc 65.586%\n",
      "Test Epoch [109/200]Batch [  0/ 79] Loss: 0.805 Acc 75.000%\n",
      "Train Epoch [110/200]Batch [  0/391] Loss: 1.140 Acc 59.375%\n",
      "Train Epoch [110/200]Batch [100/391] Loss: 1.001 Acc 65.695%\n",
      "Train Epoch [110/200]Batch [200/391] Loss: 1.011 Acc 65.683%\n",
      "Train Epoch [110/200]Batch [300/391] Loss: 1.011 Acc 65.612%\n",
      "Test Epoch [110/200]Batch [  0/ 79] Loss: 0.809 Acc 76.562%\n",
      "Saving..\n",
      "Train Epoch [111/200]Batch [  0/391] Loss: 1.024 Acc 69.531%\n",
      "Train Epoch [111/200]Batch [100/391] Loss: 1.012 Acc 66.166%\n",
      "Train Epoch [111/200]Batch [200/391] Loss: 1.008 Acc 66.099%\n",
      "Train Epoch [111/200]Batch [300/391] Loss: 1.004 Acc 66.183%\n",
      "Test Epoch [111/200]Batch [  0/ 79] Loss: 0.814 Acc 74.219%\n",
      "Train Epoch [112/200]Batch [  0/391] Loss: 0.987 Acc 62.500%\n",
      "Train Epoch [112/200]Batch [100/391] Loss: 1.001 Acc 65.996%\n",
      "Train Epoch [112/200]Batch [200/391] Loss: 1.001 Acc 66.157%\n",
      "Train Epoch [112/200]Batch [300/391] Loss: 1.005 Acc 65.983%\n",
      "Test Epoch [112/200]Batch [  0/ 79] Loss: 0.809 Acc 75.000%\n",
      "Train Epoch [113/200]Batch [  0/391] Loss: 0.893 Acc 70.312%\n",
      "Train Epoch [113/200]Batch [100/391] Loss: 1.008 Acc 66.143%\n",
      "Train Epoch [113/200]Batch [200/391] Loss: 1.006 Acc 66.010%\n",
      "Train Epoch [113/200]Batch [300/391] Loss: 1.009 Acc 65.944%\n",
      "Test Epoch [113/200]Batch [  0/ 79] Loss: 0.843 Acc 73.438%\n",
      "Train Epoch [114/200]Batch [  0/391] Loss: 0.920 Acc 68.750%\n",
      "Train Epoch [114/200]Batch [100/391] Loss: 0.995 Acc 66.035%\n",
      "Train Epoch [114/200]Batch [200/391] Loss: 1.005 Acc 66.014%\n",
      "Train Epoch [114/200]Batch [300/391] Loss: 1.005 Acc 65.801%\n",
      "Test Epoch [114/200]Batch [  0/ 79] Loss: 0.837 Acc 76.562%\n",
      "Train Epoch [115/200]Batch [  0/391] Loss: 0.909 Acc 70.312%\n",
      "Train Epoch [115/200]Batch [100/391] Loss: 0.997 Acc 66.468%\n",
      "Train Epoch [115/200]Batch [200/391] Loss: 1.007 Acc 65.924%\n",
      "Train Epoch [115/200]Batch [300/391] Loss: 1.008 Acc 65.778%\n",
      "Test Epoch [115/200]Batch [  0/ 79] Loss: 0.831 Acc 74.219%\n",
      "Train Epoch [116/200]Batch [  0/391] Loss: 1.038 Acc 66.406%\n",
      "Train Epoch [116/200]Batch [100/391] Loss: 1.017 Acc 65.439%\n",
      "Train Epoch [116/200]Batch [200/391] Loss: 1.002 Acc 65.986%\n",
      "Train Epoch [116/200]Batch [300/391] Loss: 1.000 Acc 66.061%\n",
      "Test Epoch [116/200]Batch [  0/ 79] Loss: 0.806 Acc 75.781%\n",
      "Train Epoch [117/200]Batch [  0/391] Loss: 0.859 Acc 67.969%\n",
      "Train Epoch [117/200]Batch [100/391] Loss: 0.997 Acc 65.958%\n",
      "Train Epoch [117/200]Batch [200/391] Loss: 1.011 Acc 65.823%\n",
      "Train Epoch [117/200]Batch [300/391] Loss: 1.001 Acc 66.121%\n",
      "Test Epoch [117/200]Batch [  0/ 79] Loss: 0.811 Acc 75.781%\n",
      "Train Epoch [118/200]Batch [  0/391] Loss: 1.037 Acc 63.281%\n",
      "Train Epoch [118/200]Batch [100/391] Loss: 1.014 Acc 65.509%\n",
      "Train Epoch [118/200]Batch [200/391] Loss: 1.011 Acc 65.473%\n",
      "Train Epoch [118/200]Batch [300/391] Loss: 1.009 Acc 65.591%\n",
      "Test Epoch [118/200]Batch [  0/ 79] Loss: 0.839 Acc 73.438%\n",
      "Train Epoch [119/200]Batch [  0/391] Loss: 0.975 Acc 65.625%\n",
      "Train Epoch [119/200]Batch [100/391] Loss: 0.993 Acc 66.267%\n",
      "Train Epoch [119/200]Batch [200/391] Loss: 0.994 Acc 66.115%\n",
      "Train Epoch [119/200]Batch [300/391] Loss: 0.999 Acc 65.895%\n",
      "Test Epoch [119/200]Batch [  0/ 79] Loss: 0.798 Acc 74.219%\n",
      "Train Epoch [120/200]Batch [  0/391] Loss: 0.976 Acc 66.406%\n",
      "Train Epoch [120/200]Batch [100/391] Loss: 0.999 Acc 66.205%\n",
      "Train Epoch [120/200]Batch [200/391] Loss: 1.000 Acc 66.006%\n",
      "Train Epoch [120/200]Batch [300/391] Loss: 1.003 Acc 65.952%\n",
      "Test Epoch [120/200]Batch [  0/ 79] Loss: 0.858 Acc 75.000%\n",
      "Train Epoch [121/200]Batch [  0/391] Loss: 1.084 Acc 65.625%\n",
      "Train Epoch [121/200]Batch [100/391] Loss: 1.009 Acc 65.671%\n",
      "Train Epoch [121/200]Batch [200/391] Loss: 1.010 Acc 65.753%\n",
      "Train Epoch [121/200]Batch [300/391] Loss: 1.006 Acc 65.859%\n",
      "Test Epoch [121/200]Batch [  0/ 79] Loss: 0.844 Acc 74.219%\n",
      "Train Epoch [122/200]Batch [  0/391] Loss: 0.947 Acc 65.625%\n",
      "Train Epoch [122/200]Batch [100/391] Loss: 1.011 Acc 66.050%\n",
      "Train Epoch [122/200]Batch [200/391] Loss: 1.003 Acc 65.990%\n",
      "Train Epoch [122/200]Batch [300/391] Loss: 1.005 Acc 66.105%\n",
      "Test Epoch [122/200]Batch [  0/ 79] Loss: 0.830 Acc 75.781%\n",
      "Train Epoch [123/200]Batch [  0/391] Loss: 1.114 Acc 58.594%\n",
      "Train Epoch [123/200]Batch [100/391] Loss: 1.009 Acc 65.068%\n",
      "Train Epoch [123/200]Batch [200/391] Loss: 1.007 Acc 65.637%\n",
      "Train Epoch [123/200]Batch [300/391] Loss: 1.007 Acc 65.641%\n",
      "Test Epoch [123/200]Batch [  0/ 79] Loss: 0.848 Acc 72.656%\n",
      "Train Epoch [124/200]Batch [  0/391] Loss: 0.891 Acc 68.750%\n",
      "Train Epoch [124/200]Batch [100/391] Loss: 1.007 Acc 65.965%\n",
      "Train Epoch [124/200]Batch [200/391] Loss: 1.007 Acc 65.850%\n",
      "Train Epoch [124/200]Batch [300/391] Loss: 1.006 Acc 65.752%\n",
      "Test Epoch [124/200]Batch [  0/ 79] Loss: 0.839 Acc 75.781%\n",
      "Saving..\n",
      "Train Epoch [125/200]Batch [  0/391] Loss: 1.069 Acc 57.031%\n",
      "Train Epoch [125/200]Batch [100/391] Loss: 0.998 Acc 65.842%\n",
      "Train Epoch [125/200]Batch [200/391] Loss: 0.999 Acc 65.749%\n",
      "Train Epoch [125/200]Batch [300/391] Loss: 1.002 Acc 65.648%\n",
      "Test Epoch [125/200]Batch [  0/ 79] Loss: 0.810 Acc 76.562%\n",
      "Train Epoch [126/200]Batch [  0/391] Loss: 1.006 Acc 64.844%\n",
      "Train Epoch [126/200]Batch [100/391] Loss: 0.992 Acc 66.228%\n",
      "Train Epoch [126/200]Batch [200/391] Loss: 0.996 Acc 66.049%\n",
      "Train Epoch [126/200]Batch [300/391] Loss: 0.997 Acc 65.939%\n",
      "Test Epoch [126/200]Batch [  0/ 79] Loss: 0.839 Acc 75.000%\n",
      "Train Epoch [127/200]Batch [  0/391] Loss: 1.061 Acc 67.188%\n",
      "Train Epoch [127/200]Batch [100/391] Loss: 1.010 Acc 65.981%\n",
      "Train Epoch [127/200]Batch [200/391] Loss: 1.006 Acc 65.843%\n",
      "Train Epoch [127/200]Batch [300/391] Loss: 1.004 Acc 66.108%\n",
      "Test Epoch [127/200]Batch [  0/ 79] Loss: 0.827 Acc 71.875%\n",
      "Train Epoch [128/200]Batch [  0/391] Loss: 0.860 Acc 65.625%\n",
      "Train Epoch [128/200]Batch [100/391] Loss: 0.997 Acc 65.555%\n",
      "Train Epoch [128/200]Batch [200/391] Loss: 0.990 Acc 66.157%\n",
      "Train Epoch [128/200]Batch [300/391] Loss: 0.997 Acc 65.931%\n",
      "Test Epoch [128/200]Batch [  0/ 79] Loss: 0.833 Acc 74.219%\n",
      "Train Epoch [129/200]Batch [  0/391] Loss: 1.095 Acc 64.844%\n",
      "Train Epoch [129/200]Batch [100/391] Loss: 0.996 Acc 66.700%\n",
      "Train Epoch [129/200]Batch [200/391] Loss: 0.998 Acc 66.161%\n",
      "Train Epoch [129/200]Batch [300/391] Loss: 0.993 Acc 66.154%\n",
      "Test Epoch [129/200]Batch [  0/ 79] Loss: 0.829 Acc 75.000%\n",
      "Train Epoch [130/200]Batch [  0/391] Loss: 0.945 Acc 64.844%\n",
      "Train Epoch [130/200]Batch [100/391] Loss: 0.996 Acc 66.174%\n",
      "Train Epoch [130/200]Batch [200/391] Loss: 0.991 Acc 66.255%\n",
      "Train Epoch [130/200]Batch [300/391] Loss: 0.992 Acc 66.276%\n",
      "Test Epoch [130/200]Batch [  0/ 79] Loss: 0.806 Acc 76.562%\n",
      "Saving..\n",
      "Train Epoch [131/200]Batch [  0/391] Loss: 1.160 Acc 60.156%\n",
      "Train Epoch [131/200]Batch [100/391] Loss: 1.006 Acc 65.927%\n",
      "Train Epoch [131/200]Batch [200/391] Loss: 1.000 Acc 65.917%\n",
      "Train Epoch [131/200]Batch [300/391] Loss: 0.999 Acc 65.918%\n",
      "Test Epoch [131/200]Batch [  0/ 79] Loss: 0.826 Acc 75.781%\n",
      "Train Epoch [132/200]Batch [  0/391] Loss: 0.999 Acc 67.188%\n",
      "Train Epoch [132/200]Batch [100/391] Loss: 0.998 Acc 65.811%\n",
      "Train Epoch [132/200]Batch [200/391] Loss: 0.989 Acc 66.259%\n",
      "Train Epoch [132/200]Batch [300/391] Loss: 0.990 Acc 66.398%\n",
      "Test Epoch [132/200]Batch [  0/ 79] Loss: 0.826 Acc 73.438%\n",
      "Train Epoch [133/200]Batch [  0/391] Loss: 1.010 Acc 64.844%\n",
      "Train Epoch [133/200]Batch [100/391] Loss: 1.020 Acc 65.223%\n",
      "Train Epoch [133/200]Batch [200/391] Loss: 1.006 Acc 65.769%\n",
      "Train Epoch [133/200]Batch [300/391] Loss: 1.000 Acc 65.799%\n",
      "Test Epoch [133/200]Batch [  0/ 79] Loss: 0.818 Acc 75.000%\n",
      "Train Epoch [134/200]Batch [  0/391] Loss: 0.996 Acc 65.625%\n",
      "Train Epoch [134/200]Batch [100/391] Loss: 1.003 Acc 65.718%\n",
      "Train Epoch [134/200]Batch [200/391] Loss: 0.998 Acc 66.126%\n",
      "Train Epoch [134/200]Batch [300/391] Loss: 0.999 Acc 66.007%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [134/200]Batch [  0/ 79] Loss: 0.828 Acc 73.438%\n",
      "Train Epoch [135/200]Batch [  0/391] Loss: 0.792 Acc 74.219%\n",
      "Train Epoch [135/200]Batch [100/391] Loss: 0.990 Acc 66.236%\n",
      "Train Epoch [135/200]Batch [200/391] Loss: 0.989 Acc 66.445%\n",
      "Train Epoch [135/200]Batch [300/391] Loss: 0.996 Acc 66.251%\n",
      "Test Epoch [135/200]Batch [  0/ 79] Loss: 0.814 Acc 74.219%\n",
      "Train Epoch [136/200]Batch [  0/391] Loss: 1.012 Acc 71.094%\n",
      "Train Epoch [136/200]Batch [100/391] Loss: 0.987 Acc 66.074%\n",
      "Train Epoch [136/200]Batch [200/391] Loss: 0.992 Acc 65.905%\n",
      "Train Epoch [136/200]Batch [300/391] Loss: 0.992 Acc 66.017%\n",
      "Test Epoch [136/200]Batch [  0/ 79] Loss: 0.809 Acc 76.562%\n",
      "Train Epoch [137/200]Batch [  0/391] Loss: 0.948 Acc 68.750%\n",
      "Train Epoch [137/200]Batch [100/391] Loss: 0.997 Acc 66.569%\n",
      "Train Epoch [137/200]Batch [200/391] Loss: 0.986 Acc 66.892%\n",
      "Train Epoch [137/200]Batch [300/391] Loss: 0.990 Acc 66.474%\n",
      "Test Epoch [137/200]Batch [  0/ 79] Loss: 0.821 Acc 75.000%\n",
      "Train Epoch [138/200]Batch [  0/391] Loss: 0.937 Acc 73.438%\n",
      "Train Epoch [138/200]Batch [100/391] Loss: 1.001 Acc 66.267%\n",
      "Train Epoch [138/200]Batch [200/391] Loss: 0.996 Acc 66.208%\n",
      "Train Epoch [138/200]Batch [300/391] Loss: 0.997 Acc 66.142%\n",
      "Test Epoch [138/200]Batch [  0/ 79] Loss: 0.805 Acc 74.219%\n",
      "Train Epoch [139/200]Batch [  0/391] Loss: 1.078 Acc 64.062%\n",
      "Train Epoch [139/200]Batch [100/391] Loss: 1.003 Acc 65.718%\n",
      "Train Epoch [139/200]Batch [200/391] Loss: 0.995 Acc 65.905%\n",
      "Train Epoch [139/200]Batch [300/391] Loss: 0.997 Acc 65.908%\n",
      "Test Epoch [139/200]Batch [  0/ 79] Loss: 0.799 Acc 76.562%\n",
      "Train Epoch [140/200]Batch [  0/391] Loss: 1.005 Acc 64.844%\n",
      "Train Epoch [140/200]Batch [100/391] Loss: 0.987 Acc 65.950%\n",
      "Train Epoch [140/200]Batch [200/391] Loss: 0.989 Acc 66.371%\n",
      "Train Epoch [140/200]Batch [300/391] Loss: 0.994 Acc 66.071%\n",
      "Test Epoch [140/200]Batch [  0/ 79] Loss: 0.792 Acc 75.000%\n",
      "Train Epoch [141/200]Batch [  0/391] Loss: 1.074 Acc 58.594%\n",
      "Train Epoch [141/200]Batch [100/391] Loss: 0.978 Acc 66.662%\n",
      "Train Epoch [141/200]Batch [200/391] Loss: 0.986 Acc 66.558%\n",
      "Train Epoch [141/200]Batch [300/391] Loss: 0.987 Acc 66.404%\n",
      "Test Epoch [141/200]Batch [  0/ 79] Loss: 0.800 Acc 74.219%\n",
      "Train Epoch [142/200]Batch [  0/391] Loss: 1.131 Acc 59.375%\n",
      "Train Epoch [142/200]Batch [100/391] Loss: 0.998 Acc 65.958%\n",
      "Train Epoch [142/200]Batch [200/391] Loss: 0.987 Acc 66.379%\n",
      "Train Epoch [142/200]Batch [300/391] Loss: 0.991 Acc 66.149%\n",
      "Test Epoch [142/200]Batch [  0/ 79] Loss: 0.799 Acc 75.781%\n",
      "Train Epoch [143/200]Batch [  0/391] Loss: 0.919 Acc 72.656%\n",
      "Train Epoch [143/200]Batch [100/391] Loss: 1.001 Acc 65.362%\n",
      "Train Epoch [143/200]Batch [200/391] Loss: 0.994 Acc 65.839%\n",
      "Train Epoch [143/200]Batch [300/391] Loss: 0.993 Acc 65.840%\n",
      "Test Epoch [143/200]Batch [  0/ 79] Loss: 0.803 Acc 75.781%\n",
      "Train Epoch [144/200]Batch [  0/391] Loss: 0.970 Acc 69.531%\n",
      "Train Epoch [144/200]Batch [100/391] Loss: 0.979 Acc 67.071%\n",
      "Train Epoch [144/200]Batch [200/391] Loss: 0.989 Acc 66.616%\n",
      "Train Epoch [144/200]Batch [300/391] Loss: 0.994 Acc 66.310%\n",
      "Test Epoch [144/200]Batch [  0/ 79] Loss: 0.800 Acc 74.219%\n",
      "Train Epoch [145/200]Batch [  0/391] Loss: 1.120 Acc 64.844%\n",
      "Train Epoch [145/200]Batch [100/391] Loss: 0.985 Acc 66.623%\n",
      "Train Epoch [145/200]Batch [200/391] Loss: 0.993 Acc 66.041%\n",
      "Train Epoch [145/200]Batch [300/391] Loss: 0.992 Acc 66.219%\n",
      "Test Epoch [145/200]Batch [  0/ 79] Loss: 0.800 Acc 75.781%\n",
      "Train Epoch [146/200]Batch [  0/391] Loss: 0.875 Acc 70.312%\n",
      "Train Epoch [146/200]Batch [100/391] Loss: 0.981 Acc 66.917%\n",
      "Train Epoch [146/200]Batch [200/391] Loss: 0.985 Acc 66.639%\n",
      "Train Epoch [146/200]Batch [300/391] Loss: 0.991 Acc 66.347%\n",
      "Test Epoch [146/200]Batch [  0/ 79] Loss: 0.800 Acc 76.562%\n",
      "Saving..\n",
      "Train Epoch [147/200]Batch [  0/391] Loss: 1.107 Acc 58.594%\n",
      "Train Epoch [147/200]Batch [100/391] Loss: 0.999 Acc 65.803%\n",
      "Train Epoch [147/200]Batch [200/391] Loss: 0.992 Acc 66.367%\n",
      "Train Epoch [147/200]Batch [300/391] Loss: 0.993 Acc 66.411%\n",
      "Test Epoch [147/200]Batch [  0/ 79] Loss: 0.792 Acc 75.000%\n",
      "Train Epoch [148/200]Batch [  0/391] Loss: 1.048 Acc 63.281%\n",
      "Train Epoch [148/200]Batch [100/391] Loss: 0.986 Acc 66.468%\n",
      "Train Epoch [148/200]Batch [200/391] Loss: 0.984 Acc 66.585%\n",
      "Train Epoch [148/200]Batch [300/391] Loss: 0.983 Acc 66.710%\n",
      "Test Epoch [148/200]Batch [  0/ 79] Loss: 0.787 Acc 75.781%\n",
      "Train Epoch [149/200]Batch [  0/391] Loss: 0.895 Acc 67.969%\n",
      "Train Epoch [149/200]Batch [100/391] Loss: 0.993 Acc 66.855%\n",
      "Train Epoch [149/200]Batch [200/391] Loss: 0.982 Acc 66.803%\n",
      "Train Epoch [149/200]Batch [300/391] Loss: 0.989 Acc 66.642%\n",
      "Test Epoch [149/200]Batch [  0/ 79] Loss: 0.802 Acc 75.781%\n",
      "Train Epoch [150/200]Batch [  0/391] Loss: 0.976 Acc 64.844%\n",
      "Train Epoch [150/200]Batch [100/391] Loss: 0.982 Acc 66.414%\n",
      "Train Epoch [150/200]Batch [200/391] Loss: 0.986 Acc 66.527%\n",
      "Train Epoch [150/200]Batch [300/391] Loss: 0.983 Acc 66.494%\n",
      "Test Epoch [150/200]Batch [  0/ 79] Loss: 0.835 Acc 74.219%\n",
      "Train Epoch [151/200]Batch [  0/391] Loss: 0.994 Acc 65.625%\n",
      "Train Epoch [151/200]Batch [100/391] Loss: 0.990 Acc 66.275%\n",
      "Train Epoch [151/200]Batch [200/391] Loss: 0.984 Acc 66.721%\n",
      "Train Epoch [151/200]Batch [300/391] Loss: 0.984 Acc 66.728%\n",
      "Test Epoch [151/200]Batch [  0/ 79] Loss: 0.773 Acc 77.344%\n",
      "Train Epoch [152/200]Batch [  0/391] Loss: 0.958 Acc 70.312%\n",
      "Train Epoch [152/200]Batch [100/391] Loss: 0.979 Acc 66.638%\n",
      "Train Epoch [152/200]Batch [200/391] Loss: 0.979 Acc 66.783%\n",
      "Train Epoch [152/200]Batch [300/391] Loss: 0.985 Acc 66.554%\n",
      "Test Epoch [152/200]Batch [  0/ 79] Loss: 0.796 Acc 77.344%\n",
      "Train Epoch [153/200]Batch [  0/391] Loss: 1.020 Acc 62.500%\n",
      "Train Epoch [153/200]Batch [100/391] Loss: 0.979 Acc 66.453%\n",
      "Train Epoch [153/200]Batch [200/391] Loss: 0.974 Acc 66.694%\n",
      "Train Epoch [153/200]Batch [300/391] Loss: 0.979 Acc 66.606%\n",
      "Test Epoch [153/200]Batch [  0/ 79] Loss: 0.796 Acc 75.781%\n",
      "Train Epoch [154/200]Batch [  0/391] Loss: 0.865 Acc 72.656%\n",
      "Train Epoch [154/200]Batch [100/391] Loss: 0.993 Acc 66.221%\n",
      "Train Epoch [154/200]Batch [200/391] Loss: 0.988 Acc 66.329%\n",
      "Train Epoch [154/200]Batch [300/391] Loss: 0.985 Acc 66.385%\n",
      "Test Epoch [154/200]Batch [  0/ 79] Loss: 0.805 Acc 76.562%\n",
      "Train Epoch [155/200]Batch [  0/391] Loss: 0.946 Acc 67.188%\n",
      "Train Epoch [155/200]Batch [100/391] Loss: 0.986 Acc 66.267%\n",
      "Train Epoch [155/200]Batch [200/391] Loss: 0.990 Acc 66.286%\n",
      "Train Epoch [155/200]Batch [300/391] Loss: 0.986 Acc 66.401%\n",
      "Test Epoch [155/200]Batch [  0/ 79] Loss: 0.792 Acc 76.562%\n",
      "Train Epoch [156/200]Batch [  0/391] Loss: 0.946 Acc 68.750%\n",
      "Train Epoch [156/200]Batch [100/391] Loss: 0.990 Acc 66.321%\n",
      "Train Epoch [156/200]Batch [200/391] Loss: 0.985 Acc 66.604%\n",
      "Train Epoch [156/200]Batch [300/391] Loss: 0.985 Acc 66.570%\n",
      "Test Epoch [156/200]Batch [  0/ 79] Loss: 0.801 Acc 75.000%\n",
      "Train Epoch [157/200]Batch [  0/391] Loss: 0.934 Acc 70.312%\n",
      "Train Epoch [157/200]Batch [100/391] Loss: 0.976 Acc 67.126%\n",
      "Train Epoch [157/200]Batch [200/391] Loss: 0.979 Acc 66.772%\n",
      "Train Epoch [157/200]Batch [300/391] Loss: 0.980 Acc 66.653%\n",
      "Test Epoch [157/200]Batch [  0/ 79] Loss: 0.827 Acc 75.000%\n",
      "Train Epoch [158/200]Batch [  0/391] Loss: 1.032 Acc 63.281%\n",
      "Train Epoch [158/200]Batch [100/391] Loss: 0.978 Acc 66.754%\n",
      "Train Epoch [158/200]Batch [200/391] Loss: 0.985 Acc 66.395%\n",
      "Train Epoch [158/200]Batch [300/391] Loss: 0.987 Acc 66.339%\n",
      "Test Epoch [158/200]Batch [  0/ 79] Loss: 0.822 Acc 75.000%\n",
      "Train Epoch [159/200]Batch [  0/391] Loss: 0.940 Acc 67.969%\n",
      "Train Epoch [159/200]Batch [100/391] Loss: 0.986 Acc 66.654%\n",
      "Train Epoch [159/200]Batch [200/391] Loss: 0.982 Acc 66.694%\n",
      "Train Epoch [159/200]Batch [300/391] Loss: 0.982 Acc 66.546%\n",
      "Test Epoch [159/200]Batch [  0/ 79] Loss: 0.817 Acc 74.219%\n",
      "Train Epoch [160/200]Batch [  0/391] Loss: 1.124 Acc 63.281%\n",
      "Train Epoch [160/200]Batch [100/391] Loss: 0.976 Acc 67.226%\n",
      "Train Epoch [160/200]Batch [200/391] Loss: 0.976 Acc 67.211%\n",
      "Train Epoch [160/200]Batch [300/391] Loss: 0.980 Acc 66.928%\n",
      "Test Epoch [160/200]Batch [  0/ 79] Loss: 0.808 Acc 74.219%\n",
      "Train Epoch [161/200]Batch [  0/391] Loss: 1.007 Acc 66.406%\n",
      "Train Epoch [161/200]Batch [100/391] Loss: 0.976 Acc 66.615%\n",
      "Train Epoch [161/200]Batch [200/391] Loss: 0.977 Acc 66.845%\n",
      "Train Epoch [161/200]Batch [300/391] Loss: 0.976 Acc 66.977%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [161/200]Batch [  0/ 79] Loss: 0.825 Acc 75.781%\n",
      "Train Epoch [162/200]Batch [  0/391] Loss: 1.015 Acc 58.594%\n",
      "Train Epoch [162/200]Batch [100/391] Loss: 0.964 Acc 67.497%\n",
      "Train Epoch [162/200]Batch [200/391] Loss: 0.972 Acc 67.129%\n",
      "Train Epoch [162/200]Batch [300/391] Loss: 0.976 Acc 67.027%\n",
      "Test Epoch [162/200]Batch [  0/ 79] Loss: 0.815 Acc 72.656%\n",
      "Train Epoch [163/200]Batch [  0/391] Loss: 0.860 Acc 66.406%\n",
      "Train Epoch [163/200]Batch [100/391] Loss: 0.995 Acc 66.190%\n",
      "Train Epoch [163/200]Batch [200/391] Loss: 0.988 Acc 66.363%\n",
      "Train Epoch [163/200]Batch [300/391] Loss: 0.981 Acc 66.502%\n",
      "Test Epoch [163/200]Batch [  0/ 79] Loss: 0.811 Acc 74.219%\n",
      "Train Epoch [164/200]Batch [  0/391] Loss: 0.821 Acc 73.438%\n",
      "Train Epoch [164/200]Batch [100/391] Loss: 1.002 Acc 66.476%\n",
      "Train Epoch [164/200]Batch [200/391] Loss: 0.985 Acc 66.678%\n",
      "Train Epoch [164/200]Batch [300/391] Loss: 0.983 Acc 66.432%\n",
      "Test Epoch [164/200]Batch [  0/ 79] Loss: 0.794 Acc 73.438%\n",
      "Train Epoch [165/200]Batch [  0/391] Loss: 1.106 Acc 67.188%\n",
      "Train Epoch [165/200]Batch [100/391] Loss: 0.981 Acc 66.445%\n",
      "Train Epoch [165/200]Batch [200/391] Loss: 0.976 Acc 66.768%\n",
      "Train Epoch [165/200]Batch [300/391] Loss: 0.975 Acc 66.816%\n",
      "Test Epoch [165/200]Batch [  0/ 79] Loss: 0.795 Acc 75.781%\n",
      "Train Epoch [166/200]Batch [  0/391] Loss: 0.855 Acc 71.094%\n",
      "Train Epoch [166/200]Batch [100/391] Loss: 0.974 Acc 66.894%\n",
      "Train Epoch [166/200]Batch [200/391] Loss: 0.984 Acc 66.531%\n",
      "Train Epoch [166/200]Batch [300/391] Loss: 0.977 Acc 66.790%\n",
      "Test Epoch [166/200]Batch [  0/ 79] Loss: 0.793 Acc 74.219%\n",
      "Train Epoch [167/200]Batch [  0/391] Loss: 0.843 Acc 72.656%\n",
      "Train Epoch [167/200]Batch [100/391] Loss: 0.978 Acc 66.530%\n",
      "Train Epoch [167/200]Batch [200/391] Loss: 0.976 Acc 66.628%\n",
      "Train Epoch [167/200]Batch [300/391] Loss: 0.975 Acc 66.746%\n",
      "Test Epoch [167/200]Batch [  0/ 79] Loss: 0.772 Acc 76.562%\n",
      "Train Epoch [168/200]Batch [  0/391] Loss: 0.895 Acc 66.406%\n",
      "Train Epoch [168/200]Batch [100/391] Loss: 0.964 Acc 67.450%\n",
      "Train Epoch [168/200]Batch [200/391] Loss: 0.973 Acc 67.149%\n",
      "Train Epoch [168/200]Batch [300/391] Loss: 0.973 Acc 66.995%\n",
      "Test Epoch [168/200]Batch [  0/ 79] Loss: 0.775 Acc 77.344%\n",
      "Train Epoch [169/200]Batch [  0/391] Loss: 1.059 Acc 64.062%\n",
      "Train Epoch [169/200]Batch [100/391] Loss: 0.978 Acc 66.344%\n",
      "Train Epoch [169/200]Batch [200/391] Loss: 0.976 Acc 66.433%\n",
      "Train Epoch [169/200]Batch [300/391] Loss: 0.972 Acc 66.489%\n",
      "Test Epoch [169/200]Batch [  0/ 79] Loss: 0.807 Acc 75.000%\n",
      "Train Epoch [170/200]Batch [  0/391] Loss: 1.052 Acc 63.281%\n",
      "Train Epoch [170/200]Batch [100/391] Loss: 0.981 Acc 66.600%\n",
      "Train Epoch [170/200]Batch [200/391] Loss: 0.979 Acc 66.686%\n",
      "Train Epoch [170/200]Batch [300/391] Loss: 0.978 Acc 66.715%\n",
      "Test Epoch [170/200]Batch [  0/ 79] Loss: 0.806 Acc 75.781%\n",
      "Train Epoch [171/200]Batch [  0/391] Loss: 0.974 Acc 67.969%\n",
      "Train Epoch [171/200]Batch [100/391] Loss: 0.974 Acc 67.304%\n",
      "Train Epoch [171/200]Batch [200/391] Loss: 0.978 Acc 67.055%\n",
      "Train Epoch [171/200]Batch [300/391] Loss: 0.979 Acc 66.956%\n",
      "Test Epoch [171/200]Batch [  0/ 79] Loss: 0.816 Acc 72.656%\n",
      "Train Epoch [172/200]Batch [  0/391] Loss: 1.129 Acc 57.031%\n",
      "Train Epoch [172/200]Batch [100/391] Loss: 0.980 Acc 66.553%\n",
      "Train Epoch [172/200]Batch [200/391] Loss: 0.977 Acc 66.671%\n",
      "Train Epoch [172/200]Batch [300/391] Loss: 0.977 Acc 66.796%\n",
      "Test Epoch [172/200]Batch [  0/ 79] Loss: 0.810 Acc 72.656%\n",
      "Train Epoch [173/200]Batch [  0/391] Loss: 0.970 Acc 64.062%\n",
      "Train Epoch [173/200]Batch [100/391] Loss: 0.972 Acc 67.133%\n",
      "Train Epoch [173/200]Batch [200/391] Loss: 0.973 Acc 67.102%\n",
      "Train Epoch [173/200]Batch [300/391] Loss: 0.976 Acc 66.881%\n",
      "Test Epoch [173/200]Batch [  0/ 79] Loss: 0.815 Acc 75.781%\n",
      "Train Epoch [174/200]Batch [  0/391] Loss: 0.988 Acc 68.750%\n",
      "Train Epoch [174/200]Batch [100/391] Loss: 0.989 Acc 66.174%\n",
      "Train Epoch [174/200]Batch [200/391] Loss: 0.979 Acc 66.651%\n",
      "Train Epoch [174/200]Batch [300/391] Loss: 0.977 Acc 66.889%\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0 \n",
    "\n",
    "for lr in tqdm_notebook(LRS):\n",
    "    for norm in tqdm_notebook(NORMS):\n",
    "        net = DenseNet(3, 10, norm=norm).cuda()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "        train_loss_log =[]\n",
    "        train_acc_log = []\n",
    "        test_loss_log =[]\n",
    "        test_acc_log =[]\n",
    "\n",
    "        for epoch in tqdm_notebook(range(NUM_EPOCH)):\n",
    "            train(epoch)\n",
    "            test(epoch)\n",
    "\n",
    "        np.save(SAVE_PATH+\"DenseNet_Train_loss_{}_{}.npy\".format(norm,lr), train_loss_log)  \n",
    "        np.save(SAVE_PATH+\"DenseNet_Test_loss_{}_{}.npy\".format(norm,lr), test_loss_log)    \n",
    "        np.save(SAVE_PATH+\"DenseNet_Train_Acc_{}_{}.npy\".format(norm,lr), train_acc_log)    \n",
    "        np.save(SAVE_PATH+\"DenseNet_Test_Acc_{}_{}.npy\".format(norm,lr), test_acc_log)   \n",
    "        del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "plt.figure(figsize=(18, 15))\n",
    "for i, lr in enumerate(tqdm_notebook(LRS)):    \n",
    "    for j, norm in enumerate(tqdm_notebook([None,'BN', 'SN', 'MSN'])):\n",
    "        print(k)\n",
    "        train_loss_log = np.load(SAVE_PATH+\"DenseNet_Train_loss_{}_{}.npy\".format(norm,lr) )  \n",
    "        test_loss_log = np.load(SAVE_PATH+\"DenseNet_Test_loss_{}_{}.npy\".format(norm,lr))    \n",
    "        train_acc_log = np.load(SAVE_PATH+\"DenseNet_Train_Acc_{}_{}.npy\".format(norm,lr))    \n",
    "        test_acc_log= np.load(SAVE_PATH+\"DenseNet_Test_Acc_{}_{}.npy\".format(norm,lr))   \n",
    "        \n",
    "        ax = plt.subplot(3,4,k+1)\n",
    "        plt.plot(train_loss_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Train Loss', fontsize=18)     \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "        ax = plt.subplot(3,4,k+2)\n",
    "        plt.plot(test_loss_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Test Loss', fontsize=18) \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "        ax = plt.subplot(3,4,k+3)\n",
    "        plt.plot(train_acc_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Train Accuracy', fontsize=18) \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "                \n",
    "        ax = plt.subplot(3,4,k+4)\n",
    "        plt.plot(test_acc_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Test Accuracy', fontsize=18)        \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "    k+= 4\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_PATH+'Act_Norm_Results.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "plt.figure(figsize=(18, 15))\n",
    "for i, lr in enumerate(tqdm_notebook(LRS)):    \n",
    "    for j, norm in enumerate(tqdm_notebook(['WN', 'SN', 'MSN'])):\n",
    "        print(k)\n",
    "        train_loss_log = np.load(SAVE_PATH+\"DenseNet_Train_loss_{}_{}.npy\".format(norm,lr) )  \n",
    "        test_loss_log = np.load(SAVE_PATH+\"DenseNet_Test_loss_{}_{}.npy\".format(norm,lr))    \n",
    "        train_acc_log = np.load(SAVE_PATH+\"DenseNet_Train_Acc_{}_{}.npy\".format(norm,lr))    \n",
    "        test_acc_log= np.load(SAVE_PATH+\"DenseNet_Test_Acc_{}_{}.npy\".format(norm,lr))   \n",
    "        \n",
    "        ax = plt.subplot(3,4,k+1)\n",
    "        plt.plot(train_loss_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Train Loss', fontsize=18)     \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "        ax = plt.subplot(3,4,k+2)\n",
    "        plt.plot(test_loss_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Test Loss', fontsize=18) \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "        ax = plt.subplot(3,4,k+3)\n",
    "        plt.plot(train_acc_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Train Accuracy', fontsize=18) \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "                \n",
    "        ax = plt.subplot(3,4,k+4)\n",
    "        plt.plot(test_acc_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Test Accuracy', fontsize=18)        \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "    k+= 4\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_PATH+'Weight_reparam_Results.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "plt.figure(figsize=(18, 15))\n",
    "for i, lr in enumerate(tqdm_notebook(LRS)):    \n",
    "    for j, norm in enumerate(tqdm_notebook(['MSNTReLU', 'WNTReLU'])):\n",
    "        print(k)\n",
    "        train_loss_log = np.load(SAVE_PATH+\"DenseNet_Train_loss_{}_{}.npy\".format(norm,lr) )  \n",
    "        test_loss_log = np.load(SAVE_PATH+\"DenseNet_Test_loss_{}_{}.npy\".format(norm,lr))    \n",
    "        train_acc_log = np.load(SAVE_PATH+\"DenseNet_Train_Acc_{}_{}.npy\".format(norm,lr))    \n",
    "        test_acc_log= np.load(SAVE_PATH+\"DenseNet_Test_Acc_{}_{}.npy\".format(norm,lr))   \n",
    "        \n",
    "        ax = plt.subplot(3,4,k+1)\n",
    "        plt.plot(train_loss_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Train Loss', fontsize=18)     \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "        ax = plt.subplot(3,4,k+2)\n",
    "        plt.plot(test_loss_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Test Loss', fontsize=18) \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "\n",
    "        ax = plt.subplot(3,4,k+3)\n",
    "        plt.plot(train_acc_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Train Accuracy', fontsize=18) \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "                \n",
    "        ax = plt.subplot(3,4,k+4)\n",
    "        plt.plot(test_acc_log, lw=2.5, label=str(norm))\n",
    "        plt.xlabel('Iterations', fontsize=18)\n",
    "        plt.ylabel('Test Accuracy', fontsize=18)        \n",
    "        plt.legend(fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Rate ={}\".format(lr), fontsize=20);\n",
    "    k+= 4\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_PATH+'Weight_reparam_act_Results.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
